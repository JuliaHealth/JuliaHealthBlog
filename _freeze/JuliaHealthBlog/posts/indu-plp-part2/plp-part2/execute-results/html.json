{
  "hash": "38e74cfe87c49d5d9110c792519699bb",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"PLP-Pipeline Series Part 2: From Raw Clinical Data to Predictive Models\"\ndescription: \"Part 2 of the PLP-Pipeline blog series â€“ how we preprocess OMOP CDM data, extract features, and train ML models using Julia tools\"\nauthor: \"Kosuri Lakshmi Indu\"\ndate: \"4/20/2025\"\nbibliography: ./references.bib\ncsl: ./../../ieee-with-url.csl\ntoc: true\nengine: julia\nimage: false\ncategories:\n  - patient-level prediction\n  - omop cdm\n  - observational health\n  - machine learning\n---\n\n\n\n# Introduction ðŸ‘‹\n\nWelcome back to Part 2 of the PLP-Pipeline blog series!\n\nIn [Part 1](../indu-plp-part1/plp-part1.qmd), we formulated a research question, loaded synthetic patient data, and constructed our target (hypertension) and outcome (diabetes) cohorts using OHDSI definitions and Julia tools. If you havenâ€™t read that yet, I recommend checking it out before diving in here.\n\nNow in Part 2, we move from cohorts to models - i.e., weâ€™ll bridge the gap between structured clinical data and predictive modeling. Specifically, weâ€™ll walk through:\n\n- Extracting patient-level features using cohort definitions\n- Performing preprocessing (imputation, encoding, normalization)\n- Splitting data into train/test sets\n- Training ML models using MLJ.jl\n\nThese steps will guide you through the prediction pipeline, aiming to predict the likelihood of a patient with hypertension developing diabetes.\n\n# Step 1: Feature Engineering from OMOP CDM\n\nWe extract features from several structured OMOP CDM tables such as:\n\n| **Table**               | **Features Extracted**                                                                 |\n|-------------------------|----------------------------------------------------------------------------------------|\n| `condition_occurrence`   | Count of distinct parent and child conditions diagnosed or observed before the index date. |\n| `drug_exposure`          | Count of distinct parent and child drugs exposed, total days of drug supply, total drug quantity, and the most common drug route before the index date. |\n| `procedure_occurrence`   | Count of distinct past procedures undergone before the index date. |\n| `observation`            | Count of distinct observation concepts and the latest recorded observation value before the index date. |\n| `measurement`            | Maximum recorded measurement value and the most common unit of measurement before the index date. |\n| `person`                 | Age, gender, race, and ethnicity information for the patient. |\n\nThe features are computed for each patient in the **target cohort** using a **365-day window prior** to the cohort start date.\n\nEach table is queried independently, and the results are merged on `subject_id` to create a patient-level feature matrix for modeling.\n\n### Example: Drug Exposure Feature Extraction\n\nThe query below summarizes drug history for each patient using several aggregations:\n\n- **`drug_count`**: number of distinct drug classes (using `concept_ancestor` for generalization)\n- **`total_days_supply`**: total days covered by prescriptions\n- **`total_quantity`**: total quantity of drugs supplied\n- **`max_common_route`**: most frequent route of administration\n\n**File:** `feature_extraction.jl`\n\n```julia\n# Drug exposure features: summarizing drug history in the past 365 days\ndrugs_query = \"\"\"\nSELECT \n    c.subject_id, \n\n    # Count of distinct parent-level drug concepts\n    COUNT(DISTINCT ca.ancestor_concept_id) AS drug_count,\n\n    # Sum of days supply to estimate treatment duration\n    SUM(de.days_supply) AS total_days_supply,\n\n    # Sum of quantity to capture volume of medication used\n    SUM(de.quantity) AS total_quantity,\n\n    # Most common route of administration\n    MAX(de.route_concept_id) AS max_common_route\n\nFROM dbt_synthea_dev.cohort c\n\n# Join drug exposures for each subject\nJOIN dbt_synthea_dev.drug_exposure de \n    ON c.subject_id = de.person_id\n\n# Map each drug to a higher-level concept using the concept hierarchy\nJOIN dbt_synthea_dev.concept_ancestor ca \n    ON de.drug_concept_id = ca.descendant_concept_id\n\n# Filter to target cohort (e.g., patients with hypertension)\nWHERE c.cohort_definition_id = 1\n\n# Limit events to the 365 days before the cohort start date\nAND de.drug_exposure_start_date BETWEEN c.cohort_start_date - INTERVAL 365 DAY AND c.cohort_start_date\n\n# Aggregate by subject\nGROUP BY c.subject_id\n\"\"\"\n```\n\nThis approach is repeated across other OMOP CDM tables (condition_occurrence, observation, measurement, procedure_occurrence, and visit_occurrence). This aligns with the paperâ€™s recommendation to generate temporal, interpretable, and structured patient features from OMOP CDM.\n\n# Step 2: Attaching Outcome Labels\n\nNow that we have extracted features for the target cohort (patients diagnosed with hypertension), the next step is to attach outcome labels that indicate whether a patient later developed diabetes.\n\nWe use Cohort 2 i.e outcome cohort, which includes patients from the target cohort who were subsequently diagnosed with diabetes, as defined using `OHDSICohortExpressions.jl`. This ensures that diabetes occurs *after* the hypertension event, maintaining proper temporal ordering between the target and outcome.\n\nWe query the `cohort` table for all subjects in outcome cohort and assign them an outcome label of `1`.\n\n**File:** `outcome_attach.jl`\n\n```julia\ndiabetes_query = \"\"\"\n    SELECT subject_id, 1 AS outcome\n    FROM dbt_synthea_dev.cohort\n    WHERE cohort_definition_id = 2\n\"\"\"\ndiabetes_df = execute(conn, diabetes_query) | > DataFrame\n```\n\nNext, we perform a **left join** with the previously created `features_df`, which contains covariates for all patients in the hypertension cohort. This ensures every patient has their features preserved, and only those present in the diabetes cohort will have a $1$ under the outcome column.\n\nFor patients not found in the outcome cohort, the join results in a missing value. We treat these as negative cases $0$, meaning the patient did not develop diabetes during the follow-up period.\n\n```julia\ndf = leftjoin(features_df, diabetes_df, on=:subject_id)\ndf[!, :outcome] .= coalesce.(df[!, :outcome], 0)\n```\nThis gives us a binary classification dataset with:\n\n- $1$: patient developed diabetes after hypertension\n- $0$: patient did not develop diabetes (or not within the observed period)\n\nThis labeled dataset is now ready for preprocessing and model training in later steps.\n\n# Step 3: Preprocessing for Modeling\n\nWe perform the following three tasks:\n\n- Handling missing values\n- Standardizing numeric features\n- Encoding categorical variables\n\n**File:** `preprocessing.jl`\n\n```julia\n# Impute missing values\nfor col in names(df)\n    if eltype(df[!, col]) <: Union{Missing, Number}\n        df[!, col] = coalesce.(df[!, col], 0)\n    else\n        df[!, col] = coalesce.(df[!, col], \"unknown\")\n    end\nend\n\n# Standardize numerical columns\nfor col in [:age, :condition_count, :drug_count]\n    Î¼, Ïƒ = mean(skipmissing(df[!, col])), std(skipmissing(df[!, col]))\n    df[!, col] .= (df[!, col] .- Î¼) ./ Ïƒ\nend\n\n# Encode categoricals\ndf.gender_concept_id = categorical(df.gender_concept_id)\ndf.race_concept_id = categorical(df.race_concept_id)\n```\n\n**Train-Test Splitting**\n\nFinally, we split the data into training and testing sets using an 80-20 split. This ensures we can evaluate how well the model generalizes to unseen data.\n\n```julia\nusing MLJ\ntrain, test = partition(eachindex(df.outcome), 0.8, shuffle=true)\n```\n<br>\n<center>\n  ![](./train_test_splitting.png)\n\n  Train-Test splitting (80% - 20%)\n</center>\n\n# Step 4: Model Training with MLJ.jl\n\nWe evaluated three machine learning models to identify the best-performing approach for predicting whether a patient develops diabetes. The paper specifically mentions using logistic regression (with L1 regularization) , random forest, and gradient boosting machines (like XGBoost) as part of their standardized framework.\n\nThe three models we implemented in Julia are:\n\n- **L1-Regularized Logistic Regression**: Used for feature selection and classification, offering strong baseline performance and interpretability in sparse healthcare data.\n\n- **Random Forest**: An ensemble method that builds multiple decision trees to capture non-linear relationships and interactions, robust against overfitting.\n\n- **XGBoost (Extreme Gradient Boosting)**: A boosting algorithm that sequentially builds trees to improve accuracy, known for its high predictive performance and tested in the paper for comparison.\n\n**File:** `train_model.jl`\n\n### Shared Evaluation Function\n\n```julia\nusing MLJLinearModels, MLJDecisionTreeInterface, MLJXGBoostInterface, ROCAnalysis\n\n# Generic function to evaluate a classifier using AUC\nfunction evaluate_model(model, X_train, y_train, X_test, y_test)\n    m = machine(model, X_train, y_train)   # Bind model with data\n    fit!(m)                                # Train the model\n    preds = predict(m, X_test)             # Predict on test set\n    probs = [pdf(p, \"1\") for p in preds]   # Get probability of class \"1\"\n    auc_val = auc(roc(probs, y_test .== \"1\"))  # Compute AUC\n    return auc_val\nend\n```\n\n### Logistic Regression\n\n```julia\n# Logistic Regression with L1 regularization\nlog_model = MLJLinearModels.LogisticClassifier(penalty=:l1, lambda=0.0428)\nauc_log = evaluate_model(log_model, X_train, y_train, X_test, y_test)\n```\n\n### Random Forest\n\n```julia\n# Random Forest with 100 trees\nrf_model = MLJDecisionTreeInterface.RandomForestClassifier(n_trees=100)\nauc_rf = evaluate_model(rf_model, X_train, y_train, X_test, y_test)\n```\n\n### XGBoost\n\n```julia\n# XGBoost with 100 boosting rounds and depth 5\nxgb_model = MLJXGBoostInterface.XGBoostClassifier(num_round=100, max_depth=5)\nauc_xgb = evaluate_model(xgb_model, X_train, y_train, X_test, y_test)\n```\n\nEach model is trained on the same training set and evaluated on the same test set using AUC (Area Under the ROC Curve), which the paper uses as the primary metric for discrimination. This approach ensures fair comparison between models and helps identify the most suitable one for the task.\n\n<br>\n<center>\n  ![](./binary_classification.png)\n\n  Binary Classification Model\n</center>\n\n# Wrapping Up\n\nIn this post, we:\n\n- Built patient-level features from OMOP CDM using DuckDB SQL\n- Labeled outcomes based on cohort definitions\n- Preprocessed the data for ML tasks\n- Trained and evaluated 3 ML models using MLJ\n\nIn Part 3, Iâ€™ll wrap up with key lessons learned throughout the pipeline development - from working with real-world structured health data to implementing scalable PLP workflows in Julia. Iâ€™ll also reflect on what worked well, what could be improved, and propose directions for future enhancements. The final post will also share tips for those looking to build similar pipelines using OMOP CDM and Julia's rich ecosystem.\n\nStay tuned!\n\n## Acknowledgements\n\nThanks to Jacob Zelko for his mentorship, clarity, and constant feedback throughout the project. I also thank the JuliaHealth community for building an ecosystem where composable science can thrive.\n\n[Jacob S. Zelko](https://jacobzelko.com): aka, [TheCedarPrince](https://github.com/TheCedarPrince)\n\n_Note: This blog post was drafted with the assistance of LLM technologies to support grammar, clarity and structure._\n\n",
    "supporting": [
      "plp-part2_files"
    ],
    "filters": [],
    "includes": {}
  }
}