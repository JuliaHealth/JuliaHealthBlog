{
  "hash": "8919f8a02a6b7bbb5ae612863e1c18be",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"PLP-Pipeline Series Part 2: From Raw Clinical Data to Predictive Models\"\ndescription: \"Part 2 of the PLP-Pipeline blog series â€“ how we preprocess OMOP CDM data, extract features, and train ML models using Julia tools\"\nauthor: \"Kosuri Lakshmi Indu\"\ndate: \"4/20/2025\"\nbibliography: ./references.bib\ncsl: ./../../ieee-with-url.csl\ntoc: true\nengine: julia\nimage: false\ncategories:\n  - patient-level prediction\n  - omop cdm\n  - observational health\n  - feature engineering\n  - preprocessing\n  - machine learning\n---\n\n\n\n# Introduction ðŸ‘‹\n\nWelcome back to Part 2 of the PLP-Pipeline blog series!\n\nIn [Part 1](../indu-plp-part1plp-part1.qmd), we formulated a research question, loaded synthetic patient data, and constructed our **target (hypertension)** and **outcome (diabetes)** cohorts using OHDSI definitions and Julia tools. If you havenâ€™t read that yet, I recommend checking it out before diving in here.\n\nNow in Part 2, we move from **cohorts to models** - i.e., weâ€™ll bridge the gap between structured clinical data and predictive modeling. Specifically, weâ€™ll walk through:\n\n- Extracting patient-level features using cohort definitions\n- Performing preprocessing (imputation, encoding, normalization)\n- Splitting data into train/test sets\n- Training ML models using MLJ.jl\n\n# Reference: Foundation from the OHDSI PLP Framework\n\nAs mentioned in Part 1, my work follows the methodology from:\n\n> Reps, J. M., Schuemie, M. J., Suchard, M. A., Ryan, P. B., Rijnbeek, P. R., & Madigan, D. (2018). Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data. *Journal of the American Medical Informatics Association, 25(8), 969â€“975*. [https://doi.org/10.1093/jamia/ocy032](https://doi.org/10.1093/jamia/ocy032)\n\nThis paper outlines best practices for building patient-level prediction pipelines from observational health data. It emphasizes:\n\n- Extracting predictors from multiple OMOP CDM domains\n- Applying standardized time windows (e.g., 365 days prior to index)\n- Aggregating using counts, max values, or categorical indicators\n- Building reproducible pipelines\n\nI have translated these principles into Julia using DuckDB and MLJ.\n\n# Step 1: Feature Engineering from OMOP CDM\n\nWe extract features from several structured OMOP CDM domains such as:\n\n- `condition_occurrence`\n- `drug_exposure`\n- `procedure_occurrence`\n- `observation`\n- `measurement`\n- `person`\n\nThese features are computed for each patient in the **target cohort** using a **365-day window prior to the cohort start date**, aligning with the standardized methodology described in the OHDSI PLP framework by Reps et al. (JAMIA 2018).\n\nEach domain is queried independently, and the results are merged on `subject_id` to create a patient-level feature matrix for modeling.\n\n### Example: Drug Exposure Feature Extraction\n\nThe query below summarizes drug history for each patient using several aggregations:\n\n- **`drug_count`**: number of distinct drug classes (using `concept_ancestor` for generalization)\n- **`total_days_supply`**: total days covered by prescriptions\n- **`total_quantity`**: total quantity of drugs supplied\n- **`max_common_route`**: most frequent route of administration\n\n**File:** `feature_extraction.jl`\n\n```julia\n# Drug exposure features: summarizing drug history in the past 365 days\ndrugs_query = \"\"\"\nSELECT \n    c.subject_id, \n\n    # Count of distinct parent-level drug concepts\n    COUNT(DISTINCT ca.ancestor_concept_id) AS drug_count,\n\n    # Sum of days supply to estimate treatment duration\n    SUM(de.days_supply) AS total_days_supply,\n\n    # Sum of quantity to capture volume of medication used\n    SUM(de.quantity) AS total_quantity,\n\n    # Most common route of administration\n    MAX(de.route_concept_id) AS max_common_route\n\nFROM dbt_synthea_dev.cohort c\n\n# Join drug exposures for each subject\nJOIN dbt_synthea_dev.drug_exposure de \n    ON c.subject_id = de.person_id\n\n# Map each drug to a higher-level concept using the concept hierarchy\nJOIN dbt_synthea_dev.concept_ancestor ca \n    ON de.drug_concept_id = ca.descendant_concept_id\n\n# Filter to target cohort (e.g., patients with hypertension)\nWHERE c.cohort_definition_id = 1\n\n# Limit events to the 365 days before the cohort start date\nAND de.drug_exposure_start_date BETWEEN c.cohort_start_date - INTERVAL 365 DAY AND c.cohort_start_date\n\n# Aggregate by subject\nGROUP BY c.subject_id\n\"\"\"\n```\n\nThis approach is repeated across other domains (conditions, observations, etc.) using similar logic-adhering to the paperâ€™s recommendation to generate temporal, interpretable, and structured patient features from OMOP CDM.\n\n\n# Step 2: Attaching Outcome Labels\n\nNow that we have extracted features for the target population (patients diagnosed with hypertension), the next step is to attach outcome labels that indicate whether a patient later developed diabetes.\n\nWe use **Cohort 2**, which contains patients from the hypertension cohort who were subsequently diagnosed with diabetes, as defined using `OHDSICohortExpressions.jl`. This ensures that diabetes occurs *after* the hypertension event, maintaining proper temporal ordering between target and outcome.\n\nWe query the `cohort` table for all subjects in Cohort 2 and assign them an outcome label of `1`.\n\n**File:** `outcome_attach.jl`\n\n```julia\ndiabetes_query = \"\"\"\n    SELECT subject_id, 1 AS outcome\n    FROM dbt_synthea_dev.cohort\n    WHERE cohort_definition_id = 2\n\"\"\"\ndiabetes_df = execute(conn, diabetes_query) |> DataFrame\n```\n\nNext, we perform a **left join** with the previously created features_df, which contains covariates for all patients in the hypertension cohort. This ensures every patient has their features preserved, and only those present in the diabetes cohort will have a 1 under the outcome column.\n\nFor patients not found in the outcome cohort, the join results in a missing value. We treat these as negative cases (0), meaning the patient did not develop diabetes during the follow-up period.\n\n```julia\ndf = leftjoin(features_df, diabetes_df, on=:subject_id)\ndf[!, :outcome] .= coalesce.(df[!, :outcome], 0)\n```\nThis gives us a binary classification dataset with:\n\n- **1** â†’ patient developed diabetes after hypertension\n- **0** â†’ patient did not develop diabetes (or not within the observed period)\n\nThis labeled dataset is now ready for preprocessing and model training in later steps.\n\n<br>\n<center>\n  ![](./binary_classification.png)\n\n  Binary Classification Model\n</center>\n\n# Step 3: Preprocessing for Modeling\n\nOnce we have the raw feature matrix and outcome labels, the next crucial step is to preprocess the dataset to make it suitable for training machine learning models. This step is essential because raw healthcare data, even when structured via the OMOP CDM, often contains missing values, variable scales, and unencoded categorical information. These issues can affect model performance, stability, and interpretability.\n\nThe preprocessing approach we use is directly inspired by best practices discussed in the JAMIA paper. The authors highlight the importance of handling missing values, standardizing predictors, and transforming categorical data into usable formats.\n\nWe perform the following three tasks:\n\n- Handling missing values\n- Standardizing numeric features\n- Encoding categorical variables\n\n**File:** `preprocessing.jl`\n\n```julia\n# Impute missing values\nfor col in names(df)\n    if eltype(df[!, col]) <: Union{Missing, Number}\n        df[!, col] = coalesce.(df[!, col], 0)\n    else\n        df[!, col] = coalesce.(df[!, col], \"unknown\")\n    end\nend\n\n# Standardize numerical columns\nfor col in [:age, :condition_count, :drug_count]\n    Î¼, Ïƒ = mean(skipmissing(df[!, col])), std(skipmissing(df[!, col]))\n    df[!, col] .= (df[!, col] .- Î¼) ./ Ïƒ\nend\n\n# Encode categoricals\ndf.gender_concept_id = categorical(df.gender_concept_id)\ndf.race_concept_id = categorical(df.race_concept_id)\n```\n\n**Train-Test Splitting**\n\nFinally, we split the data into training and testing sets using an 80-20 split. This ensures we can evaluate how well the model generalizes to unseen data.\n\n```julia\nusing MLJ\ntrain, test = partition(eachindex(df.outcome), 0.8, shuffle=true)\n```\n<br>\n<center>\n  ![](./train_test_splitting.png)\n\n  Train-Test splitting (80% - 20%)\n</center>\n\n# Step 4: Model Training with MLJ.jl\n\nWe evaluated three machine learning models to identify the best-performing approach for predicting whether a patient develops diabetes. These models are consistent with those evaluated in the original JAMIA paper, which highlights the importance of testing multiple classifiers for each prediction problem. The paper specifically mentions using logistic regression (with L1 regularization) , random forest, and gradient boosting machines (like XGBoost) as part of their standardized framework.\n\nThe three models we implemented in Julia are:\n\n- **L1-Regularized Logistic Regression**: This model applies L1 (lasso) regularization to perform both classification and feature selection. It is particularly useful when working with a large number of sparse features, as is common in observational healthcare data. The JAMIA paper used L1-regularized logistic regression as a primary model due to its strong baseline performance and interpretability.\n\n- **Random Forest**: A tree-based ensemble model that builds multiple decision trees and averages their predictions. It handles non-linear relationships and is robust to overfitting. The paper includes random forest to capture interactions and non-linearities in clinical data that logistic regression might miss.\n\n- **XGBoost (Extreme Gradient Boosting)**: A powerful boosting algorithm that builds trees sequentially to correct previous errors. It is known for high predictive accuracy and was also one of the models tested in the paper for comparative performance analysis.\n\n**File:** `train_model.jl`\n\n### Shared Evaluation Function\n\n```julia\nusing MLJLinearModels, MLJDecisionTreeInterface, MLJXGBoostInterface, ROCAnalysis\n\n# Generic function to evaluate a classifier using AUC\nfunction evaluate_model(model, X_train, y_train, X_test, y_test)\n    m = machine(model, X_train, y_train)   # Bind model with data\n    fit!(m)                                # Train the model\n    preds = predict(m, X_test)             # Predict on test set\n    probs = [pdf(p, \"1\") for p in preds]   # Get probability of class \"1\"\n    auc_val = auc(roc(probs, y_test .== \"1\"))  # Compute AUC\n    return auc_val\nend\n```\n\n### Logistic Regression\n\n```julia\n# Logistic Regression with L1 regularization\nlog_model = MLJLinearModels.LogisticClassifier(penalty=:l1, lambda=0.0428)\nauc_log = evaluate_model(log_model, X_train, y_train, X_test, y_test)\n```\n\n### Random Forest\n\n```julia\n# Random Forest with 100 trees\nrf_model = MLJDecisionTreeInterface.RandomForestClassifier(n_trees=100)\nauc_rf = evaluate_model(rf_model, X_train, y_train, X_test, y_test)\n```\n\n### XGBoost\n\n```julia\n# XGBoost with 100 boosting rounds and depth 5\nxgb_model = MLJXGBoostInterface.XGBoostClassifier(num_round=100, max_depth=5)\nauc_xgb = evaluate_model(xgb_model, X_train, y_train, X_test, y_test)\n```\n\nEach model is trained on the same training set and evaluated on the same test set using AUC (Area Under the ROC Curve), which the paper uses as the primary metric for discrimination. This approach ensures fair comparison between models and helps identify the most suitable one for the task.\n\n# Wrapping Up\n\nIn this post, we:\n\n- Built patient-level features from OMOP CDM using DuckDB SQL\n- Labeled outcomes based on cohort definitions\n- Preprocessed the data for ML tasks\n- Trained and evaluated 3 ML models using MLJ\n\nIn Part 3, Iâ€™ll wrap up with key lessons learned throughout the pipeline development - from working with real-world structured health data to implementing scalable PLP workflows in Julia. Iâ€™ll also reflect on what worked well, what could be improved, and propose directions for future enhancements. The final post will also share tips for those looking to build similar pipelines using OMOP CDM and Julia's rich ecosystem.\n\nStay tuned!\n\n## Acknowledgements\n\nThanks to Jacob Zelko for his mentorship, clarity, and constant feedback throughout the project. I also thank the JuliaHealth community for building an ecosystem where composable science can thrive.\n\n[Jacob S. Zelko](https://jacobzelko.com): aka, [TheCedarPrince](https://github.com/TheCedarPrince)\n\n_Note: This blog post was drafted with the assistance of LLM technologies to support grammar, clarity and structure._\n\n",
    "supporting": [
      "plp-part2_files"
    ],
    "filters": [],
    "includes": {}
  }
}