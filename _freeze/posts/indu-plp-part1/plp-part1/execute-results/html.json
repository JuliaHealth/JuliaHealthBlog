{
  "hash": "bbd619c0a5c9249006d46e8fdbb35256",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"PLP-Pipeline Series Part 1: From Research Question to Cohort Construction\"\ndescription: \"Kicking off the PLP-Pipeline blog series - how we define research questions and construct cohorts using OMOP CDM and Julia tools.\"\nauthor: \"Kosuri Lakshmi Indu\"\ndate: \"4/12/2025\"\nbibliography: ./references.bib\ncsl: ./../../ieee-with-url.csl\ntoc: true\nengine: julia\nimage: false\ncategories:\n  - patient-level prediction\n  - omop cdm\n  - observational health\n---\n\n\n\n# Introduction ðŸ‘‹\n\nHi everyone! Iâ€™m **Kosuri Lakshmi Indu**, a third-year undergraduate student in Computer Science and an aspiring GSoC 2025 contributor. My interest in using data science for public health led me to the **JuliaHealth** community and, under the mentorship of **Jacob S. Zelko**, I began working on a project titled **PLP-Pipeline**. This project focuses on building modular, efficient tooling for **Patient-Level Prediction (PLP)** entirely in **Julia**, using the **OMOP Common Data Model (CDM)**.\n\nIn this post, Iâ€™ll walk through the first part of a three-part blog series documenting my work on building a Patient-Level Prediction (PLP) pipeline in Julia. Each post focuses on a different stage of the pipeline:\n\n1. **From Research Question to Cohort Construction (this post)**\n\n2. From Raw Clinical Data to Predictive Models\n\n3. Lessons Learned, Key Challenges, and What Comes Next\n\nIn Part 1, weâ€™ll start at the very beginning-formulating the research question, exploring the OMOP Common Data Model (CDM), setting up the local database, and defining target and outcome cohorts using Julia tools. Whether you're a health researcher, a GSoC aspirant, or a Julia enthusiast, I hope this gives you a clear and accessible introduction to how observational health research can be made more composable, reproducible, and efficient using Julia.\n\nYou can find my [**PLP-Pipeline Project Link Here**](https://github.com/kosuri-indu/PLP-Pipeline)\n\n[**LinkedIn**](https://www.linkedin.com/in/kosuri-indu/) | [**GitHub**](https://github.com/kosuri-indu/)\n \n# Background\n\n## What is Observational Health?\n\nObservational health research involves studying patient data collected in real-world settings, typically through electronic health records (EHRs), claims databases, or registries. Unlike clinical trials, which are controlled and experimental, observational studies allow researchers to analyze the effects of treatments, interventions, and disease progressions in a more naturalistic setting. The advantage of observational health research is that it provides a vast, diverse dataset that reflects how treatments and conditions unfold in everyday clinical practice.\n\nHowever, analyzing observational health data can be challenging due to its complexity, unstructured nature, and biases inherent in real-world data. This is where standardized data models like OMOP CDM come into play.\n\n## What is OMOP CDM?\n\nThe **Observational Medical Outcomes Partnership Common Data Model (OMOP CDM)** is a standardized framework for organizing and analyzing observational healthcare data. The OMOP CDM converts diverse sources of health data into a common format that supports large-scale, systematic analysis.\n\nOMOP CDM organizes data into a consistent set of relational tables like `condition_occurrence`, `drug_exposure`, `person`, `visit_occurrence` etc, using standardized vocabularies. These tables are interconnected, allowing for relational analysis across a patient's medical history.\n\nBy transforming diverse healthcare datasets into a common format, OMOP CDM enables reproducibility, interoperability, and large-scale studies across institutions and populations.\n\n<br>\n<center>\n  ![](./omopcdm.png)\n\n  OMOP Common Data Model\n</center>\n\n## What is Patient-Level Prediction (PLP)?\n\n**Patient-Level Prediction (PLP)** refers to the use of machine learning or statistical models to estimate the likelihood of a specific clinical outcome for an individual patient, based on their personal medical history and characteristics.\n\nThe key goal of PLP is to answer personalized clinical questions like:\n\n> *\"For a patient who recently had a hospital visit for chest pain, what is their risk of experiencing a heart attack?\"*\n\nPLP leverages patient data such as diagnoses, medications, procedures, and lifestyle factors and applies machine learning models to estimate the risk of future health events. Unlike traditional studies that provide population-level insights, PLP aims to provide targeted predictions that can help inform individual clinical decisions.\n\n## Why PLP in Julia?\n\nTraditional PLP workflows often involve switching between several tools and languages: using SQL for data extraction, R for cohort definition, and Python for machine learning. This fragmented approach leads to inefficiencies and challenges in reproducibility, scalability, and maintainability.\n\n**Julia** offers a compelling alternative by enabling end-to-end PLP pipelines within a single, high-performance language. It brings together:\n\n- Composability: Julia makes it easy to develop modular, reusable components, enabling clear and extendable PLP pipelines.\n\n- Speed: Julia's performance is comparable to C, making it ideal for handling large healthcare datasets, which are often complex and voluminous.\n\n- Unified Ecosystem: Julia integrates packages such as OHDSICohortExpressions.jl for cohort definitions, MLJ.jl for machine learning, and DataFrames.jl for structured data handling, enabling a seamless end-to-end PLP pipeline within one language.\n\nWith Julia, it becomes possible to define, build, and evaluate PLP pipelines in a clean, cohesive, and reproducible manner-making it an ideal language for modern health informatics research.\n\n<br>\n<center>\n  ![](./julia.webp)\n\n  Julia Equivalents\n</center>\n\n# Research Question\n\nThe core research question we aim to address in this PLP pipeline is:\n\n> **Among patients diagnosed with hypertension, who will go on to develop diabetes?**\n\nThis question is of paramount importance in clinical decision-making, as it helps identify high-risk individuals who may benefit from early intervention to prevent or manage diabetes. The challenge in answering this question lies in accurately identifying the patients with hypertension who are at risk of progressing to diabetes, based on their medical history and risk factors.\n\n## Cohort Construction\n\nCohorts are groups of patients defined by specific criteria that are relevant to the research question. For this task, two main cohorts need to be defined:\n\n- **Target Cohort**: This refers to the group of patients we want to make predictions for. In our case, it includes patients who have been diagnosed with hypertension. These patients serve as the starting point for our prediction timeline.\n\n- **Outcome Cohort**: This refers to the clinical event we aim to predict. In our case, it includes patients from the target cohort who are subsequently diagnosed with diabetes within a specified time window. This event marks the outcome that our model will learn to forecast.\n\nThese cohort definitions are central to structuring the data pipeline, as they form the foundation for downstream tasks like feature extraction, model training, and evaluation.\n\n# Defining Cohorts using OHDSICohortExpressions.jl\n\nIn the context of this research, I received a **20GB synthetic dataset** that contains **1,115,000 fake patients** (1,000,000 alive and 115,000 deceased), each with **3 years of medical history**. This dataset was loaded into a DuckDB database, a lightweight, high-performance analytical database that allows fast querying of large datasets directly from local files without the need for a server.\n\nFor cohort creation, I used OHDSI cohort definitions provided directly by my mentor in the form of two JSON files: **Hypertension.json** (for the target cohort) and **Diabetes.json** (for the outcome cohort). To execute them, I used the OHDSICohortExpressions.jl package to convert the JSON definitions into SQL queries, which were then run against the DuckDB database to extract the relevant cohorts.\n\nHereâ€™s the breakdown of the process:\n\n- Reading the cohort definitions from JSON files.\n\n- Connecting to DuckDB, which stores the synthetic patient data.\n\n- Translating the cohort definitions into SQL using OHDSICohortExpressions.jl.\n\n- Executing the SQL queries to create the target and outcome cohorts in the database.\n\n### Cohort Definition Code\n\nHereâ€™s how we set up the database connection and load the Parquet files into DuckDB for querying:\n\n**File:** `cohort_definition.jl`\n\n```julia\nimport DBInterface: connect, execute\nimport FunSQL: reflect, render\nimport OHDSICohortExpressions: translate\nusing DuckDB, DataFrames\n\n# Read the cohort definitions from JSON files (Hypertension and Diabetes definitions)\ntarget_json = read(datadir(\"exp_raw\", \"definitions\", \"Hypertension.json\"), String)  # Target cohort (Hypertension)\noutcome_json = read(datadir(\"exp_raw\", \"definitions\", \"Diabetes.json\"), String)  # Outcome cohort (Diabetes)\n\n# Establish a connection to the DuckDB database\nconnection = connect(DuckDB.DB, datadir(\"exp_raw\", \"synthea_1M_3YR.duckdb\"))\n\n# Function to process a cohort definition (translate the JSON to SQL and execute)\nfunction process_cohort(def_json, def_id, conn)\n  catalog = reflect(conn; schema=\"dbt_synthea_dev\", dialect=:duckdb)  # Reflect the database schema\n  fun_sql = translate(def_json; cohort_definition_id=def_id)  # Translate the JSON to SQL query\n  sql = render(catalog, fun_sql)  # Render the SQL query\n\n  # Execute the SQL query to insert cohort data into the database\n  execute(conn, \"\"\"\n  INSERT INTO dbt_synthea_dev.cohort\n  SELECT * FROM ($sql) AS foo;\n  \"\"\")\nend\n\n# Process the target and outcome cohorts\nprocess_cohort(target_json, 1, connection)  # Define the target cohort (Hypertension)\nprocess_cohort(outcome_json, 2, connection)  # Define the outcome cohort (Diabetes)\n\nclose!(connection)\n```\n\nThis code uses FunSQL.jl and OHDSICohortExpressions.jl to translate and render OHDSI ATLAS cohort definitions into executable SQL for DuckDB. The translate() function from OHDSICohortExpressions.jl converts the JSON cohort definitions (Hypertension and Diabetes) into a FunSQL query representation. Then, reflect() is used to introspect the DuckDB schema, and render() from FunSQL.jl turns the abstract query into concrete SQL. The process_cohort() function executes this SQL using DBInterface.execute() to insert the resulting cohort data into the cohort table. This pipeline allows OHDSI cohort logic to be ported directly into a local DuckDB environment without relying on external OHDSI tools.\n\n\n# Wrapping Up\n\nThis post covered the foundations of the PLP pipeline:\n\n- Explored observational health research, OMOP CDM, PLP, and Julia for large-scale clinical data analysis.\n\n- Formulated the research question: predicting diabetes progression in hypertension patients.\n\n- Explained OMOP CDM's role in standardizing clinical data.\n\n- Defined target and outcome cohorts for the study.\n\n- Used Julia to convert cohort definitions into executable SQL for DuckDB querying.\n\nIn the next post, Iâ€™ll walk through how we go from raw clinical data to predictive modeling, with Julia code examples that highlight feature extraction, data processing, and model training-bringing the full PLP pipeline to life.\n\n## Acknowledgements\n\nThanks to Jacob Zelko for his mentorship, clarity, and constant feedback throughout the project. I also thank the JuliaHealth community for building an ecosystem where composable science can thrive.\n\n[Jacob S. Zelko](https://jacobzelko.com): aka, [TheCedarPrince](https://github.com/TheCedarPrince)\n\n_Note: This blog post was drafted with the assistance of LLM technologies to support grammar, clarity and structure._\n\n",
    "supporting": [
      "plp-part1_files"
    ],
    "filters": [],
    "includes": {}
  }
}