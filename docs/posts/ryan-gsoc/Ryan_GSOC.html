<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ryan Kierulf">
<meta name="dcterms.date" content="2024-08-30">
<meta name="description" content="A summary of my project for Google Summer of Code">

<title>GSoC ’24: Enhancements to KomaMRI.jl GPU Support – The JuliaHealth Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dbcbc950a42bcaf185fdba3d0ea6113f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-21ab05999d9ba4f3397b27295ec92d09.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script data-goatcounter="https://juliahealthblog.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="GSoC ’24: Enhancements to KomaMRI.jl GPU Support – The JuliaHealth Blog">
<meta property="og:description" content="A summary of my project for Google Summer of Code">
<meta property="og:site_name" content="The JuliaHealth Blog">
<meta property="og:locale" content="en_EN">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../profile.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">The JuliaHealth Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Write with Us</span>
    </span>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-join-juliahealth" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Join JuliaHealth</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-join-juliahealth">    
        <li>
    <a class="dropdown-item" href="https://julialang.org/slack/"><i class="bi bi-slack" role="img">
</i> 
 <span class="dropdown-text">Slack (#health-and-medicine)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://julialang.zulipchat.com/"><i class="bi bi-lightning-charge-fill" role="img">
</i> 
 <span class="dropdown-text">Julia Zulip</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://discourse.julialang.org/"><i class="bi bi-pencil-square" role="img">
</i> 
 <span class="dropdown-text">Julia Discourse</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/JuliaHealth/JuliaHealthBlog">
 <span class="dropdown-text">Source Code</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/JuliaHealth/JuliaHealthBlog/issues/new/choose">
 <span class="dropdown-text">Report a Bug</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<b>Navigation &amp; Tips:</b>

<ul>
    <li>Press <b>?</b> for search</li>
    <li><b>"&lt;/&gt; Code"</b> for code nav</li>
</ul>

</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#hi" id="toc-hi" class="nav-link active" data-scroll-target="#hi">Hi! 👋</a></li>
  <li><a href="#what-is-komamri" id="toc-what-is-komamri" class="nav-link" data-scroll-target="#what-is-komamri">What is KomaMRI?</a></li>
  <li><a href="#project-goals" id="toc-project-goals" class="nav-link" data-scroll-target="#project-goals">Project Goals</a></li>
  <li><a href="#step-1-support-for-different-gpu-backends" id="toc-step-1-support-for-different-gpu-backends" class="nav-link" data-scroll-target="#step-1-support-for-different-gpu-backends">Step 1: Support for Different GPU backends</a></li>
  <li><a href="#step-2-buildkite-ci" id="toc-step-2-buildkite-ci" class="nav-link" data-scroll-target="#step-2-buildkite-ci">Step 2: Buildkite CI</a></li>
  <li><a href="#step-3-optimization" id="toc-step-3-optimization" class="nav-link" data-scroll-target="#step-3-optimization">Step 3: Optimization</a></li>
  <li><a href="#step-4-distributed-support" id="toc-step-4-distributed-support" class="nav-link" data-scroll-target="#step-4-distributed-support">4. Step 4: Distributed Support</a></li>
  <li><a href="#conclusions-future-work" id="toc-conclusions-future-work" class="nav-link" data-scroll-target="#conclusions-future-work">Conclusions / Future Work</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/JuliaHealth/JuliaHealthBlog/edit/main/posts/ryan-gsoc/Ryan_GSOC.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/JuliaHealth/JuliaHealthBlog/issues/new/choose" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">GSoC ’24: Enhancements to KomaMRI.jl GPU Support</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">gsoc</div>
    <div class="quarto-category">mri</div>
    <div class="quarto-category">gpu</div>
    <div class="quarto-category">hpc</div>
    <div class="quarto-category">simulation</div>
  </div>
  </div>

<div>
  <div class="description">
    A summary of my project for Google Summer of Code
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ryan Kierulf </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 30, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="hi" class="level1">
<h1>Hi! 👋</h1>
<p>I am Ryan, an MS student currently studying computer science at the University of Wisconsin-Madison. Looking for a project to work on this summer, my interest in high-performance computing and affinity for the Julia programming language drew me to Google Summer of Code, where I learned about this project opportunity to work on enhancing GPU support for KomaMRI.jl.</p>
<p>In this post, I’d like to summarize what I did this summer and everything I learned along the way!</p>
<blockquote class="blockquote">
<p>If you want to learn more about me, you can connect with me here: <a href="https://www.linkedin.com/in/ryan-kierulf-022062201/"><strong>LinkedIn</strong></a>, <a href="https://github.com/rkierulf"><strong>GitHub</strong></a></p>
</blockquote>
</section>
<section id="what-is-komamri" class="level1">
<h1>What is KomaMRI?</h1>
<p><a href="https://github.com/JuliaHealth/KomaMRI.jl">KomaMRI</a> is a Julia package for efficiently simulating Magnetic Resonance Imaging (MRI) acquisitions. MRI simulation is a useful tool for researchers, as it allows testing new pulse sequences to analyze the signal output and image reconstruction quality without needing to actually take an MRI, which may be time or cost-prohibitive.</p>
<p>In contrast to many other MRI simulators, KomaMRI.jl is open-source, cross-platform, and comes with an intuitive user interface (To learn more about KomaMRI, you can read the paper introducing it <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.29635">here</a>). However, being developed fairly recently, there are still new features that can be added and optimization to be done.</p>
</section>
<section id="project-goals" class="level1">
<h1>Project Goals</h1>
<p>The goals outlined by Carlos (my project mentor) and I the beginning of this summer were:</p>
<ol type="1">
<li><p>Extend GPU support beyond CUDA to include AMD, Intel, and Apple Silicon GPUs, through the packages <a href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a>, <a href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a>, and <a href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a></p></li>
<li><p>Create a CI pipeline to be able to test each of the GPU backends</p></li>
<li><p>Create a new kernel-based simulation method optimized for the GPU, which we expected would outperform array broadcasting</p></li>
<li><p>(Stretch Goal) Look into ways to support running distributed simulations across multiple nodes or GPUs</p></li>
</ol>
</section>
<section id="step-1-support-for-different-gpu-backends" class="level1">
<h1>Step 1: Support for Different GPU backends</h1>
<p>Previously, KomaMRI’s support for GPU acceleration worked by converting each array used within the simulation to a <code>CuArray</code>, the device array type defined in <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>. This was done through a general <code>gpu</code> function. The inner simulation code is GPU-agnostic, as the same operations can be performed on a CuArray or a plain CPU Array. This approach is good for extensibility, as it does not require writing different simulation code for the CPU / GPU, or different GPU backends, and would only work in a language like Julia based on runtime dispatch!</p>
<p>To extend this to multiple GPU backends, all that is needed is to generalize the <code>gpu</code> function to convert to either the device types of CUDA.jl, AMDGPU.jl, Metal.jl, or oneAPI.jl, depending on which backend is being used. To give an idea of what the gpu conversion code looked like before, here is a snippet:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">struct</span> KomaCUDAAdaptor <span class="kw">end</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">adapt_storage</span>(to<span class="op">::</span><span class="dt">KomaCUDAAdaptor</span>, x) <span class="op">=</span> CUDA.<span class="fu">cu</span>(x)</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">function</span> <span class="fu">gpu</span>(x)</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="fu">check_use_cuda</span>()</span>
<span id="cb1-6"><a href="#cb1-6"></a>    <span class="cf">return</span> use_cuda[] ? <span class="fu">fmap</span>(x <span class="op">-&gt;</span> <span class="fu">adapt</span>(<span class="fu">KomaCUDAAdaptor</span>(), x), x; exclude<span class="op">=</span>_isleaf) <span class="op">:</span> x</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="kw">end</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">#CPU adaptor</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="kw">struct</span> KomaCPUAdaptor <span class="kw">end</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="fu">adapt_storage</span>(to<span class="op">::</span><span class="dt">KomaCPUAdaptor</span>, x<span class="op">::</span><span class="dt">AbstractArray</span>) <span class="op">=</span> <span class="fu">adapt</span>(<span class="dt">Array</span>, x)</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="fu">adapt_storage</span>(to<span class="op">::</span><span class="dt">KomaCPUAdaptor</span>, x<span class="op">::</span><span class="dt">AbstractRange</span>) <span class="op">=</span> x</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="fu">cpu</span>(x) <span class="op">=</span> <span class="fu">fmap</span>(x <span class="op">-&gt;</span> <span class="fu">adapt</span>(<span class="fu">KomaCPUAdaptor</span>(), x), x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>fmap</code> function is from the package <code>Functors.jl</code> and can recursively apply a function to a struct tagged with <code>@functor</code>. The function being applied is <code>adapt</code> from <code>Adapt.jl</code>, which will call the lower-level <code>adapt_storage</code> function to actually convert to / from the device type. The second parameter to <code>adapt</code> is what is being adapted, and the first is what it is being adapted to, which in this case is a custom adapter struct <code>KomaCUDAAdapter</code>.</p>
<p>One possible approach to generalize to different backends would be to define additional adapter structs for each backend and corresponding <code>adapt_storage</code> functions. This is what the popular machine learning library <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> does. However, there is a simpler way!</p>
<p>Each backend package (CUDA.jl, Metal.jl, etc.) already defines <code>adapt_storage</code> functions for converting different types to / from corresponding device type. Reusing these functions is preferable to defining our own since, not only does it save work, but it allows us to rely on the expertise of the developers who wrote those packages! If there is an issue with types being converted incorrectly that is fixed in one of those packages, then we would not need to update our code to get this fix since we are using the definitions they created.</p>
<p>Our final <code>gpu</code> and <code>cpu</code> functions are very simple. The <code>backend</code> parameter is a type derived from the abstract <code>Backend</code> type of <a href="https://github.com/JuliaGPU/KernelAbstractions.jl"><code>KernelAbstractions.jl</code></a>, which is extended by each of the backend packages:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> <span class="bu">KernelAbstractions</span> as KA</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="kw">function</span> <span class="fu">gpu</span>(x, backend<span class="op">::</span><span class="dt">KA.GPU</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>    <span class="cf">return</span> <span class="fu">fmap</span>(x <span class="op">-&gt;</span> <span class="fu">adapt</span>(backend, x), x; exclude<span class="op">=</span>_isleaf)</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="kw">end</span></span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="fu">cpu</span>(x) <span class="op">=</span> <span class="fu">fmap</span>(x <span class="op">-&gt;</span> <span class="fu">adapt</span>(KA.<span class="fu">CPU</span>(), x), x, exclude<span class="op">=</span>_isleaf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The other work needed to generalize our GPU support involved switching to use <a href="https://pkgdocs.julialang.org/v1/creating-packages/#Conditional-loading-of-code-in-packages-(Extensions)">package extensions</a> to avoid having each of the backend packages as an explicit dependency, and defining some basic GPU functions for backend selection and printing information about available GPU devices. The pull request for adding support for multiple backends is linked below:</p>
<blockquote class="blockquote">
<p>https://github.com/JuliaHealth/KomaMRI.jl/pull/405</p>
</blockquote>
</section>
<section id="step-2-buildkite-ci" class="level1">
<h1>Step 2: Buildkite CI</h1>
<p>At the time the above pull request was merged, we weren’t sure whether the added support for AMD and Intel GPUs actually worked, since we only had access to CUDA and Apple Silicon GPUs. So the next step was to set up a CI to test each GPU backend. To do this, we used <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">Buildkite</a>, which is a CI platform that many other Julia packages also use. Since there were many examples to follow, setting up our testing pipeline was not too difficult. Each step of the pipeline does the required environment setup and then calls <code>Pkg.test()</code> for KomaMRICore. As an example, here is what the AMDGPU step of our pipeline looks like:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource {yml} number-lines code-with-copy"><code class="sourceCode"><span id="cb3-1"><a href="#cb3-1"></a>      - label: "AMDGPU: Run tests on v{{matrix.version}}"</span>
<span id="cb3-2"><a href="#cb3-2"></a>        matrix:</span>
<span id="cb3-3"><a href="#cb3-3"></a>          setup:</span>
<span id="cb3-4"><a href="#cb3-4"></a>            version:</span>
<span id="cb3-5"><a href="#cb3-5"></a>              - "1"</span>
<span id="cb3-6"><a href="#cb3-6"></a>        plugins:</span>
<span id="cb3-7"><a href="#cb3-7"></a>          - JuliaCI/julia#v1:</span>
<span id="cb3-8"><a href="#cb3-8"></a>              version: "{{matrix.version}}"</span>
<span id="cb3-9"><a href="#cb3-9"></a>          - JuliaCI/julia-coverage#v1:</span>
<span id="cb3-10"><a href="#cb3-10"></a>              codecov: true</span>
<span id="cb3-11"><a href="#cb3-11"></a>              dirs:</span>
<span id="cb3-12"><a href="#cb3-12"></a>                - KomaMRICore/src</span>
<span id="cb3-13"><a href="#cb3-13"></a>                - KomaMRICore/ext</span>
<span id="cb3-14"><a href="#cb3-14"></a>        command: |</span>
<span id="cb3-15"><a href="#cb3-15"></a>          julia -e 'println("--- :julia: Instantiating project")</span>
<span id="cb3-16"><a href="#cb3-16"></a>              using Pkg</span>
<span id="cb3-17"><a href="#cb3-17"></a>              Pkg.develop([</span>
<span id="cb3-18"><a href="#cb3-18"></a>                  PackageSpec(path=pwd(), subdir="KomaMRIBase"),</span>
<span id="cb3-19"><a href="#cb3-19"></a>                  PackageSpec(path=pwd(), subdir="KomaMRICore"),</span>
<span id="cb3-20"><a href="#cb3-20"></a>              ])'</span>
<span id="cb3-21"><a href="#cb3-21"></a>          </span>
<span id="cb3-22"><a href="#cb3-22"></a>          julia --project=KomaMRICore/test -e 'println("--- :julia: Add AMDGPU to test environment")</span>
<span id="cb3-23"><a href="#cb3-23"></a>              using Pkg</span>
<span id="cb3-24"><a href="#cb3-24"></a>              Pkg.add("AMDGPU")'</span>
<span id="cb3-25"><a href="#cb3-25"></a>          </span>
<span id="cb3-26"><a href="#cb3-26"></a>          julia -e 'println("--- :julia: Running tests")</span>
<span id="cb3-27"><a href="#cb3-27"></a>              using Pkg</span>
<span id="cb3-28"><a href="#cb3-28"></a>              Pkg.test("KomaMRICore"; coverage=true, test_args=["AMDGPU"])'</span>
<span id="cb3-29"><a href="#cb3-29"></a>        agents:</span>
<span id="cb3-30"><a href="#cb3-30"></a>          queue: "juliagpu"</span>
<span id="cb3-31"><a href="#cb3-31"></a>          rocm: "*"</span>
<span id="cb3-32"><a href="#cb3-32"></a>        timeout_in_minutes: 60</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We also decided that in addition to a testing CI, it would also be helpful to have a benchmarking CI to track performance changes resulting from each commit to the main branch of the repository. <a href="https://github.com/LuxDL/Lux.jl">Lux.jl</a> had a very nice-looking benchmarking page, so I decided to look into their approach. They were using <a href="https://github.com/benchmark-action/github-action-benchmark">github-action-benchmark</a>, a popular benchmarking action that integrates with the Julia package <a href="https://github.com/JuliaCI/BenchmarkTools.jl"><code>BenchmarkTools.jl</code></a>. github-action-benchmark does two very useful things:</p>
<ol type="1">
<li><p>Collects benchmarking data into a json file and provides a default index.html to display this data. If put inside a relative path in the gh-pages branch of a repository, this results in a public benchmarking page which is automatically updated after each commit!</p></li>
<li><p>Comments on a pull request with the benchmarking results compared with before the pull request. Example: https://github.com/JuliaHealth/KomaMRI.jl/pull/442#pullrequestreview-2213921334</p></li>
</ol>
<p>The only issue was that since github-action-benchmark is a github action, it is meant to be run within github by one of the available github runners. While this works for CPU benchmarking, only Buildkite has the CI setup for each of the GPU backends we are using, and Lux.jl’s benchmarks page only included CPU benchmarks, not GPU benchmarks (Note: we talked with Avik, the repository owner of Lux.jl, and Lux.jl has since adopted the approach outlined below to display GPU and CPU benchmarks together). I was not able to find any examples of other julia packages using github-action-benchmark for GPU benchmarking.</p>
<p>Fortunately, there is a tool someone developed to download results from Buildkite into a github action (https://github.com/EnricoMi/download-buildkite-artifact-action). This repository only had 1 star when I found it, but it does exactly what we needed: it identifies the corresponding Buildkite build for a commit, waits for it to finish, and then downloads the artifacts for the build into the github action it is being run from. With this, we were able to download the Buildkite benchmark results from a final aggregation step into our benchmarking action and upload to github-action-benchmark to publish to either the main data.js file for our benchmarking website, or pull request.</p>
<p>Our final benchmarking page looks like this and is <a href="https://juliahealth.org/KomaMRI.jl/benchmarks/">publicly accessible</a>:</p>
<p><img src="./Benchmark_Page.png" class="img-fluid"></p>
<p>One neat thing about github-action-benchmark is that the default index.html is extensible, so even though by deault it only shows time, the information for memory usage and number of allocations is also collected into the json file, and can be displayed as well.</p>
<p>A successful CI run on Buildkite Looks like <a href="https://buildkite.com/julialang/komamri-dot-jl/builds/925">this</a>:</p>
<p><img src="./CI_Run.png" class="img-fluid"></p>
<p>The pull requests for creating the CI testing and benchmarking pipeline, and changing the index.html for our benchmark page are listed below:</p>
<ol type="1">
<li>https://github.com/JuliaHealth/KomaMRI.jl/pull/411</li>
<li>https://github.com/JuliaHealth/KomaMRI.jl/pull/418</li>
<li>https://github.com/JuliaHealth/KomaMRI.jl/pull/421</li>
</ol>
</section>
<section id="step-3-optimization" class="level1">
<h1>Step 3: Optimization</h1>
<p>With support for multiple backends enabled, and a robust CI, the next step was to optimize our simulation code as much as possible. Our original idea was to create a new GPU-optimized simulation method, but before doing this we wanted to look more at the existing code and optimize for the CPU.</p>
<p>The simulation code is solving a differential equation (the [Bloch equations(https://en.wikipedia.org/wiki/Bloch_equations)]) over time. Most differential equation solvers step through time, updating the current state at each time step, but our previous simulation code, more optimized for the GPU, did a lot of computations across all time points in a simulation block, allocating a matrix of size <code>Nspins by NΔt</code> each time this was done. Although this is beneficial for the GPU, where there are millions of threads available on which to parallelize these computations, for the CPU it is more important to conserve memory, and the aforementioned approach of stepping through time is preferable.</p>
<p>After seeing that this approach did help speed up simulation time on the CPU, but was not faster on the GPU (7x slower for Metal!) we decided to separate our simulation code for the GPU and CPU, dispatching based on the <code>KernelAbstractions.Backend</code> type depending on if it is <code>&lt;:KernelAbstractions.CPU</code> or <code>&lt;:KernelAbstractions.GPU</code>.</p>
<p>Other things we were able to do to speed up CPU computation time:</p>
<ol type="1">
<li><p>Preallocating each array used inside the core simulation code so it can be re-used from one simulation block to the next.</p></li>
<li><p><a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/BlochCPU.jl#L90">Skipping an expensive computation</a> if the magnetization at that time point is not added to the final signal</p></li>
<li><p>Ensuring that each statement is fully broadcasted. We were surprised to see the difference between the following examples:</p></li>
</ol>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">#Fast</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>Bz <span class="op">=</span> x <span class="op">.*</span> seq.Gx<span class="op">'</span> <span class="op">.+</span> y <span class="op">.*</span> seq.Gy<span class="op">'</span> <span class="op">.+</span> z <span class="op">.*</span> seq.Gz<span class="op">'</span> <span class="op">.+</span> p.Δw <span class="op">./</span> <span class="fu">T</span>(<span class="fl">2</span>π <span class="op">.*</span> γ)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co">#Slow</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>Bz <span class="op">=</span> x <span class="op">.*</span> seq.Gx<span class="op">'</span> <span class="op">.+</span> y <span class="op">.*</span> seq.Gy<span class="op">'</span> <span class="op">.+</span> z <span class="op">.*</span> seq.Gz<span class="op">'</span> <span class="op">.+</span> p.Δw <span class="op">/</span> <span class="fu">T</span>(<span class="fl">2</span>π <span class="op">*</span> γ)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="4" type="1">
<li>Using the <code>cis</code> function for complex exponentiation, which is faster than <code>exp</code></li>
</ol>
<p>With these changes, the mean improvement in simulation time aggregating across each of our benchmarks for 1, 2, 4, and 8 CPU threads was ~4.28. For 1 thread, the average improvement in memory usage was 90x!</p>
<p>The next task was optimizing the simulation code for the GPU. Although our original idea was to put everything into one GPU kernel, we found that the existing broadcasting operations were already very fast, and that custom kernels we wrote were not able to outperform the previous implementation. The Julia GPU compiler team deserves a lot of credit for developing such fast broadcasting implementations!</p>
<p>However, this does not mean that we were unable to improve the GPU simulation time. Similar to with the CPU, preallocation made a substantial difference. Parallelizing as much work as possible across the time points for a simulation block was also found to beneficial. For the parts that needed to be done sequentially, a <a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/KernelFunctions.jl#L5">custom GPU kernel</a> was written which used the <code>KernelAbstractions.@localmem</code> macro for arrays being updated at each time step to yield faster memory access.</p>
<p>The mean speedup we saw across the 4 supported GPU backends was 4.16, although this varied accross each backend (for example, CUDA was only 2.66x faster while oneAPI was 28x faster). There is a <a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/BlochGPU.jl#L151">remaining bottleneck</a> in the <code>run_spin_preceession!</code> function having to do with logical indexing that I was not able to resolve, but could be solved in the future to speed up the GPU simulation time even further!</p>
<p>The pull requests optimizing code for the CPU and GPU are below:</p>
<ol type="1">
<li><p>https://github.com/JuliaHealth/KomaMRI.jl/pull/443</p></li>
<li><p>https://github.com/JuliaHealth/KomaMRI.jl/pull/459</p></li>
<li><p>https://github.com/JuliaHealth/KomaMRI.jl/pull/462</p></li>
</ol>
</section>
<section id="step-4-distributed-support" class="level1">
<h1>4. Step 4: Distributed Support</h1>
<p>This last step was a stretch goal for exploring how to add distributed support to KomaMRI. MRI simulations can become quite large, so it is useful to be able to distribute work across either multiple GPUs or multiple compute nodes.</p>
<p>A nice thing about MRI simulation is the independent spin property: if a phantom object (representing, for example a brain tissue slice) is divided into two parts, and each part is simulated separately, the signal result from simulating the whole phantom will be equal to the sum of the signal results from simulating each subdivision of the original phantom. This makes it quite easy to distribute work, either across more than one GPU or accross multiple compute nodes.</p>
<p>The following scripts worked, with the only necessary code change to the repository being a new + function to add two RawAcquisitionData structs:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#Use multiple GPUs:</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">using</span> <span class="bu">Distributed</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">using</span> <span class="bu">CUDA</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co">#Add workers based on the number of available devices</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="fu">addprocs</span>(<span class="fu">length</span>(<span class="fu">devices</span>()))</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co">#Define inputs on each worker process</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="pp">@everywhere</span> <span class="cf">begin</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>    <span class="im">using</span> <span class="bu">KomaMRI</span>, <span class="bu">CUDA</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>    sys <span class="op">=</span> <span class="fu">Scanner</span>()</span>
<span id="cb5-12"><a href="#cb5-12"></a>    seq <span class="op">=</span> PulseDesigner.<span class="fu">EPI_example</span>()</span>
<span id="cb5-13"><a href="#cb5-13"></a>    obj <span class="op">=</span> <span class="fu">brain_phantom2D</span>()</span>
<span id="cb5-14"><a href="#cb5-14"></a>    <span class="co">#Divide phantom</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>    parts <span class="op">=</span> <span class="fu">kfoldperm</span>(<span class="fu">length</span>(obj), <span class="fu">nworkers</span>())</span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="cf">end</span></span>
<span id="cb5-17"><a href="#cb5-17"></a></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">#Distribute simulation across workers</span></span>
<span id="cb5-19"><a href="#cb5-19"></a>raw <span class="op">=</span> <span class="bu">Distributed</span>.<span class="pp">@distributed</span> (<span class="op">+</span>) <span class="cf">for</span> i<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fu">nworkers</span>()</span>
<span id="cb5-20"><a href="#cb5-20"></a>    KomaMRICore.<span class="fu">set_device!</span>(i<span class="op">-</span><span class="fl">1</span>) <span class="co">#Sets device for this worker, note that CUDA devices are indexed from 0</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>    <span class="fu">simulate</span>(obj[parts[i]], seq, sys)</span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1"></a><span class="co">#Use multiple compute nodes</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">using</span> <span class="bu">Distributed</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="im">using</span> <span class="bu">ClusterManagers</span></span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co">#Add workers based on the specified number of SLURM tasks</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="fu">addprocs</span>(<span class="fu">SlurmManager</span>(<span class="fu">parse</span>(<span class="dt">Int</span>, <span class="cn">ENV</span>[<span class="st">"SLURM_NTASKS"</span>])))</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co">#Define inputs on each worker process</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="pp">@everywhere</span> <span class="cf">begin</span></span>
<span id="cb6-10"><a href="#cb6-10"></a>    <span class="im">using</span> <span class="bu">KomaMRI</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>    sys <span class="op">=</span> <span class="fu">Scanner</span>()</span>
<span id="cb6-12"><a href="#cb6-12"></a>    seq <span class="op">=</span> PulseDesigner.<span class="fu">EPI_example</span>()</span>
<span id="cb6-13"><a href="#cb6-13"></a>    obj <span class="op">=</span> <span class="fu">brain_phantom2D</span>()</span>
<span id="cb6-14"><a href="#cb6-14"></a>    parts <span class="op">=</span> <span class="fu">kfoldperm</span>(<span class="fu">length</span>(obj), <span class="fu">nworkers</span>())</span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="cf">end</span></span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="co">#Distribute simulation across workers</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>raw <span class="op">=</span> <span class="bu">Distributed</span>.<span class="pp">@distributed</span> (<span class="op">+</span>) <span class="cf">for</span> i<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fu">nworkers</span>()</span>
<span id="cb6-19"><a href="#cb6-19"></a>    <span class="fu">simulate</span>(obj[parts[i]], seq, sys)</span>
<span id="cb6-20"><a href="#cb6-20"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Pull reqeust for adding these examples to the KomaMRI documentation: https://github.com/JuliaHealth/KomaMRI.jl/pull/468</p>
</section>
<section id="conclusions-future-work" class="level1">
<h1>Conclusions / Future Work</h1>
<p>This project was a 350-hour large project, since there were many goals to accomplish. To summarize what changed since the beginning of the project:</p>
<ol type="1">
<li><p>Added support for AMDGPU.jl, Metal.jl, and oneAPI.jl GPU backends</p></li>
<li><p>CI for automated testing and benchmarking accross each backend + <a href="https://juliahealth.org/KomaMRI.jl/benchmarks/">public benchmarks page</a></p></li>
<li><p>Significantly faster CPU and GPU performance</p></li>
<li><p>Demonstrated distributed support and examples added in documentation</p></li>
</ol>
<p>Future work could look at ways to further optimize the simulation code, since despite the progress made, I believe there is more work to be done! The aforementioned logical indexing issue is still not resolved, and the kernel used inside the <code>run_spin_excitation!</code> function has not been profiled in depth. KomaMRI is also looking into adding support for higher-order ODE methods, which could require more GPU kernels being written.</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>I would like to thank my mentor, Carlos Castillo, for his help and support on this project. I would also like to thank Jakub Mitura, who attended some of our meetings to help with GPU optimization, Dilum Aluthge who helped set up our BuildKite pipeline, and Tim Besard, who answered many GPU-related questions that Carlos and I had.</p>


<!-- -->

</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{kierulf2024,
  author = {Kierulf, Ryan},
  title = {GSoC ’24: {Enhancements} to {KomaMRI.jl} {GPU} {Support}},
  date = {2024-08-30},
  url = {https://juliahealth.org/JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-kierulf2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Kierulf, Ryan. 2024. <span>“GSoC ’24: Enhancements to KomaMRI.jl GPU
Support.”</span> August 30, 2024. <a href="https://juliahealth.org/JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html">https://juliahealth.org/JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/juliahealth\.org\/JuliaHealthBlog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="JuliaHealth/JuliaHealthBlog" issue-term="title" theme="github-dark" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="an">title:</span><span class="co"> "GSoC '24: Enhancements to KomaMRI.jl GPU Support"</span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="an">description:</span><span class="co"> "A summary of my project for Google Summer of Code"</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="an">author:</span><span class="co"> "Ryan Kierulf"</span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="an">date:</span><span class="co"> "8/30/2024"</span></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="an">engine:</span><span class="co"> julia</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="an">image:</span><span class="co"> false</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="an">categories:</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co">  - gsoc</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co">  - mri</span></span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="co">  - gpu</span></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="co">  - hpc</span></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co">  - simulation</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="co">---</span></span>
<span id="cb7-16"><a href="#cb7-16"></a></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="fu"># Hi! 👋</span></span>
<span id="cb7-18"><a href="#cb7-18"></a></span>
<span id="cb7-19"><a href="#cb7-19"></a>I am Ryan, an MS student currently studying computer science at the University of Wisconsin-Madison. Looking for a project to work on this summer, my interest in high-performance computing and affinity for the Julia programming language drew me to Google Summer of Code, where I learned about this project opportunity to work on enhancing GPU support for KomaMRI.jl. </span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a>In this post, I'd like to summarize what I did this summer and everything I learned along the way!</span>
<span id="cb7-22"><a href="#cb7-22"></a></span>
<span id="cb7-23"><a href="#cb7-23"></a><span class="at">&gt; If you want to learn more about me, you can connect with me here: </span><span class="co">[</span><span class="ot">**LinkedIn**</span><span class="co">](https://www.linkedin.com/in/ryan-kierulf-022062201/)</span><span class="at">, </span><span class="co">[</span><span class="ot">**GitHub**</span><span class="co">](https://github.com/rkierulf)</span></span>
<span id="cb7-24"><a href="#cb7-24"></a></span>
<span id="cb7-25"><a href="#cb7-25"></a><span class="fu"># What is KomaMRI?</span></span>
<span id="cb7-26"><a href="#cb7-26"></a></span>
<span id="cb7-27"><a href="#cb7-27"></a><span class="co">[</span><span class="ot">KomaMRI</span><span class="co">](https://github.com/JuliaHealth/KomaMRI.jl)</span> is a Julia package for efficiently simulating Magnetic Resonance Imaging (MRI) acquisitions. MRI simulation is a useful tool for researchers, as it allows testing new pulse sequences to analyze the signal output and image reconstruction quality without needing to actually take an MRI, which may be time or cost-prohibitive.</span>
<span id="cb7-28"><a href="#cb7-28"></a></span>
<span id="cb7-29"><a href="#cb7-29"></a>In contrast to many other MRI simulators, KomaMRI.jl is open-source, cross-platform, and comes with an intuitive user interface (To learn more about KomaMRI, you can read the paper introducing it <span class="co">[</span><span class="ot">here</span><span class="co">](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.29635)</span>). However, being developed fairly recently, there are still new features that can be added and optimization to be done.</span>
<span id="cb7-30"><a href="#cb7-30"></a></span>
<span id="cb7-31"><a href="#cb7-31"></a><span class="fu"># Project Goals</span></span>
<span id="cb7-32"><a href="#cb7-32"></a></span>
<span id="cb7-33"><a href="#cb7-33"></a>The goals outlined by Carlos (my project mentor) and I the beginning of this summer were:</span>
<span id="cb7-34"><a href="#cb7-34"></a></span>
<span id="cb7-35"><a href="#cb7-35"></a><span class="ss">1. </span>Extend GPU support beyond CUDA to include AMD, Intel, and Apple Silicon GPUs, through the packages <span class="co">[</span><span class="ot">AMDGPU.jl</span><span class="co">](https://github.com/JuliaGPU/AMDGPU.jl)</span>, <span class="co">[</span><span class="ot">oneAPI.jl</span><span class="co">](https://github.com/JuliaGPU/oneAPI.jl)</span>, and <span class="co">[</span><span class="ot">Metal.jl</span><span class="co">](https://github.com/JuliaGPU/Metal.jl)</span></span>
<span id="cb7-36"><a href="#cb7-36"></a></span>
<span id="cb7-37"><a href="#cb7-37"></a><span class="ss">2. </span>Create a CI pipeline to be able to test each of the GPU backends</span>
<span id="cb7-38"><a href="#cb7-38"></a></span>
<span id="cb7-39"><a href="#cb7-39"></a><span class="ss">3. </span>Create a new kernel-based simulation method optimized for the GPU, which we expected would outperform array broadcasting</span>
<span id="cb7-40"><a href="#cb7-40"></a></span>
<span id="cb7-41"><a href="#cb7-41"></a><span class="ss">4. </span>(Stretch Goal) Look into ways to support running distributed simulations across multiple nodes or GPUs</span>
<span id="cb7-42"><a href="#cb7-42"></a></span>
<span id="cb7-43"><a href="#cb7-43"></a></span>
<span id="cb7-44"><a href="#cb7-44"></a><span class="fu"># Step 1: Support for Different GPU backends</span></span>
<span id="cb7-45"><a href="#cb7-45"></a></span>
<span id="cb7-46"><a href="#cb7-46"></a>Previously, KomaMRI's support for GPU acceleration worked by converting each array used within the simulation to a <span class="in">`CuArray`</span>, the device array type defined in <span class="co">[</span><span class="ot">CUDA.jl</span><span class="co">](https://github.com/JuliaGPU/CUDA.jl)</span>. This was done through a general <span class="in">`gpu`</span> function. The inner simulation code is GPU-agnostic, as the same operations can be performed on a CuArray or a plain CPU Array. This approach is good for extensibility, as it does not require writing different simulation code for the CPU / GPU, or different GPU backends, and would only work in a language like Julia based on runtime dispatch!</span>
<span id="cb7-47"><a href="#cb7-47"></a></span>
<span id="cb7-48"><a href="#cb7-48"></a>To extend this to multiple GPU backends, all that is needed is to generalize the <span class="in">`gpu`</span> function to convert to either the device types of CUDA.jl, AMDGPU.jl, Metal.jl, or oneAPI.jl, depending on which backend is being used. To give an idea of what the gpu conversion code looked like before, here is a snippet:</span>
<span id="cb7-49"><a href="#cb7-49"></a></span>
<span id="cb7-50"><a href="#cb7-50"></a><span class="in">```julia</span></span>
<span id="cb7-51"><a href="#cb7-51"></a><span class="in">struct KomaCUDAAdaptor end</span></span>
<span id="cb7-52"><a href="#cb7-52"></a><span class="in">adapt_storage(to::KomaCUDAAdaptor, x) = CUDA.cu(x)</span></span>
<span id="cb7-53"><a href="#cb7-53"></a></span>
<span id="cb7-54"><a href="#cb7-54"></a><span class="in">function gpu(x)</span></span>
<span id="cb7-55"><a href="#cb7-55"></a><span class="in">    check_use_cuda()</span></span>
<span id="cb7-56"><a href="#cb7-56"></a><span class="in">    return use_cuda[] ? fmap(x -&gt; adapt(KomaCUDAAdaptor(), x), x; exclude=_isleaf) : x</span></span>
<span id="cb7-57"><a href="#cb7-57"></a><span class="in">end</span></span>
<span id="cb7-58"><a href="#cb7-58"></a></span>
<span id="cb7-59"><a href="#cb7-59"></a><span class="in">#CPU adaptor</span></span>
<span id="cb7-60"><a href="#cb7-60"></a><span class="in">struct KomaCPUAdaptor end</span></span>
<span id="cb7-61"><a href="#cb7-61"></a><span class="in">adapt_storage(to::KomaCPUAdaptor, x::AbstractArray) = adapt(Array, x)</span></span>
<span id="cb7-62"><a href="#cb7-62"></a><span class="in">adapt_storage(to::KomaCPUAdaptor, x::AbstractRange) = x</span></span>
<span id="cb7-63"><a href="#cb7-63"></a></span>
<span id="cb7-64"><a href="#cb7-64"></a><span class="in">cpu(x) = fmap(x -&gt; adapt(KomaCPUAdaptor(), x), x)</span></span>
<span id="cb7-65"><a href="#cb7-65"></a><span class="in">```</span></span>
<span id="cb7-66"><a href="#cb7-66"></a></span>
<span id="cb7-67"><a href="#cb7-67"></a>The <span class="in">`fmap`</span> function is from the package <span class="in">`Functors.jl`</span> and can recursively apply a function to a struct tagged with <span class="in">`@functor`</span>. The function being applied is <span class="in">`adapt`</span> from <span class="in">`Adapt.jl`</span>, which will call the lower-level <span class="in">`adapt_storage`</span> function to actually convert to / from the device type. The second parameter to <span class="in">`adapt`</span> is what is being adapted, and the first is what it is being adapted to, which in this case is a custom adapter struct <span class="in">`KomaCUDAAdapter`</span>. </span>
<span id="cb7-68"><a href="#cb7-68"></a></span>
<span id="cb7-69"><a href="#cb7-69"></a>One possible approach to generalize to different backends would be to define additional adapter structs for each backend and corresponding <span class="in">`adapt_storage`</span> functions. This is what the popular machine learning library <span class="co">[</span><span class="ot">Flux.jl</span><span class="co">](https://github.com/FluxML/Flux.jl)</span> does. However, there is a simpler way!</span>
<span id="cb7-70"><a href="#cb7-70"></a></span>
<span id="cb7-71"><a href="#cb7-71"></a>Each backend package (CUDA.jl, Metal.jl, etc.) already defines <span class="in">`adapt_storage`</span> functions for converting different types to / from corresponding device type. Reusing these functions is preferable to defining our own since, not only does it save work, but it allows us to rely on the expertise of the developers who wrote those packages! If there is an issue with types being converted incorrectly that is fixed in one of those packages, then we would not need to update our code to get this fix since we are using the definitions they created.</span>
<span id="cb7-72"><a href="#cb7-72"></a></span>
<span id="cb7-73"><a href="#cb7-73"></a>Our final <span class="in">`gpu`</span> and <span class="in">`cpu`</span> functions are very simple. The <span class="in">`backend`</span> parameter is a type derived from the abstract <span class="in">`Backend`</span> type of <span class="co">[</span><span class="ot">`KernelAbstractions.jl`</span><span class="co">](https://github.com/JuliaGPU/KernelAbstractions.jl)</span>, which is extended by each of the backend packages:</span>
<span id="cb7-74"><a href="#cb7-74"></a></span>
<span id="cb7-75"><a href="#cb7-75"></a><span class="in">```julia</span></span>
<span id="cb7-76"><a href="#cb7-76"></a><span class="in">import KernelAbstractions as KA</span></span>
<span id="cb7-77"><a href="#cb7-77"></a></span>
<span id="cb7-78"><a href="#cb7-78"></a><span class="in">function gpu(x, backend::KA.GPU)</span></span>
<span id="cb7-79"><a href="#cb7-79"></a><span class="in">    return fmap(x -&gt; adapt(backend, x), x; exclude=_isleaf)</span></span>
<span id="cb7-80"><a href="#cb7-80"></a><span class="in">end</span></span>
<span id="cb7-81"><a href="#cb7-81"></a></span>
<span id="cb7-82"><a href="#cb7-82"></a><span class="in">cpu(x) = fmap(x -&gt; adapt(KA.CPU(), x), x, exclude=_isleaf)</span></span>
<span id="cb7-83"><a href="#cb7-83"></a><span class="in">```</span></span>
<span id="cb7-84"><a href="#cb7-84"></a></span>
<span id="cb7-85"><a href="#cb7-85"></a>The other work needed to generalize our GPU support involved switching to use <span class="co">[</span><span class="ot">package extensions</span><span class="co">]</span>(https://pkgdocs.julialang.org/v1/creating-packages/#Conditional-loading-of-code-in-packages-(Extensions)) to avoid having each of the backend packages as an explicit dependency, and defining some basic GPU functions for backend selection and printing information about available GPU devices. The pull request for adding support for multiple backends is linked below:</span>
<span id="cb7-86"><a href="#cb7-86"></a></span>
<span id="cb7-87"><a href="#cb7-87"></a><span class="at">&gt; https://github.com/JuliaHealth/KomaMRI.jl/pull/405</span></span>
<span id="cb7-88"><a href="#cb7-88"></a></span>
<span id="cb7-89"><a href="#cb7-89"></a><span class="fu"># Step 2: Buildkite CI</span></span>
<span id="cb7-90"><a href="#cb7-90"></a></span>
<span id="cb7-91"><a href="#cb7-91"></a>At the time the above pull request was merged, we weren't sure whether the added support for AMD and Intel GPUs actually worked, since we only had access to CUDA and Apple Silicon GPUs. So the next step was to set up a CI to test each GPU backend. To do this, we used <span class="co">[</span><span class="ot">Buildkite</span><span class="co">](https://github.com/JuliaGPU/KernelAbstractions.jl)</span>, which is a CI platform that many other Julia packages also use. Since there were many examples to follow, setting up our testing pipeline was not too difficult. Each step of the pipeline does the required environment setup and then calls <span class="in">`Pkg.test()`</span> for KomaMRICore. As an example, here is what the AMDGPU step of our pipeline looks like:</span>
<span id="cb7-92"><a href="#cb7-92"></a></span>
<span id="cb7-95"><a href="#cb7-95"></a><span class="in">```{yml}</span></span>
<span id="cb7-96"><a href="#cb7-96"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">label</span><span class="kw">:</span><span class="at"> </span><span class="st">"AMDGPU: Run tests on v{{matrix.version}}"</span></span>
<span id="cb7-97"><a href="#cb7-97"></a><span class="at">        </span><span class="fu">matrix</span><span class="kw">:</span></span>
<span id="cb7-98"><a href="#cb7-98"></a><span class="at">          </span><span class="fu">setup</span><span class="kw">:</span></span>
<span id="cb7-99"><a href="#cb7-99"></a><span class="at">            </span><span class="fu">version</span><span class="kw">:</span></span>
<span id="cb7-100"><a href="#cb7-100"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="st">"1"</span></span>
<span id="cb7-101"><a href="#cb7-101"></a><span class="at">        </span><span class="fu">plugins</span><span class="kw">:</span></span>
<span id="cb7-102"><a href="#cb7-102"></a><span class="at">          </span><span class="kw">-</span><span class="at"> </span>JuliaCI/julia<span class="co">#v1:</span></span>
<span id="cb7-103"><a href="#cb7-103"></a><span class="at">              </span><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="st">"{{matrix.version}}"</span></span>
<span id="cb7-104"><a href="#cb7-104"></a><span class="at">          </span><span class="kw">-</span><span class="at"> </span>JuliaCI/julia-coverage<span class="co">#v1:</span></span>
<span id="cb7-105"><a href="#cb7-105"></a><span class="at">              </span><span class="fu">codecov</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb7-106"><a href="#cb7-106"></a><span class="at">              </span><span class="fu">dirs</span><span class="kw">:</span></span>
<span id="cb7-107"><a href="#cb7-107"></a><span class="at">                </span><span class="kw">-</span><span class="at"> </span>KomaMRICore/src</span>
<span id="cb7-108"><a href="#cb7-108"></a><span class="at">                </span><span class="kw">-</span><span class="at"> </span>KomaMRICore/ext</span>
<span id="cb7-109"><a href="#cb7-109"></a><span class="fu">        command</span><span class="kw">: </span><span class="ch">|</span></span>
<span id="cb7-110"><a href="#cb7-110"></a>          julia -e 'println("--- :julia: Instantiating project")</span>
<span id="cb7-111"><a href="#cb7-111"></a>              using Pkg</span>
<span id="cb7-112"><a href="#cb7-112"></a>              Pkg.develop([</span>
<span id="cb7-113"><a href="#cb7-113"></a>                  PackageSpec(path=pwd(), subdir="KomaMRIBase"),</span>
<span id="cb7-114"><a href="#cb7-114"></a>                  PackageSpec(path=pwd(), subdir="KomaMRICore"),</span>
<span id="cb7-115"><a href="#cb7-115"></a>              ])'</span>
<span id="cb7-116"><a href="#cb7-116"></a>          </span>
<span id="cb7-117"><a href="#cb7-117"></a>          julia --project=KomaMRICore/test -e 'println("--- :julia: Add AMDGPU to test environment")</span>
<span id="cb7-118"><a href="#cb7-118"></a>              using Pkg</span>
<span id="cb7-119"><a href="#cb7-119"></a>              Pkg.add("AMDGPU")'</span>
<span id="cb7-120"><a href="#cb7-120"></a>          </span>
<span id="cb7-121"><a href="#cb7-121"></a>          julia -e 'println("--- :julia: Running tests")</span>
<span id="cb7-122"><a href="#cb7-122"></a>              using Pkg</span>
<span id="cb7-123"><a href="#cb7-123"></a>              Pkg.test("KomaMRICore"; coverage=true, test_args=["AMDGPU"])'</span>
<span id="cb7-124"><a href="#cb7-124"></a><span class="at">        </span><span class="fu">agents</span><span class="kw">:</span></span>
<span id="cb7-125"><a href="#cb7-125"></a><span class="at">          </span><span class="fu">queue</span><span class="kw">:</span><span class="at"> </span><span class="st">"juliagpu"</span></span>
<span id="cb7-126"><a href="#cb7-126"></a><span class="at">          </span><span class="fu">rocm</span><span class="kw">:</span><span class="at"> </span><span class="st">"*"</span></span>
<span id="cb7-127"><a href="#cb7-127"></a><span class="at">        </span><span class="fu">timeout_in_minutes</span><span class="kw">:</span><span class="at"> </span><span class="dv">60</span></span>
<span id="cb7-128"><a href="#cb7-128"></a><span class="in">```</span></span>
<span id="cb7-129"><a href="#cb7-129"></a></span>
<span id="cb7-130"><a href="#cb7-130"></a>We also decided that in addition to a testing CI, it would also be helpful to have a benchmarking CI to track performance changes resulting from each commit to the main branch of the repository. <span class="co">[</span><span class="ot">Lux.jl</span><span class="co">](https://github.com/LuxDL/Lux.jl)</span> had a very nice-looking benchmarking page, so I decided to look into their approach. They were using <span class="co">[</span><span class="ot">github-action-benchmark</span><span class="co">](https://github.com/benchmark-action/github-action-benchmark)</span>, a popular benchmarking action that integrates with the Julia package <span class="co">[</span><span class="ot">`BenchmarkTools.jl`</span><span class="co">](https://github.com/JuliaCI/BenchmarkTools.jl)</span>. github-action-benchmark does two very useful things:</span>
<span id="cb7-131"><a href="#cb7-131"></a></span>
<span id="cb7-132"><a href="#cb7-132"></a><span class="ss">1. </span>Collects benchmarking data into a json file and provides a default index.html to display this data. If put inside a relative path in the gh-pages branch of a repository, this results in a public benchmarking page which is automatically updated after each commit!</span>
<span id="cb7-133"><a href="#cb7-133"></a></span>
<span id="cb7-134"><a href="#cb7-134"></a><span class="ss">2. </span>Comments on a pull request with the benchmarking results compared with before the pull request. Example: https://github.com/JuliaHealth/KomaMRI.jl/pull/442#pullrequestreview-2213921334</span>
<span id="cb7-135"><a href="#cb7-135"></a></span>
<span id="cb7-136"><a href="#cb7-136"></a>The only issue was that since github-action-benchmark is a github action, it is meant to be run within github by one of the available github runners. While this works for CPU benchmarking, only Buildkite has the CI setup for each of the GPU backends we are using, and Lux.jl's benchmarks page only included CPU benchmarks, not GPU benchmarks (Note: we talked with Avik, the repository owner of Lux.jl, and Lux.jl has since adopted the approach outlined below to display GPU and CPU benchmarks together). I was not able to find any examples of other julia packages using github-action-benchmark for GPU benchmarking.</span>
<span id="cb7-137"><a href="#cb7-137"></a></span>
<span id="cb7-138"><a href="#cb7-138"></a>Fortunately, there is a tool someone developed to download results from Buildkite into a github action (https://github.com/EnricoMi/download-buildkite-artifact-action). This repository only had 1 star when I found it, but it does exactly what we needed: it identifies the corresponding Buildkite build for a commit, waits for it to finish, and then downloads the artifacts for the build into the github action it is being run from. With this, we were able to download the Buildkite benchmark results from a final aggregation step into our benchmarking action and upload to github-action-benchmark to publish to either the main data.js file for our benchmarking website, or pull request.</span>
<span id="cb7-139"><a href="#cb7-139"></a></span>
<span id="cb7-140"><a href="#cb7-140"></a>Our final benchmarking page looks like this and is <span class="co">[</span><span class="ot">publicly accessible</span><span class="co">](https://juliahealth.org/KomaMRI.jl/benchmarks/)</span>:</span>
<span id="cb7-141"><a href="#cb7-141"></a></span>
<span id="cb7-142"><a href="#cb7-142"></a><span class="al">![](./Benchmark_Page.png)</span></span>
<span id="cb7-143"><a href="#cb7-143"></a></span>
<span id="cb7-144"><a href="#cb7-144"></a>One neat thing about github-action-benchmark is that the default index.html is extensible, so even though by deault it only shows time, the information for memory usage and number of allocations is also collected into the json file, and can be displayed as well.</span>
<span id="cb7-145"><a href="#cb7-145"></a></span>
<span id="cb7-146"><a href="#cb7-146"></a>A successful CI run on Buildkite Looks like <span class="co">[</span><span class="ot">this</span><span class="co">](https://buildkite.com/julialang/komamri-dot-jl/builds/925)</span>:</span>
<span id="cb7-147"><a href="#cb7-147"></a></span>
<span id="cb7-148"><a href="#cb7-148"></a><span class="al">![](./CI_Run.png)</span></span>
<span id="cb7-149"><a href="#cb7-149"></a></span>
<span id="cb7-150"><a href="#cb7-150"></a>The pull requests for creating the CI testing and benchmarking pipeline, and changing the index.html for our benchmark page are listed below:</span>
<span id="cb7-151"><a href="#cb7-151"></a></span>
<span id="cb7-152"><a href="#cb7-152"></a><span class="ss">1. </span>https://github.com/JuliaHealth/KomaMRI.jl/pull/411</span>
<span id="cb7-153"><a href="#cb7-153"></a><span class="ss">2. </span>https://github.com/JuliaHealth/KomaMRI.jl/pull/418</span>
<span id="cb7-154"><a href="#cb7-154"></a><span class="ss">3. </span>https://github.com/JuliaHealth/KomaMRI.jl/pull/421</span>
<span id="cb7-155"><a href="#cb7-155"></a></span>
<span id="cb7-156"><a href="#cb7-156"></a><span class="fu"># Step 3: Optimization</span></span>
<span id="cb7-157"><a href="#cb7-157"></a></span>
<span id="cb7-158"><a href="#cb7-158"></a>With support for multiple backends enabled, and a robust CI, the next step was to optimize our simulation code as much as possible. Our original idea was to create a new GPU-optimized simulation method, but before doing this we wanted to look more at the existing code and optimize for the CPU. </span>
<span id="cb7-159"><a href="#cb7-159"></a></span>
<span id="cb7-160"><a href="#cb7-160"></a>The simulation code is solving a differential equation (the <span class="co">[</span><span class="ot">Bloch equations(https://en.wikipedia.org/wiki/Bloch_equations)</span><span class="co">]</span>) over time. Most differential equation solvers step through time, updating the current state at each time step, but our previous simulation code, more optimized for the GPU, did a lot of computations across all time points in a simulation block, allocating a matrix of size <span class="in">`Nspins by NΔt`</span> each time this was done. Although this is beneficial for the GPU, where there are millions of threads available on which to parallelize these computations, for the CPU it is more important to conserve memory, and the aforementioned approach of stepping through time is preferable.</span>
<span id="cb7-161"><a href="#cb7-161"></a></span>
<span id="cb7-162"><a href="#cb7-162"></a>After seeing that this approach did help speed up simulation time on the CPU, but was not faster on the GPU (7x slower for Metal!) we decided to separate our simulation code for the GPU and CPU, dispatching based on the <span class="in">`KernelAbstractions.Backend`</span> type depending on if it is <span class="in">`&lt;:KernelAbstractions.CPU`</span> or <span class="in">`&lt;:KernelAbstractions.GPU`</span>. </span>
<span id="cb7-163"><a href="#cb7-163"></a></span>
<span id="cb7-164"><a href="#cb7-164"></a>Other things we were able to do to speed up CPU computation time:</span>
<span id="cb7-165"><a href="#cb7-165"></a></span>
<span id="cb7-166"><a href="#cb7-166"></a><span class="ss">1. </span>Preallocating each array used inside the core simulation code so it can be re-used from one simulation block to the next.</span>
<span id="cb7-167"><a href="#cb7-167"></a></span>
<span id="cb7-168"><a href="#cb7-168"></a><span class="ss">2. </span><span class="co">[</span><span class="ot">Skipping an expensive computation</span><span class="co">](https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/BlochCPU.jl#L90)</span> if the magnetization at that time point is not added to the final signal</span>
<span id="cb7-169"><a href="#cb7-169"></a></span>
<span id="cb7-170"><a href="#cb7-170"></a><span class="ss">3. </span>Ensuring that each statement is fully broadcasted. We were surprised to see the difference between the following examples:</span>
<span id="cb7-171"><a href="#cb7-171"></a></span>
<span id="cb7-172"><a href="#cb7-172"></a><span class="in">```julia</span></span>
<span id="cb7-173"><a href="#cb7-173"></a><span class="in">#Fast</span></span>
<span id="cb7-174"><a href="#cb7-174"></a><span class="in">Bz = x .* seq.Gx' .+ y .* seq.Gy' .+ z .* seq.Gz' .+ p.Δw ./ T(2π .* γ)</span></span>
<span id="cb7-175"><a href="#cb7-175"></a></span>
<span id="cb7-176"><a href="#cb7-176"></a><span class="in">#Slow</span></span>
<span id="cb7-177"><a href="#cb7-177"></a><span class="in">Bz = x .* seq.Gx' .+ y .* seq.Gy' .+ z .* seq.Gz' .+ p.Δw / T(2π * γ)</span></span>
<span id="cb7-178"><a href="#cb7-178"></a><span class="in">```</span></span>
<span id="cb7-179"><a href="#cb7-179"></a></span>
<span id="cb7-180"><a href="#cb7-180"></a><span class="ss">4. </span>Using the <span class="in">`cis`</span> function for complex exponentiation, which is faster than <span class="in">`exp`</span></span>
<span id="cb7-181"><a href="#cb7-181"></a></span>
<span id="cb7-182"><a href="#cb7-182"></a>With these changes, the mean improvement in simulation time aggregating across each of our benchmarks for 1, 2, 4, and 8 CPU threads was ~4.28. For 1 thread, the average improvement in memory usage was 90x!</span>
<span id="cb7-183"><a href="#cb7-183"></a></span>
<span id="cb7-184"><a href="#cb7-184"></a>The next task was optimizing the simulation code for the GPU. Although our original idea was to put everything into one GPU kernel, we found that the existing broadcasting operations were already very fast, and that custom kernels we wrote were not able to outperform the previous implementation. The Julia GPU compiler team deserves a lot of credit for developing such fast broadcasting implementations!</span>
<span id="cb7-185"><a href="#cb7-185"></a></span>
<span id="cb7-186"><a href="#cb7-186"></a>However, this does not mean that we were unable to improve the GPU simulation time. Similar to with the CPU, preallocation made a substantial difference. Parallelizing as much work as possible across the time points for a simulation block was also found to beneficial. For the parts that needed to be done sequentially, a <span class="co">[</span><span class="ot">custom GPU kernel</span><span class="co">](https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/KernelFunctions.jl#L5)</span> was written which used the <span class="in">`KernelAbstractions.@localmem`</span> macro for arrays being updated at each time step to yield faster memory access.</span>
<span id="cb7-187"><a href="#cb7-187"></a></span>
<span id="cb7-188"><a href="#cb7-188"></a>The mean speedup we saw across the 4 supported GPU backends was 4.16, although this varied accross each backend (for example, CUDA was only 2.66x faster while oneAPI was 28x faster). There is a <span class="co">[</span><span class="ot">remaining bottleneck</span><span class="co">](https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/BlochGPU.jl#L151)</span> in the <span class="in">`run_spin_preceession!`</span> function having to do with logical indexing that I was not able to resolve, but could be solved in the future to speed up the GPU simulation time even further!</span>
<span id="cb7-189"><a href="#cb7-189"></a></span>
<span id="cb7-190"><a href="#cb7-190"></a>The pull requests optimizing code for the CPU and GPU are below:</span>
<span id="cb7-191"><a href="#cb7-191"></a></span>
<span id="cb7-192"><a href="#cb7-192"></a><span class="ss">1. </span>https://github.com/JuliaHealth/KomaMRI.jl/pull/443</span>
<span id="cb7-193"><a href="#cb7-193"></a></span>
<span id="cb7-194"><a href="#cb7-194"></a><span class="ss">2. </span>https://github.com/JuliaHealth/KomaMRI.jl/pull/459</span>
<span id="cb7-195"><a href="#cb7-195"></a></span>
<span id="cb7-196"><a href="#cb7-196"></a><span class="ss">3. </span>https://github.com/JuliaHealth/KomaMRI.jl/pull/462</span>
<span id="cb7-197"><a href="#cb7-197"></a></span>
<span id="cb7-198"><a href="#cb7-198"></a><span class="fu"># 4. Step 4: Distributed Support</span></span>
<span id="cb7-199"><a href="#cb7-199"></a></span>
<span id="cb7-200"><a href="#cb7-200"></a>This last step was a stretch goal for exploring how to add distributed support to KomaMRI. MRI simulations can become quite large, so it is useful to be able to distribute work across either multiple GPUs or multiple compute nodes.</span>
<span id="cb7-201"><a href="#cb7-201"></a></span>
<span id="cb7-202"><a href="#cb7-202"></a>A nice thing about MRI simulation is the independent spin property: if a phantom object (representing, for example a brain tissue slice) is divided into two parts, and each part is simulated separately, the signal result from simulating the whole phantom will be equal to the sum of the signal results from simulating each subdivision of the original phantom. This makes it quite easy to distribute work, either across more than one GPU or accross multiple compute nodes.</span>
<span id="cb7-203"><a href="#cb7-203"></a></span>
<span id="cb7-204"><a href="#cb7-204"></a>The following scripts worked, with the only necessary code change to the repository being a new + function to add two RawAcquisitionData structs:</span>
<span id="cb7-205"><a href="#cb7-205"></a></span>
<span id="cb7-206"><a href="#cb7-206"></a><span class="in">```julia</span></span>
<span id="cb7-207"><a href="#cb7-207"></a><span class="in">#Use multiple GPUs:</span></span>
<span id="cb7-208"><a href="#cb7-208"></a><span class="in">using Distributed</span></span>
<span id="cb7-209"><a href="#cb7-209"></a><span class="in">using CUDA</span></span>
<span id="cb7-210"><a href="#cb7-210"></a></span>
<span id="cb7-211"><a href="#cb7-211"></a><span class="in">#Add workers based on the number of available devices</span></span>
<span id="cb7-212"><a href="#cb7-212"></a><span class="in">addprocs(length(devices()))</span></span>
<span id="cb7-213"><a href="#cb7-213"></a></span>
<span id="cb7-214"><a href="#cb7-214"></a><span class="in">#Define inputs on each worker process</span></span>
<span id="cb7-215"><a href="#cb7-215"></a><span class="in">@everywhere begin</span></span>
<span id="cb7-216"><a href="#cb7-216"></a><span class="in">    using KomaMRI, CUDA</span></span>
<span id="cb7-217"><a href="#cb7-217"></a><span class="in">    sys = Scanner()</span></span>
<span id="cb7-218"><a href="#cb7-218"></a><span class="in">    seq = PulseDesigner.EPI_example()</span></span>
<span id="cb7-219"><a href="#cb7-219"></a><span class="in">    obj = brain_phantom2D()</span></span>
<span id="cb7-220"><a href="#cb7-220"></a><span class="in">    #Divide phantom</span></span>
<span id="cb7-221"><a href="#cb7-221"></a><span class="in">    parts = kfoldperm(length(obj), nworkers())</span></span>
<span id="cb7-222"><a href="#cb7-222"></a><span class="in">end</span></span>
<span id="cb7-223"><a href="#cb7-223"></a></span>
<span id="cb7-224"><a href="#cb7-224"></a><span class="in">#Distribute simulation across workers</span></span>
<span id="cb7-225"><a href="#cb7-225"></a><span class="in">raw = Distributed.@distributed (+) for i=1:nworkers()</span></span>
<span id="cb7-226"><a href="#cb7-226"></a><span class="in">    KomaMRICore.set_device!(i-1) #Sets device for this worker, note that CUDA devices are indexed from 0</span></span>
<span id="cb7-227"><a href="#cb7-227"></a><span class="in">    simulate(obj[parts[i]], seq, sys)</span></span>
<span id="cb7-228"><a href="#cb7-228"></a><span class="in">end</span></span>
<span id="cb7-229"><a href="#cb7-229"></a><span class="in">```</span></span>
<span id="cb7-230"><a href="#cb7-230"></a></span>
<span id="cb7-231"><a href="#cb7-231"></a><span class="in">```julia</span></span>
<span id="cb7-232"><a href="#cb7-232"></a><span class="in">#Use multiple compute nodes</span></span>
<span id="cb7-233"><a href="#cb7-233"></a><span class="in">using Distributed</span></span>
<span id="cb7-234"><a href="#cb7-234"></a><span class="in">using ClusterManagers</span></span>
<span id="cb7-235"><a href="#cb7-235"></a></span>
<span id="cb7-236"><a href="#cb7-236"></a><span class="in">#Add workers based on the specified number of SLURM tasks</span></span>
<span id="cb7-237"><a href="#cb7-237"></a><span class="in">addprocs(SlurmManager(parse(Int, ENV["SLURM_NTASKS"])))</span></span>
<span id="cb7-238"><a href="#cb7-238"></a></span>
<span id="cb7-239"><a href="#cb7-239"></a><span class="in">#Define inputs on each worker process</span></span>
<span id="cb7-240"><a href="#cb7-240"></a><span class="in">@everywhere begin</span></span>
<span id="cb7-241"><a href="#cb7-241"></a><span class="in">    using KomaMRI</span></span>
<span id="cb7-242"><a href="#cb7-242"></a><span class="in">    sys = Scanner()</span></span>
<span id="cb7-243"><a href="#cb7-243"></a><span class="in">    seq = PulseDesigner.EPI_example()</span></span>
<span id="cb7-244"><a href="#cb7-244"></a><span class="in">    obj = brain_phantom2D()</span></span>
<span id="cb7-245"><a href="#cb7-245"></a><span class="in">    parts = kfoldperm(length(obj), nworkers())</span></span>
<span id="cb7-246"><a href="#cb7-246"></a><span class="in">end</span></span>
<span id="cb7-247"><a href="#cb7-247"></a></span>
<span id="cb7-248"><a href="#cb7-248"></a><span class="in">#Distribute simulation across workers</span></span>
<span id="cb7-249"><a href="#cb7-249"></a><span class="in">raw = Distributed.@distributed (+) for i=1:nworkers()</span></span>
<span id="cb7-250"><a href="#cb7-250"></a><span class="in">    simulate(obj[parts[i]], seq, sys)</span></span>
<span id="cb7-251"><a href="#cb7-251"></a><span class="in">end</span></span>
<span id="cb7-252"><a href="#cb7-252"></a><span class="in">```</span></span>
<span id="cb7-253"><a href="#cb7-253"></a></span>
<span id="cb7-254"><a href="#cb7-254"></a>Pull reqeust for adding these examples to the KomaMRI documentation: https://github.com/JuliaHealth/KomaMRI.jl/pull/468</span>
<span id="cb7-255"><a href="#cb7-255"></a></span>
<span id="cb7-256"><a href="#cb7-256"></a><span class="fu"># Conclusions / Future Work</span></span>
<span id="cb7-257"><a href="#cb7-257"></a></span>
<span id="cb7-258"><a href="#cb7-258"></a>This project was a 350-hour large project, since there were many goals to accomplish. To summarize what changed since the beginning of the project:</span>
<span id="cb7-259"><a href="#cb7-259"></a></span>
<span id="cb7-260"><a href="#cb7-260"></a><span class="ss">1. </span>Added support for AMDGPU.jl, Metal.jl, and oneAPI.jl GPU backends</span>
<span id="cb7-261"><a href="#cb7-261"></a></span>
<span id="cb7-262"><a href="#cb7-262"></a><span class="ss">2. </span>CI for automated testing and benchmarking accross each backend + <span class="co">[</span><span class="ot">public benchmarks page</span><span class="co">](https://juliahealth.org/KomaMRI.jl/benchmarks/)</span></span>
<span id="cb7-263"><a href="#cb7-263"></a></span>
<span id="cb7-264"><a href="#cb7-264"></a><span class="ss">3. </span>Significantly faster CPU and GPU performance</span>
<span id="cb7-265"><a href="#cb7-265"></a></span>
<span id="cb7-266"><a href="#cb7-266"></a><span class="ss">4. </span>Demonstrated distributed support and examples added in documentation</span>
<span id="cb7-267"><a href="#cb7-267"></a></span>
<span id="cb7-268"><a href="#cb7-268"></a>Future work could look at ways to further optimize the simulation code, since despite the progress made, I believe there is more work to be done! The aforementioned logical indexing issue is still not resolved, and the kernel used inside the <span class="in">`run_spin_excitation!`</span> function has not been profiled in depth. KomaMRI is also looking into adding support for higher-order ODE methods, which could require more GPU kernels being written.</span>
<span id="cb7-269"><a href="#cb7-269"></a></span>
<span id="cb7-270"><a href="#cb7-270"></a><span class="fu"># Acknowledgements</span></span>
<span id="cb7-271"><a href="#cb7-271"></a></span>
<span id="cb7-272"><a href="#cb7-272"></a>I would like to thank my mentor, Carlos Castillo, for his help and support on this project. I would also like to thank Jakub Mitura, who attended some of our meetings to help with GPU optimization, Dilum Aluthge who helped set up our BuildKite pipeline, and Tim Besard, who answered many GPU-related questions that Carlos and I had.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, JuliaHealth.</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JuliaHealth/JuliaHealthBlog">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/c/TheJuliaLanguage">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://julialang.org/slack/">
      <i class="bi bi-slack" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/julialanguage">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/JuliaHealth/JuliaHealthBlog/edit/main/posts/ryan-gsoc/Ryan_GSOC.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/JuliaHealth/JuliaHealthBlog/issues/new/choose" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>