[
  {
    "objectID": "pages/related_organizations.html",
    "href": "pages/related_organizations.html",
    "title": "Related Organizations",
    "section": "",
    "text": "This is a (not necessarily comprehensive) list of organizations that focus primarily on developing and maintaining open-source Julia packages related to the life sciences and health sciences.\nIf you would like to add an organization to this list, please feel free to make a pull request."
  },
  {
    "objectID": "pages/related_organizations.html#julia-community-organizations",
    "href": "pages/related_organizations.html#julia-community-organizations",
    "title": "Related Organizations",
    "section": "Julia community organizations",
    "text": "Julia community organizations\n\nBioJulia – Biology, bioinformatics, and computational biology (website | Gitter)\nEcoJulia - Ecology (website)\nJuliaHealth – Medicine, health care, public health, and biomedical research (website)\nJuliaEpi – Epidemiology\nJuliaNeuro - Neuroscience (website)\nJuliaNeuroscience - Neuroscience\nMagneticResonanceImaging - Magnetic resonance imaging"
  },
  {
    "objectID": "pages/related_organizations.html#labs-and-research-groups",
    "href": "pages/related_organizations.html#labs-and-research-groups",
    "title": "Related Organizations",
    "section": "Labs and research groups",
    "text": "Labs and research groups\n\nBCBI – Center for Biomedical Informatics at Brown University (website)\nHoly Lab - Holy Lab at Washington University in St. Louis (website)\nInPhyT - Interdisciplinary Physics Team"
  },
  {
    "objectID": "pages/related_organizations.html#companies",
    "href": "pages/related_organizations.html#companies",
    "title": "Related Organizations",
    "section": "Companies",
    "text": "Companies\n\nBeacon Biosignals - Intelligent brain monitoring technologies (website)\nPumasAI - Pharmaceutical modeling and simulation (website)"
  },
  {
    "objectID": "pages/connect_with_us.html",
    "href": "pages/connect_with_us.html",
    "title": "Connect With Us",
    "section": "",
    "text": "Visit us on GitHub: https://github.com/JuliaHealth\nPost in the Biology, Health, and Medicine category on Discourse.\nJoin us in the #biology-health-and-medicine stream on Zulip.\nChat with us in the #health-and-medicine channel on Slack. (Get a Slack invite here.)"
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part2/plp-part2.html",
    "href": "JuliaHealthBlog/posts/indu-plp-part2/plp-part2.html",
    "title": "PLP-Pipeline Series Part 2: From Raw Clinical Data to Predictive Models",
    "section": "",
    "text": "Welcome back to Part 2 of the PLP-Pipeline blog series!\nIn Part 1, we formulated a research question, loaded synthetic patient data, and constructed our target (hypertension) and outcome (diabetes) cohorts using OHDSI definitions and Julia tools. If you haven’t read that yet, I recommend checking it out before diving in here.\nNow in Part 2, we move from cohorts to models - i.e., we’ll bridge the gap between structured clinical data and predictive modeling. Specifically, we’ll walk through:\n\nExtracting patient-level features using cohort definitions\nPerforming preprocessing (imputation, encoding, normalization)\nSplitting data into train/test sets\nTraining ML models using MLJ.jl\n\nThese steps will guide you through the prediction pipeline, aiming to predict the likelihood of a patient with hypertension developing diabetes."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part2/plp-part2.html#acknowledgements",
    "href": "JuliaHealthBlog/posts/indu-plp-part2/plp-part2.html#acknowledgements",
    "title": "PLP-Pipeline Series Part 2: From Raw Clinical Data to Predictive Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Jacob Zelko for his mentorship, clarity, and constant feedback throughout the project. I also thank the JuliaHealth community for building an ecosystem where composable science can thrive.\nJacob S. Zelko: aka, TheCedarPrince\nNote: This blog post was drafted with the assistance of LLM technologies to support grammar, clarity and structure."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "Welcome back to the final part of the blog series on building a Patient-Level Prediction (PLP) Pipeline in Julia using the OMOP Common Data Model (CDM).\nIn this concluding post, we’ll reflect on the full journey – from cohort definition and feature extraction to preprocessing, modeling, and evaluation. We’ll dig into what worked well, what challenges emerged, and what lessons were learned while building this pipeline from scratch using Julia. This post aims to bring the series full circle, offering insights into the practical realities of working with real-world health data and setting the stage for future work and improvements.\n\n\nIn Part 1, we introduced the motivation and core question driving the pipeline:\n\nAmong patients diagnosed with hypertension, who will go on to develop diabetes?\n\nWe chose this prediction problem because of its clinical relevance and because hypertension and diabetes are both common chronic conditions with strong associations in literature. To handle the data, we used the OMOP CDM format, which ensures that real-world patient data is structured in a consistent and analysis-friendly way.\nTo extract patient cohorts, we used OHDSICohortExpressions.jl, which allowed us to define concept sets and logic for cohort inclusion and exclusion.\nIn Part 2, we built a modular Patient-Level Prediction pipeline using the Julia ecosystem:\n\nFeature extraction: involved pulling demographic, condition, drug, and visit data, encoding it as binary presence indicators.\nPreprocessing: included handling missing values, one-hot encoding, normalization, and creating train-test splits.\nModel training: used the MLJ.jl framework to train logistic regression, random forest, and XGBoost models.\nEvaluation: involved computing AUC and accuracy for binary classification.\n\n\n\n\nThe OHDSI ecosystem, particularly its PatientLevelPrediction package in R, offers an end-to-end solution that integrates tightly with ATLAS and standardized vocabularies. It handles cohort creation, covariate extraction, modeling, and visualization all in one environment.\nBuilding this pipeline in Julia offered a solid balance of flexibility and control. While most tasks, such as cohort extraction and model evaluation, were efficiently handled using existing Julia packages, a few areas required more custom implementation. These instances, though limited, provided valuable learning experiences, deepening my understanding of how the components of the pipeline interact, how features are generated, and how decisions at each stage impact model performance.\n\n\n\nAfter preprocessing the features and labels, model performance was not as strong as expected. The AUC values for the classifiers were relatively low for the models:\n\nLogistic Regression (L1-regularized): AUC ≈ 0.097\nRandom Forest: AUC ≈ 0.485\nXGBoost: AUC ≈ 0.52\n\nThese unexpectedly low model performances prompted a deeper investigation into the root causes. The primary issue was that the data available in our synthetic OMOP CDM was not well-suited to answer the research question: “From those diagnosed with hypertension, who goes on to develop diabetes?”\nTemporal context was also missing, as the features extracted were basic binary indicators that did not reflect the timing or frequency of clinical events. The one-year prediction window might have been too narrow for diabetes to develop meaningfully after a hypertension diagnosis. Many patients had very limited observation time before their index date, which weakened the reliability of feature construction. This further constrained the ability of the models to generalize well to new cases.\nAdditionally, many patients did not match our cohort criteria or had insufficient data to form meaningful predictions. The dataset size may have been too small to detect strong patterns or relationships between hypertension and subsequent diabetes development, especially given that the synthetic dataset may not have represented real-world complexities fully.\nThese issues suggest that the bottleneck wasn’t the modeling itself, but rather the mismatch between the research question and the available data, especially within the constraints of synthetic data. The limited sample size and the difficulty in matching the right cohort to the research question further impacted the model’s ability to make reliable predictions.\n\n\n\nThroughout the process, we encountered several technical and conceptual challenges:\nThe most critical issue lay in the quality and structure of the data itself. Many patients had sparse or short observation periods, which meant that only limited clinical history was available before the cohort entry date.\nThis directly affected the utility of the extracted features. On the modeling side, integrating with MLJ.jl required careful setup, especially when handling missing values, categorical encodings, and class imbalance.\n\n\n\nMLJ.jl\n\n\nFeature engineering was another bottleneck, since most features were binary flags or simple counts that did not capture clinical nuance or temporal dynamics. Preprocessing steps like normalization and imputation also needed fine-tuning and often had to be manually adjusted per model.\n\n\n\nAs I look to enhance the pipeline, improving the code and modularizing it for reusability is a top priority. I plan to refine the existing functions and ensure that the pipeline can handle different types of OMOP CDM datasets, including exploring other tables that could enrich the features we extract. Additionally, I’m particularly interested in incorporating temporal-based prediction features, which would allow us to account for the sequence of patient events over time and improve model accuracy.\nOne direction I’m especially excited about is building interfaces in Julia to streamline the PLP process. Creating intuitive, modular tools that wrap around cohort creation, feature engineering, and modeling would make it easier for researchers to plug into OMOP CDM data and get started with predictive tasks. These interfaces could help reduce the complexity of working directly with lower-level tools, while still offering flexibility for customization.\n\n\n\nThis project focused on building patient-level prediction (PLP) models in Julia. While the initial models underperformed, it emphasized the importance of validated cohorts, thoughtful feature engineering, strong baselines, and interpretable results.\nThe goal is to develop this into a robust Julia-native package for PLP tasks, integrated with JuliaHealth tools and OMOP CDM datasets, offering flexibility for custom pipelines. Aligning with OHDSI definitions and adding diagnostics could make it a valuable research and educational resource.\nTo enhance interpretability, I’m exploring a visualization layer for PLP pipelines using tools like Makie.jl or VegaLite.jl. Example visualizations could include:\n\nTimeline plots to visualize index dates, lookback periods, and outcome windows per patient, aiding in temporal reasoning.\nFeature density plots to assess feature sparsity and guide preprocessing decisions.\nROC and PR curves to evaluate and compare model performance.\nFeature importance charts (especially for tree-based models like Random Forest or XGBoost) to help identify clinically relevant predictors.\n\nEventually, this project could serve as a foundation for a Julia-native PLP toolkit, or at least a template for others interested in working with observational health data in Julia.\n\n\n\nThis project highlighted that Julia is well-suited for observational health research workflows. Its strong data ecosystem (DataFrames.jl, DuckDB.jl, CSV.jl), rich machine learning interfaces (MLJ.jl), and increasing support from domain-specific packages like OHDSICohortExpressions.jl make it a compelling environment for building transparent, customizable pipelines. Unlike black-box tools, Julia allows full control over every stage-from data access to model tuning-encouraging deeper understanding and reproducibility.\nSome lessons I learnt on the way:\n\nOne big realization for me was that the cohort population should closely align with the research question. I ran into situations where the cohorts didn’t actually support the prediction goal, which led to misleading outputs. I’m still learning how to validate cohort logic better, but now I see how critical this alignment is.\nI also struggled a bit with preprocessing and data understanding. There were points where I wasn’t sure how to handle missing data or what specific fields really meant. A deeper data check early on could have helped guide better feature engineering.\nWhile I didn’t build everything manually, I feel like I could have better leveraged existing Julia tools for some common steps. Exploring the ecosystem more might have simplified things.\nFinally, I have learned that debugging and iteration are part of the process. Things rarely work on the first try, and often the biggest insights came from chasing unexpected results or poor model performance. It has made me more comfortable with being wrong and learning through trial."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#recap-of-previous-parts",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#recap-of-previous-parts",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "In Part 1, we introduced the motivation and core question driving the pipeline:\n\nAmong patients diagnosed with hypertension, who will go on to develop diabetes?\n\nWe chose this prediction problem because of its clinical relevance and because hypertension and diabetes are both common chronic conditions with strong associations in literature. To handle the data, we used the OMOP CDM format, which ensures that real-world patient data is structured in a consistent and analysis-friendly way.\nTo extract patient cohorts, we used OHDSICohortExpressions.jl, which allowed us to define concept sets and logic for cohort inclusion and exclusion.\nIn Part 2, we built a modular Patient-Level Prediction pipeline using the Julia ecosystem:\n\nFeature extraction: involved pulling demographic, condition, drug, and visit data, encoding it as binary presence indicators.\nPreprocessing: included handling missing values, one-hot encoding, normalization, and creating train-test splits.\nModel training: used the MLJ.jl framework to train logistic regression, random forest, and XGBoost models.\nEvaluation: involved computing AUC and accuracy for binary classification."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#reflections-on-the-ohdsi-framework-vs-julia-approach",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#reflections-on-the-ohdsi-framework-vs-julia-approach",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "The OHDSI ecosystem, particularly its PatientLevelPrediction package in R, offers an end-to-end solution that integrates tightly with ATLAS and standardized vocabularies. It handles cohort creation, covariate extraction, modeling, and visualization all in one environment.\nBuilding this pipeline in Julia offered a solid balance of flexibility and control. While most tasks, such as cohort extraction and model evaluation, were efficiently handled using existing Julia packages, a few areas required more custom implementation. These instances, though limited, provided valuable learning experiences, deepening my understanding of how the components of the pipeline interact, how features are generated, and how decisions at each stage impact model performance."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#model-performance",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#model-performance",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "After preprocessing the features and labels, model performance was not as strong as expected. The AUC values for the classifiers were relatively low for the models:\n\nLogistic Regression (L1-regularized): AUC ≈ 0.097\nRandom Forest: AUC ≈ 0.485\nXGBoost: AUC ≈ 0.52\n\nThese unexpectedly low model performances prompted a deeper investigation into the root causes. The primary issue was that the data available in our synthetic OMOP CDM was not well-suited to answer the research question: “From those diagnosed with hypertension, who goes on to develop diabetes?”\nTemporal context was also missing, as the features extracted were basic binary indicators that did not reflect the timing or frequency of clinical events. The one-year prediction window might have been too narrow for diabetes to develop meaningfully after a hypertension diagnosis. Many patients had very limited observation time before their index date, which weakened the reliability of feature construction. This further constrained the ability of the models to generalize well to new cases.\nAdditionally, many patients did not match our cohort criteria or had insufficient data to form meaningful predictions. The dataset size may have been too small to detect strong patterns or relationships between hypertension and subsequent diabetes development, especially given that the synthetic dataset may not have represented real-world complexities fully.\nThese issues suggest that the bottleneck wasn’t the modeling itself, but rather the mismatch between the research question and the available data, especially within the constraints of synthetic data. The limited sample size and the difficulty in matching the right cohort to the research question further impacted the model’s ability to make reliable predictions."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#key-challenges-faced",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#key-challenges-faced",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "Throughout the process, we encountered several technical and conceptual challenges:\nThe most critical issue lay in the quality and structure of the data itself. Many patients had sparse or short observation periods, which meant that only limited clinical history was available before the cohort entry date.\nThis directly affected the utility of the extracted features. On the modeling side, integrating with MLJ.jl required careful setup, especially when handling missing values, categorical encodings, and class imbalance.\n\n\n\nMLJ.jl\n\n\nFeature engineering was another bottleneck, since most features were binary flags or simple counts that did not capture clinical nuance or temporal dynamics. Preprocessing steps like normalization and imputation also needed fine-tuning and often had to be manually adjusted per model."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#next-steps-for-the-pipeline",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#next-steps-for-the-pipeline",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "As I look to enhance the pipeline, improving the code and modularizing it for reusability is a top priority. I plan to refine the existing functions and ensure that the pipeline can handle different types of OMOP CDM datasets, including exploring other tables that could enrich the features we extract. Additionally, I’m particularly interested in incorporating temporal-based prediction features, which would allow us to account for the sequence of patient events over time and improve model accuracy.\nOne direction I’m especially excited about is building interfaces in Julia to streamline the PLP process. Creating intuitive, modular tools that wrap around cohort creation, feature engineering, and modeling would make it easier for researchers to plug into OMOP CDM data and get started with predictive tasks. These interfaces could help reduce the complexity of working directly with lower-level tools, while still offering flexibility for customization."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#looking-ahead",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#looking-ahead",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "This project focused on building patient-level prediction (PLP) models in Julia. While the initial models underperformed, it emphasized the importance of validated cohorts, thoughtful feature engineering, strong baselines, and interpretable results.\nThe goal is to develop this into a robust Julia-native package for PLP tasks, integrated with JuliaHealth tools and OMOP CDM datasets, offering flexibility for custom pipelines. Aligning with OHDSI definitions and adding diagnostics could make it a valuable research and educational resource.\nTo enhance interpretability, I’m exploring a visualization layer for PLP pipelines using tools like Makie.jl or VegaLite.jl. Example visualizations could include:\n\nTimeline plots to visualize index dates, lookback periods, and outcome windows per patient, aiding in temporal reasoning.\nFeature density plots to assess feature sparsity and guide preprocessing decisions.\nROC and PR curves to evaluate and compare model performance.\nFeature importance charts (especially for tree-based models like Random Forest or XGBoost) to help identify clinically relevant predictors.\n\nEventually, this project could serve as a foundation for a Julia-native PLP toolkit, or at least a template for others interested in working with observational health data in Julia."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#lessons-learned",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#lessons-learned",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "",
    "text": "This project highlighted that Julia is well-suited for observational health research workflows. Its strong data ecosystem (DataFrames.jl, DuckDB.jl, CSV.jl), rich machine learning interfaces (MLJ.jl), and increasing support from domain-specific packages like OHDSICohortExpressions.jl make it a compelling environment for building transparent, customizable pipelines. Unlike black-box tools, Julia allows full control over every stage-from data access to model tuning-encouraging deeper understanding and reproducibility.\nSome lessons I learnt on the way:\n\nOne big realization for me was that the cohort population should closely align with the research question. I ran into situations where the cohorts didn’t actually support the prediction goal, which led to misleading outputs. I’m still learning how to validate cohort logic better, but now I see how critical this alignment is.\nI also struggled a bit with preprocessing and data understanding. There were points where I wasn’t sure how to handle missing data or what specific fields really meant. A deeper data check early on could have helped guide better feature engineering.\nWhile I didn’t build everything manually, I feel like I could have better leveraged existing Julia tools for some common steps. Exploring the ecosystem more might have simplified things.\nFinally, I have learned that debugging and iteration are part of the process. Things rarely work on the first try, and often the biggest insights came from chasing unexpected results or poor model performance. It has made me more comfortable with being wrong and learning through trial."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#acknowledgements",
    "href": "JuliaHealthBlog/posts/indu-plp-part3/plp-part3.html#acknowledgements",
    "title": "PLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Jacob Zelko for his mentorship, clarity, and constant feedback throughout the project. I also thank the JuliaHealth community for building an ecosystem where composable science can thrive.\nJacob S. Zelko: aka, TheCedarPrince\nNote: This blog post was drafted with the assistance of LLM technologies to support grammar, clarity and structure."
  },
  {
    "objectID": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html",
    "href": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html",
    "title": "GSoC Co-Mentoring Experience",
    "section": "",
    "text": "Hello 👋, I am Mounika. I am a Data Engineer at Brown Center for Biomedical Informatics. This summer, I had the privilege of co-mentoring a talented student, Jay Sanjay alongside Jacob Zelko (@TheCedarPrince) on a project for Google Summer of Code (aka GSoC). Here, I would like to share my experience as a co-mentor, offering insights for future mentors and students alike.\nBefore diving into my experience, let me provide some background on how it all started. At JuliaCon 2023, I had the chance to meet Jacob Zelko and have been following his work at JuliaHealth ever since. One day, I received a message from Jacob asking if I’d be interested in co-mentoring Jay for his GSoC project. Fortunately, I was already working on several projects at BCBI involving Julia programming, OMOP CDM databases and OHDSI tools, all of which were closely aligned with Jay’s project.\n\nFeel free to visit Jay’s work on OMOPCDMPathways.jl or read about his fellowship experience from this post."
  },
  {
    "objectID": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html#tips-for-mentees",
    "href": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html#tips-for-mentees",
    "title": "GSoC Co-Mentoring Experience",
    "section": "Tips for Mentees",
    "text": "Tips for Mentees\nFrom a mentee’s perspective having the following qualities would be helpful\n\nStick to the proposal: While it’s natural to feel the urge explore new ideas beyond the original proposal, it’s essential to remain focused on the original proposal due to time constrains.\nAdaptability and open-mindedness: Be open to feedback and willing to adjust the tasks as you face challenges.\nTime Management: Many students juggle internships, interviews and other commitments during the summer. So it’s to manage time effectively and discuss with the mentor about the progress during those times.\nEffective communication: Stay up to date with any updates from GSoC or from the mentor. Keeping your mentor updated about your progress or any challenges helps build a collaborative and supportive mentor relationship."
  },
  {
    "objectID": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html#tips-for-mentors",
    "href": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html#tips-for-mentors",
    "title": "GSoC Co-Mentoring Experience",
    "section": "Tips for Mentors",
    "text": "Tips for Mentors\nOn the other hand, Jacob demonstrated what it means to be an effective mentor. He showed me how to foster a supportive, collaborative relationship with the student. These are the lessons that I will carry forward in future mentorship roles:\nFrom a mentor’s perspective having the following qualities would be helpful\n\nClear communication: Communicating well in advance about the availability to meet or to review the work, having frequent meetings with the mentee would be helpful.\nEncouragement: While offering support, it’s important to encourage the mentee to take ownership of the project.\nCommitment and time: Mentoring GSoC is a voluntary role, often taken on in addition to regular professional work. Balancing GSoC with other work commitments requires effective time management and commitment.\nStructured Guidance: Providing a well-organized plan, such as using task management tools like Trello and GitHub issues, ensures that the mentee can follow a clear path towards success completion of the project."
  },
  {
    "objectID": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html#lets-keep-in-touch",
    "href": "JuliaHealthBlog/posts/mounika-gsoc-mentor/index.html#lets-keep-in-touch",
    "title": "GSoC Co-Mentoring Experience",
    "section": "Let’s Keep in Touch!",
    "text": "Let’s Keep in Touch!\nIf you would like to know more about me, you can connect with me on Linkedin."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "",
    "text": "I am Jay Sanjay, and I am pursuing a Bachelor’s degree in Computational Sciences and Engineering at the Indian Institute of Technology (IIT) in Hyderabad, India. Coming from a mathematics and data analysis background, I was initially introduced to Julia at my university lectures. Later, I delved more into the language and the JuliaHealth community - an intersection of Julia, Health Research, Data Sciences, and Informatics. Here, I met some of the great folks in JuliaHealth and I decided to take it on as a full-fledged summer project. In this blog, I will briefly describe what my project is and what I did as a part of it.\n\nYou can find my GSoC project archive link\nYou can also find the related publication of my work on Zenodo\nIf you want to know more about me, you can connect with me on LinkedIn and follow me on GitHub"
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#what-is-observational-health-research",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#what-is-observational-health-research",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "What Is Observational Health Research?",
    "text": "What Is Observational Health Research?\nObservational Health Research refers to studies that analyze real-world data (such as patient medical claims, electronic health records, etc.) to understand patient health. These studies often encompass a vast amount of data concerning patient care. An outstanding challenge here is that these datasets can become very complex and grow large enough to require advanced computing methods to process this information."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#what-are-patient-pathways",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#what-are-patient-pathways",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "What Are Patient Pathways?",
    "text": "What Are Patient Pathways?\nPatient pathways refer to the journey that patients with specific medical conditions undergo in terms of their treatment. This concept goes beyond simple drug uptake statistics and looks at the sequence of treatments patients receive over time, including first-line treatments and subsequent therapies. Understanding patient pathways is essential for analyzing treatment patterns, adherence to clinical guidelines, and the disbursement of drugs. To analyze patient pathways, one would typically use real-world data from sources such as electronic health records, claims data, and registries. However, barriers such as data interoperability and resource requirements have hindered the full utilization of real-world data for this purpose.\nSo to address these challenges we (the JuliaHealth organization and I) want to develop a set of tools to extract and analyze these patient pathways. These sets of tools are based on the Observational Medical Outcomes Partnership (OMOP) Common Data Model, which standardizes healthcare data to promote interoperability."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#setting-up-the-package-in-juliahealth-channel",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#setting-up-the-package-in-juliahealth-channel",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "1. Setting Up the Package in JuliaHealth Channel",
    "text": "1. Setting Up the Package in JuliaHealth Channel\nInitially, there was no package as such for generating pathways, so I had to build it from scratch. First, I created the repository with the name OMOPCDMPathways.jl. Once the repository was created, we needed to have a skeleton for a standard Julia repository. For this, we used the PkgTemplates.jl this provided a basic skeleton for the repository that included - folders for test suites, documentation, src code files, GitHub files, README and LICENSE file, TOML and citation files. All this we can further edit and modify as per our work. By default, PkgTemplate.jl uses Documenter.jl for the documentation part but as suggested and discussed with my mentor we decided to shift to DocumenterVitepress.jl for the documentation part. However, we still faced some deployment issues in the new documentation due to a few mistakes in the make.jl file, thanks to Anshul Singhvi for helping fix the Deployment issues with DocumenterVitepress. With this, we were ready with the documentation set up and fully functional. After we had shifted to DocumenterVitepress the main task now was to host the documentation, this was done using Github-Actions, detailed steps for hosting are provided at this page. Then we added the CodeCov to our package by triggering it via a dummy function and a corresponding test case for it. Also, the CI for the package was set up with it. And, now finally the repository was ready with test coverage, CI, and documentation fully functional repository ready. Here’s some snapshots of the documentation set-up:\n\n\nInitial documentation with Documenter.jl\n\n\n\nNew documentation using DocumenterVitepress.jl\n\nSo, as a part of it, I created this documentation which provides detailed steps for converting docs from Documenter to DocumenterVitepress."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#loading-the-postgresql-database",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#loading-the-postgresql-database",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "2. Loading the PostgreSQL Database",
    "text": "2. Loading the PostgreSQL Database\nThe main database we worked on/built analysis was the freely available OMOPCDM Database. The Database was formatted within a PostgreSQL database with installation instructions here are some instructions on how to set up Postgres in a Linux machine. However, I was provided with some more extra synthetic data from my mentor for further testing of the functionalities. Being a very large database we had to strategically download it further, my mentor helped me in setting up the Postgres on my local machine. Once, the database was set up proper testing was performed on it to check if things were as expected. With this, we were done with the database setup as well and could finally dive into the actual code logic for the Pathways synthesis."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#testing-and-development-setup-on-my-local-computer",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#testing-and-development-setup-on-my-local-computer",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "3. Testing and Development setup on my local computer",
    "text": "3. Testing and Development setup on my local computer\nTo get a proper environment for functionality creation and concurrent testing we required a proper testing setup so that we could test the new functions made at the same time. This was done using Revise.jl, which helps to keep Julia sessions running without frequent restarts when making changes to code. It allowed me to edit my code, update packages, or switch git branches during a session, with changes applied immediately in the next command. My mentor helped me set it up, added Revise.jl to the global Julia environment, also PackageCompatUI that provides a terminal text interface to the [compat] section of a Julia Project.toml file, and finally made a Julia script by the name “startup.jl” out of it. This script was then added to /home/jay-sanjay/.julia/config/ path in my local computer.\nHere is the sample for the startup.jl file:\nusing PackageCompatUI\nusing PkgTemplates\nusing Revise\n\n###################################\n# HELPER FUNCTIONS\n###################################\nfunction template()\n    Template(;\n        user=\"jay-sanjay\",\n        dir=\"~/FOSS\",\n        authors=\"jaysanjay &lt;jaysanjay@gmail.com&gt; and contributors\",\n        julia=v\"1.6\",\n        plugins=[\n            ProjectFile(; version=v\"0.0.1\"),\n            Git(),\n            Readme(),\n            License(; name=\"MIT\"),\n            GitHubActions(; extra_versions=[\"1.6\", \"1\", \"nightly\"]),\n            TagBot(),\n            Codecov(),\n            Documenter{GitHubActions}(),\n            Citation(; readme = true),\n            RegisterAction(),\n            BlueStyleBadge(),\n            Formatter(;style = \"blue\")\n        ],\n    )\nend"
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#selecting-treatments-of-interest",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#selecting-treatments-of-interest",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "4. Selecting Treatments of Interest",
    "text": "4. Selecting Treatments of Interest\nSo, as a part of this, we used the previously mentioned research paper and discussion with the mentors we came up with logic for it. The first thing to do was to determine the moment in time from which selected treatments of interest should be included in the treatment pathway. The default is all treatments starting after the index date of the target cohort. For example, for a target cohort consisting of newly diagnosed patients, treatments after the moment of first diagnosis are included. However, it would also be desirable to include (some) treatments before the index date, for instance in case a specific disease diagnosis is only confirmed after initiating treatment. Therefore, periodPriorToIndex specifies the period (i.e. number of days) before the index date from which treatments should be included. We have created two dispatches for this function. After that proper testing and documentation are also added.\nA basic implementation for it is:\n\nConstruct a SQL query to select cohort_definition_id, subject_id, and cohort_start_date from a specified table, filtering by cohort_id.\nThe SQL query construction and execution was done using the FunSQL.jl library, in the below-shown manner:\n\nsql = From(tab) |&gt;\n            Where(Fun.in(Get.cohort_definition_id, cohort_id...)) |&gt;\n            Select(Get.cohort_definition_id, Get.subject_id, Get.cohort_start_date) |&gt;\n            q -&gt; render(q, dialect=dialect)\n\nExecutes the constructed SQL query using a database connection, fetching the results into a data frame.\nIf the DataFrame is not empty, convert cohort_start_date to DateTime and subtract date_prior from each date, then return the modified DataFrame.\n\nThis was then be called this:\nperiod_prior_to_index(\n        cohort_id = [1, 1, 1, 1, 1], \n        conn; \n        date_prior = Day(100), \n        tab=cohort\n    )"
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#filters-applied",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#filters-applied",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "5. Filters Applied",
    "text": "5. Filters Applied\nAfter this, we where needed to get the patient’s database filtered more finely so that there are minimal variations that can be ignored. The duration of the above extracted event eras may vary a lot and it can be preferable to limit to only treatments exceeding a minimum duration. Hence, minEraDuration speciﬁes the minimum time an event era should last to be included in the analysis. All these implementations were more of Dataframe manipulation where I used DataFrames.jl package.\nAfter that proper testing and documentation are also added.\nA basic implementation for the minEraDuration is: It filters the treatment history DataFrame to retain only those rows where the duration between drug_exposure_end and drug_exposure_start is at least minEraDuration. This function can be used as follows:\n#| eval: false \n\ncalculate_era_duration(test_df, 920000)\n\n#= ... =#\n\n4×3 DataFrame\n Row │ person_id  drug_exposure_start  drug_exposure_end \n     │ Int64      Float64              Int64             \n─────┼───────────────────────────────────────────────────\n   1 │         1           -3.7273e8          -364953600\n   2 │         1            2.90304e7           31449600\n   3 │         1           -8.18208e7          -80006400\n   4 │         1            1.32918e9         1330387200\nAnother filter we worked on is the EraCollapse. So, let’s suppose a case where an individual receives the same treatment for a long period of time (e.g. need for chronic treatment). Then it’s highly likely that the person would require refills. Now as patients are not 100% adherent, there might be a gap between two subsequent event eras. Usually, these eras are still considered as one treatment episode, and the eraCollapseSize deals with the maximum gap within which two eras of the same event cohort would be collapsed into one era (i.e. seen as a continuous treatment instead of a stop and re-initiation of the same treatment). After that proper testing and documentation are also added.\nA basic implementation for the eraCollapseSize is: (a) Sorts the data frame by event_start_date and event_end_date. (b) Calculates the gap between each era and the previous era. (c) Filters out rows with gap_same &gt; eraCollapseSize.\nThese functions can be used as follows:\n#| eval: false \n\n#= ... =#\n\nEraCollapse(treatment_history = test_df, eraCollapseSize = 400000000)\n4×4 DataFrame\n Row │ person_id  drug_exposure_start  drug_exposure_end  gap_same   \n     │ Int64      Float64              Int64              Float64    \n─────┼───────────────────────────────────────────────────────────────\n   1 │         1           -5.33347e8         -532483200  -1.86373e9\n   2 │         1           -3.7273e8          -364953600   1.59754e8\n   3 │         1           -8.18208e7          -80006400   2.83133e8\n   4 │         1            2.90304e7           31449600   1.09037e8"
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#treatment-history-of-the-patients",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#treatment-history-of-the-patients",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "6. Treatment History of the Patients",
    "text": "6. Treatment History of the Patients\nThe create_treatment_history function constructs a detailed treatment history for patients in a target cohort by processing and filtering event cohort data from a given DataFrame. It begins by isolating the target cohort based on its cohort_id, adding a new column for the index_year derived from the cohort’s start date. Then, it selects relevant event cohorts based on a provided list of cohort IDs and merges them with the target cohort on the subject_id to associate events with individuals in the target group. The function applies different filtering criteria depending on whether the user is interested in treatments starting or ending within a specified period before the target cohort’s start date (defined by periodPriorToIndex). It keeps only the event cohorts that match the filtering condition, ensuring that only relevant treatments are considered. After filtering, the function calculates time gaps between consecutive cohort events for each patient, adding these gaps to the DataFrame. The final DataFrame provides a history of treatments, including the dates of events and the time intervals between them, offering a clear timeline of treatment for each patient. After that proper testing and documentation are also added."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#combinationwindow-functionality-to-combine-overlapping-treatments",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#combinationwindow-functionality-to-combine-overlapping-treatments",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "7. CombinationWindow Functionality To Combine Overlapping Treatments",
    "text": "7. CombinationWindow Functionality To Combine Overlapping Treatments\nNow once we have the filtering of the treatments done, we need to combine the overlapping treatments based on some set of rules. The combinationWindow specifies the time that two event eras need to overlap to be considered a combination treatment. If there are more than two overlapping event eras, we sequentially combine treatments, starting from the ﬁrst two overlapping event eras.\nThe combination_Window function processes a patient’s treatment history by identifying overlapping treatment events and combining them into continuous treatment periods based on certain rules. It first converts event_cohort_id into strings and sorts the treatment data by person_id, event_start_date, and event_end_date. The helper function selectRowsCombinationWindow calculates gaps between consecutive treatments, marking rows where treatments overlap or occur too closely. In the main loop, the function checks these overlaps and gaps against a specified combinationWindow. If treatments overlap (or nearly overlap), the function adjusts the treatment periods by either merging adjacent rows or splitting rows to create continuous treatment periods. The process continues until all overlapping treatments are combined into one, creating an updated and accurate treatment history. The function ensures the final output reflects realistic treatment windows by handling special cases where gaps between treatments are smaller than the treatment durations themselves.\nIt mainly covers the three cases mentioned in the R-research paper:\n\nSwitch Case:\nCondition: If the gap between the two treatment events is smaller than the combinationWindow, but the gap is not equal to the duration of either event. Action: The event_end_date of the previous treatment is set to the event_start_date of the current treatment. This effectively “shifts” the previous treatment’s end date to eliminate the gap, merging the treatments into one continuous period. Purpose: This ensures that treatment gaps that are too small (less than combinationWindow) are treated as part of the same treatment window.\n#| eval: false \n\n#= ... =#\n\nif -gap_previous &lt; combinationWindow && !(-gap_previous in [duration_era, prev_duration_era])\n    treatment_history[i-1, :event_end_date] = treatment_history[i, :event_start_date]\nHere is the pictorial representation for the same: \n\n\nFRFS (First Row, First Shortened):\nCondition: If the gap is larger than or equal to the combinationWindow, or the gap equals the duration of one of the two treatments, and the first treatment ends before or on the same date as the second treatment. Action: A new row is created where the second treatment’s event_end_date is set to the end date of the first treatment. This preserves the overlap but ensures that the earlier treatment period stays intact. Purpose: This prevents unnecessary truncation of the first treatment if it spans the entire overlap window.\n#| eval: false \n\n#= ... =#\n\nelseif -gap_previous &gt;= combinationWindow || -gap_previous in [duration_era, prev_duration_era]\n    if treatment_history[i-1, :event_end_date] &lt;= treatment_history[i, :event_end_date]\n        new_row = deepcopy(treatment_history[i, :])\n        new_row.event_end_date = treatment_history[i-1, :event_end_date]\n        append!(treatment_history, DataFrame(new_row'))\nHere is the pictorial representation for the same: \n\n\nLRFS (Last Row, First Shortened):\nCondition: If the gap is larger than or equal to the combinationWindow, or the gap equals the duration of one of the treatments, and the first treatment ends after the second treatment. Action: The current treatment’s event_end_date is adjusted to match the event_end_date of the previous treatment. Purpose: This handles cases where the second treatment’s window should be shortened to prevent overlap with the previous treatment, merging them into a single continuous window.\n#| eval: false \n\n#= ... =#\n\nelse\n    treatment_history[i, :event_end_date] = treatment_history[i-1, :event_end_date]\nHere is the pictorial representation for the same: \n\nNote: However, There are a few things left to cover here, most of which are the documentation and writing the test suite for the same."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#organizing-meetings-and-communication",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#organizing-meetings-and-communication",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "1. Organizing Meetings and Communication",
    "text": "1. Organizing Meetings and Communication\nThroughout the project, I regularly met with my mentor, [Jacob Zelko], and co-mentor, [Mounika], via weekly Zoom calls to discuss progress and seek guidance. During these meetings, we reviewed my work, identified areas where I needed help, and set clear goals for the upcoming weeks. We used Trello to organize and track these goals, ensuring that nothing was overlooked. My mentors provided detailed insights into specific technical aspects and guided me through the logic behind various functions. Outside of our scheduled meetings, they were always available for quick queries via Slack, ensuring constant support."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#personal-documentation",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#personal-documentation",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "2. Personal Documentation",
    "text": "2. Personal Documentation\nIn addition to the notes from our meetings, I maintained personal documentation where I recorded every step I took, including the challenges I faced and the mistakes I made. This helped me reflect on my progress and stay organized throughout the fellowship. Following my selection for GSoC 2024, I also published a blog post on Medium to share my journey and experiences with the Julia Language community."
  },
  {
    "objectID": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#contributions-to-the-rest-of-the-juliahealth-repositories",
    "href": "JuliaHealthBlog/posts/jay-gsoc/gsoc-2024-fellows.html#contributions-to-the-rest-of-the-juliahealth-repositories",
    "title": "GSoC ’24: Developing Tooling for Observational Health Research in Julia",
    "section": "3. Contributions To the Rest of the JuliaHealth Repositories",
    "text": "3. Contributions To the Rest of the JuliaHealth Repositories\nEarlier I have contributed a lot to the OMOPCDMCohortCreator.jl including adding new functionalities writing test suites, adding blogs including - Patient Pathways within JuliaHealth. Apart from that I also initiated 3 new releases of this package."
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "",
    "text": "I am Divyansh, an undergraduate student from Guru Gobind Singh Indraprastha university, majoring in Artificial Intelligence and Machine Learning. Stumbling upon projects under the Juliahealth sub-ecosystem of medical imaging packages, the intricacies of imaging modalities and file formats, reflected in their relevant project counterparts, captured my interest. Working with standards such as NIfTI (Neuroimaging Informatics Technology Initiative) and DICOM (Digital Imaging and Communications in Medicine) with MedImages.jl, I became interested in the visualization routines of such imaging datasets and their integration within the segmentation pipelines for modern medical-imaging analysis.\nIn this post, I’d like to summarize what I did this summer and everything I learned along the way, contributing to MedEye3d.jl medical imaging visualizer under GSOC-2024!\n\nIf you want to learn more about me, you can connect with me on LinkedIn and follow me on GitHub"
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#what-is-medeye3d.jl",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#what-is-medeye3d.jl",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "What is MedEye3d.jl?",
    "text": "What is MedEye3d.jl?\nMedEye3D.jl is a package under the Julia language ecosystem designed to facilitate the visualization and annotation of medical images. Tailored specifically for medical applications, it offers a range of functionalities to enhance the interpretation and analysis of medical images. MedEye3D aims to provide an essential tool for 3D medical imaging workflow within Julia. The underlying combination of Rocket.jl and ModernGL.jl ensures the high-performance robust visualizations that the package has to offer.\nMedEye3d.jl is open-source and comes with an intuitive user interface (To learn more about MedEye3d, you can read the paper introducing it here [1])."
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#what-features-does-this-project-encompass",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#what-features-does-this-project-encompass",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "What features does this project encompass?",
    "text": "What features does this project encompass?\nThis project covers implementation of several tasks that will enable the establishment of additional important functionalities within the MedEye3D package, facilitating enhancements within the visualization’s windowing for MRI and PET data, support for super voxels (sv), improved load times, high-level functionality implementation and robust viewing for multiple images."
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#migration-of-package-from-rocket-to-julias-base.channel",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#migration-of-package-from-rocket-to-julias-base.channel",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "1. Migration of package from Rocket to Julia’s Base.Channel",
    "text": "1. Migration of package from Rocket to Julia’s Base.Channel\nInitially, there was significant screen-tearing evident from the pixelated display of the rendered text and main image which, furthermore exhibited flickering upon scrolling through the slices in the relevant displayed image’s planar views i.e (Transversal, Coronal and Saggital). Troubleshooting along the way, we narrowed down the issue within the Rocket’s actor-subscription mechanism and decided to integrate Julia’s Base.Channel within MedEye3d.jl for handling the event and state management routine. Julia has asynchronous, threadsafe channels which facilitate in asynchronous programming with the help of a producer-consumer mechanism. An example usage of Base.Channel is as follows:\nfunction consumer(channel::Base.Channel)\n    while(true)\n    channelData::String = take!(channel)\n    println(\"Channel got \" * channelData)\n    end\nend\n\nnewChannel = Base.Channel(100)\n\n@async consumer(newChannel)\nput!(newChannel, \"apples\")\nJulia’s multiple dispatch made for the architectural setup of MedEye3d, facilitated fixing the issue of screen tearing. Below is how the on_next! function, invokes different reactive components based on the types of arguments it is dealing with.\n\nDump data in channel -&gt; fetch data from the channel in an event loop -&gt; invoke on_next!(state, channelData) -&gt; invoke relevant functionality based on the type of arguments passed\n\n\nThe end result was a visualizer with a seamless display of a CT image without any pixelating artifacts."
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#implementation-of-high-level-functions-with-simplified-basic-usage",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#implementation-of-high-level-functions-with-simplified-basic-usage",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "2. Implementation of high level functions with simplified basic usage",
    "text": "2. Implementation of high level functions with simplified basic usage\nImplementing a bare-bones image visualization required a lot of function calls and definitions, in order to execute the following phases:\n\nRendering an image-plane with OpenGL\nLoading data slices from the image\nCreating texture specifications for modalities\nProducing the final segmentation display\n\nIn order to simplify basic usage, high-level abstractions were put in place with the help of MedImages.jl (under ongoing development) library to load images in the form of MedImage objects to formulate a single display function for the user. Further simplifications were made to accommodate options for the user to manipulate the imaging data that is displayed currently in the visualizer i.e retrieval of voxel arrays and their modification. Taking this in mind, the following relevant functions were exposed:\nMedEye3d.SegmentationDisplay.displayImage()\nMedEye3d.DisplayDataManag.getDisplayedData()\nMedEye3d.DisplayDataManag.setDisplayedData()\nPutting all of the above functions to use together, we can launch the visualizer, retrieve the displayed voxel data and modify it to our liking. A sample script to achieve the former, is highlighted below:\nusing MedEye3d\nctNiftiImage = \"/home/hurtbadly/Downloads/ct_soft_study.nii.gz\"\nmedEyeStruct = MedEye3d.SegmentationDisplay.displayImage(ctNiftiImage)\ndisplayData = MedEye3d.DisplayDataManag.getDisplayedData(medEyeStruct, [Int32(1), Int32(2)]) #passing the active texture number\n\n# We need to check if the return type of the displayData is a single Array{Float32,3} or a vector{Array{Float32,3}}\n# Now in this case we are setting Gaussian noise over the manualModif Texture voxel layer, and the manualModif texture defaults to 2 for active number\n\ndisplayData[2][:, :, :] = randn(Float32, size(displayData[2]))\nMedEye3d.DisplayDataManag.setDisplayedData(medEyeStruct, displayData)\nThe result of this Gaussian noise within the annotation layer, made for an outcome like the following:"
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#improved-precompilation-with-decreased-outputs-to-reduce-start-time",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#improved-precompilation-with-decreased-outputs-to-reduce-start-time",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "3. Improved precompilation with decreased outputs to reduce start time",
    "text": "3. Improved precompilation with decreased outputs to reduce start time\nPreviously, the package’s precompilation was failing in Julia v1.9 and v1.10 due to pattern matching errors arising after the usage of match macros from the Match.jl pkg in MedEye3d’s keymapping workflow between GLFW callbacks from mouse and keyboard. The relevant equivalent native conditional (if-else) statements, resolved the issue and facilitated in successful precompilation of the package. Further, only following minimal outputs were produced during precompilation:\n\nChanges highlighted within the following pull-request:\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/12"
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#automatic-windowing-for-most-common-mri-and-pet-modalities",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#automatic-windowing-for-most-common-mri-and-pet-modalities",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "4. Automatic windowing for most common MRI and PET modalities",
    "text": "4. Automatic windowing for most common MRI and PET modalities\nWindowing is a crucial aspect of medical imaging, particularly in MRI (Magnetic Resonance Imaging) and PET (Positron Emission Tomography) modalities. It enables radiologists to enhance the contrast of images, highlighting specific features and improving the overall diagnostic accuracy. Windowing involves controlling the display range of pixel values to optimize the contrast between different tissues or structures. The display range is defined by two values: the minimum (min) and maximum (max) values that contribute to the final range of pixels that are displayed. By adjusting these values, radiologists can enhance or suppress specific features in the image, facilitating a more accurate diagnosis.\nThe setTextureWindow function utilizes a set of predefined keymap controls to simplify the windowing process. The F1-F7 keys are designated for controlling windowing in MRI and PET modalities. The keymap controls are as follows:\n\nF1: Display wide window for bone (CT) or increase minimum value for PET\nF2: Display window for soft tissues (CT) or increase minimum value for PET\nF3: Display wide window for lung viewing (CT) or increase minimum value for PET\nF4: Decrease minimum value for display\nF5: Increase minimum value for display\nF6: Decrease maximum value for display\nF7: Increase maximum value for display\n\nImplementation of setTextureWindow Function\nThe setTextureWindow function is designed to update the texture window settings based on the input keymap control. The function takes three arguments:\n\nactiveTextur: The current texture specification\nstateObject: The state data fields\nwindowControlStruct: The window control structure containing the letter code for the keymap control\n\nThe function performs the following steps:\n\nChecks the letter code of the keymap control and updates the minimum and maximum values of the texture specification accordingly.\nUpdates the uniforms for the texture specification using the controlMinMaxUniformVals function.\n\nfunction setTextureWindow(activeTextur::TextureSpec, stateObject::StateDataFields, windowControlStruct::WindowControlStruct)\n    activeTexturName = activeTextur.name\n    displayRange = activeTextur.minAndMaxValue[2] - activeTextur.minAndMaxValue[1]\n    activeTexturStudyType = activeTextur.studyType\n    if windowControlStruct.letterCode == \"F1\"\n        if activeTexturStudyType == \"CT\"\n            #Bone windowing in CT\n            activeTextur.minAndMaxValue = Float32.([400, 1000])\n        elseif activeTexturStudyType == \"PET\"\n            activeTextur.minAndMaxValue[1] += 0.10 * displayRange #windowing for pet, in the case of PET simply increase the minimum by 20% , doing the same in f1,f2 and f3\n        end\n    elseif windowControlStruct.letterCode == \"F2\"\n        if activeTexturStudyType == \"CT\"\n            activeTextur.minAndMaxValue = Float32.([-40, 350])\n        elseif activeTexturStudyType == \"PET\"\n            activeTextur.minAndMaxValue[1] += 0.10 * displayRange\n        end\n    elseif windowControlStruct.letterCode == \"F3\"\n        if activeTexturStudyType == \"CT\"\n            activeTextur.minAndMaxValue = Float32.([-426, 1000])\n        elseif activeTexturStudyType == \"PET\"\n            activeTextur.minAndMaxValue[1] += 0.10 * displayRange\n        end\n    elseif windowControlStruct.letterCode == \"F4\"\n        activeTextur.minAndMaxValue[1] -= 0.20 * displayRange\n    elseif windowControlStruct.letterCode == \"F5\"\n        activeTextur.minAndMaxValue[1] += 0.20 * displayRange\n    elseif windowControlStruct.letterCode == \"F6\"\n        activeTextur.minAndMaxValue[2] -= 0.20 * displayRange\n    elseif windowControlStruct.letterCode == \"F7\"\n        activeTextur.minAndMaxValue[2] += 0.20 * displayRange\n    elseif windowControlStruct.letterCode == \"F8\"\n        activeTextur.uniforms.maskContribution -= 0.10\n    elseif windowControlStruct.letterCode == \"F9\"\n        activeTextur.uniforms.maskContribution += 0.10\n    end\n\n    stateObject.mainForDisplayObjects.listOfTextSpecifications = map(texture -&gt; texture.name == activeTexturName ? activeTextur : texture, stateObject.mainForDisplayObjects.listOfTextSpecifications)\n    coontrolMinMaxUniformVals(activeTextur)\nend\n\nBone windowing in CT\n\n\n\nBone windowing in PET"
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#adding-support-for-multi-image-viewing-with-crosshair-marker-for-image-registration",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#adding-support-for-multi-image-viewing-with-crosshair-marker-for-image-registration",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "5. Adding support for multi-image viewing with crosshair marker for image registration",
    "text": "5. Adding support for multi-image viewing with crosshair marker for image registration\nFollowing the mid-term evaluation, MedEye3d.jl underwent a significant enhancement, whereby a multi-image display capability was implemented through a series of refinements. Specifically, a novel approach was adopted, whereby separate OpenGL fragment shaders were introduced to concurrently render images on either side of the visualizer, namely the left and right views. Prior to integrating voxel data into the fragment shaders, an initial series of tests involved evaluating individual colors to validate the integrity of the double image display. A screenshot from one of these critical testing phases is presented below: \nThe shaders were further manipulated to automatically initialize for each of the images separately. Further, the reactive aspect of the visualizer in multi-image display mode was iterated upon and now, instead of a single state management struct, a vector of states was being passed around, facilitating the user to scroll each of the images separately just by simply hovering their mouse over either of the image, activating its relevant associated state struct.\nDown below, is the struct for state that handles all of the things currently related with an image:\n@with_kw mutable struct StateDataFields\n  currentDisplayedSlice::Int = 1 # stores information what slice number we are currently displaying\n  mainForDisplayObjects::forDisplayObjects = forDisplayObjects() # stores objects needed to  display using OpenGL and GLFW\n  onScrollData::FullScrollableDat = FullScrollableDat()\n  textureToModifyVec::Vector{TextureSpec} = [] # texture that we want currently to modify - if list is empty it means that we do not intend to modify any texture\n  isSliceChanged::Bool = false # set to true when slice is changed set to false when we start interacting with this slice - thanks to this we know that when we start drawing on one slice and change the slice the line would star a new on new slice\n  textDispObj::ForWordsDispStruct = ForWordsDispStruct()# set of objects and constants needed for text diplay\n  currentlyDispDat::SingleSliceDat = SingleSliceDat() # holds the data displayed or in case of scrollable data view for accessing it\n  calcDimsStruct::CalcDimsStruct = CalcDimsStruct()   #data for calculations of necessary constants needed to calculate window size , mouse position ...\n  valueForMasToSet::valueForMasToSetStruct = valueForMasToSetStruct() # value that will be used to set  pixels where we would interact with mouse\n  lastRecordedMousePosition::CartesianIndex{3} = CartesianIndex(1, 1, 1) # last position of the mouse  related to right click - usefull to know onto which slice to change when dimensions of scroll change\n  forUndoVector::AbstractArray = [] # holds lambda functions that when invoked will  undo last operations\n  maxLengthOfForUndoVector::Int64 = 15 # number controls how many step at maximum we can get back\n  fieldKeyboardStruct::KeyboardStruct = KeyboardStruct()\n  displayMode::DisplayMode = SingleImage\n  imagePosition::Int64 = 1\n  switchIndex::Int = 1\n  mainRectFields::GlShaderAndBufferFields = GlShaderAndBufferFields()\n  crosshairFields::GlShaderAndBufferFields = GlShaderAndBufferFields()\n  textFields::GlShaderAndBufferFields = GlShaderAndBufferFields()\n  spacingsValue::Union{Vector{Tuple{Float64,Float64,Float64}},Tuple{Float64,Float64,Float64}} = [(1.0, 1.0, 1.0)]\n  originValue::Union{Vector{Tuple{Float64,Float64,Float64}},Tuple{Float64,Float64,Float64}} = [(1.0, 1.0, 1.0)]\n  supervoxelFields::GlShaderAndBufferFields = GlShaderAndBufferFields()\nend\nAfter the integrity of the fragment shaders was verified in multi-image, voxel data for the images was integrated and further modifications to the high-level functions were made and eventually the following script produced a rather appealing result.\nScript for loading the same NIFTI image twice in the visualizer for side-by-side display:\nusing MedEye3d\nctNiftiImage = \"/home/hurtbadly/Downloads/ct_soft_study.nii.gz\"\nMedEye3d.SegmentationDisplay.displayImage([[ctNiftiImage],[ctNifitImage]])\n\nResults in :\n\n\nCrosshair marker for image registration are displayed in the relevant passive image to hightlight the same anatomical regions based on the spatial meta-data of the images i.e spacing, origin and direction. In order to achive the crosshair rendering in the passive image, the following action items were devised:\n\nRetrieval of GLFW Mouse Callbacks for x and y position of the cursor in window coordinates (0 to window-width) from the active image\nConversion of these x and y window coordinates into their relevant active image x and y texture coordinates\nConversion of these texture coordinates into real space point with the help of spatial metadata\nConversion of the real space point into the texture coordinates of the passive image\nConversion of the passive image texture coordinates into their relevant OpenGL coordinate system values (-1 to 1)\nRendering of crosshair on OpenGL coordinate in passive image\n\nConversion between different coordinate systems and accounting for the image’s spatial metadata during calculating proved to be challenging at first, but with multiple revisions, a final solution was achieved with seemingly no noticeable amount of lag or delay. One such frame of [CT] images with crosshair display in multi-image is depicted below:\n\n\nAnother frame from the openGL rendering cycle, highlighting PET images with crosshair display in multi-image mode:"
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#adding-support-for-the-display-of-supervoxels-sv-with-borders-within-the-image-slices-to-better-understand-anatomical-regions-within-slices",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#adding-support-for-the-display-of-supervoxels-sv-with-borders-within-the-image-slices-to-better-understand-anatomical-regions-within-slices",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "6. Adding support for the display of SuperVoxels sv with borders within the image slices to better understand anatomical regions within slices",
    "text": "6. Adding support for the display of SuperVoxels sv with borders within the image slices to better understand anatomical regions within slices\nIn enhancing MedEye3d’s functionality, supporting super voxels (sv) with boundaries becomes paramount. The sv rendering, effectively capturing gradients, serves as the cornerstone for detecting these boundaries within both MRI and PET volumes. Supervoxels, described either through indicator masks or meshes, encapsulate regions of interest with distinct image characteristics. By integrating boundary detection for super-voxels, MedEye3d can offer enhanced segmentation capabilities, enabling more precise delineation and analysis of anatomical structures and pathological regions within medical imaging data.\nSupervoxels are basically a collection of voxels that share similar image properties. For example: in MRI scans of the brain cortex, super voxels could represent clusters of voxels corresponding to specific anatomical regions or functional areas. The main objective of this task was to add support for the display of super voxel-based segmentation of images, followed by some janitorial tasks:\n\nDisplay of the borders of super-voxels (sv), extracted using the machine learning algorithms.\nChecking image gradient agreement with super-voxel borders.\n\nThis initial workflow involved, the initialization of relevant buffers in OpenGL for dynamic rendering of lines over the image display, namely vertex array buffers (vao), vertex buffers (vbo) and edge buffers (ebo). Further, these buffers are updated on a scroll event, where the information from the currently displayed slice is passed to the event handler, which invokes a function that updates the vertex buffer (vbo) with new vertices pertaining to the relevant slice number and planar view, precalculated from an HDF5 file during initialization of the visualizer. For instance, if the user is scrolling in the 3rd axis (transversal plane) and is currently on slice 40, the supervoxel display will pertain to edges specifically calculated for that specific slice in that plane.\nEventually, with ever so increasing number of attempts and a few hurdles along the way, one of which particularly stood out since it marked our first step towards a good direction:\n\nChallenges in rendering\n\n\nAt last, an appealing result hit our sight.\n\nFinal result\n\n\nNote: The image borders are intentional to emphasize the size of the visualizer which is currently defaulted to a certain width and height.\n\n\n\nNote: However, There are a few things left to cover here, most of which revolve around MedImages.jl and documentation for the same. List of PRs that facilitated the completion of the tasks highlighted above:\n\n\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/21\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/20\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/16\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/14\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/13\nhttps://github.com/JuliaHealth/MedEye3d.jl/pull/12"
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#mentoring-and-guidance",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#mentoring-and-guidance",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "1. Mentoring and Guidance",
    "text": "1. Mentoring and Guidance\nI regularly organized meetings with my mentor to seek guidance on project direction and troubleshooting issues in the visualizer. This ensured that I stayed on track, received timely feedback, and addressed any challenges that arose."
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#package-documentation-and-community-contribution",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#package-documentation-and-community-contribution",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "2. Package Documentation and Community Contribution",
    "text": "2. Package Documentation and Community Contribution\nI contributed to other medical imaging sub-ecosystem packages in JuliaHealth, including MedImages.jl and MedEval3D.jl. Specifically, I set up documentation for these packages using DocuementerVitepress.jl. This not only enhanced the functionality of these packages but also helped maintain a coherent and organized package ecosystem."
  },
  {
    "objectID": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#multirepo-management-and-collaboration",
    "href": "JuliaHealthBlog/posts/divyansh-gsoc/gsoc-2024-fellows.html#multirepo-management-and-collaboration",
    "title": "GSoC ’24: Adding functionalities to medical imaging visualizations",
    "section": "3. Multirepo Management and Collaboration",
    "text": "3. Multirepo Management and Collaboration\nIn addition to my work on the MedEye3d visualizer, I made significant contributions to other JuliaHealth repositories, including MedImages.jl and worked over an Insight Toolkit wrapper library ITKIOWrapper.jl for support in image I/O down the road in MedImages.jl. I also maintained relevant documentation and ensured continuous collaboration and synchronization across these packages."
  },
  {
    "objectID": "JuliaHealthBlog/index.html",
    "href": "JuliaHealthBlog/index.html",
    "title": "Welcome to the JuliaHealthBlog! 👋",
    "section": "",
    "text": "Welcome to the JuliaHealthBlog! 👋\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nGrowing Together: Exploring Sub-Ecosystems within JuliaHealth 👋\n\n\n\n\n\nLearn about the specialized focus areas emerging within the JuliaHealth community and how you can contribute to these exciting sub-ecosystems.\n\n\n\n\n\nApr 29, 2025\n\n\nJuliaHealth Community (Leader JacobZelko, Contributor Divyansh Goyal)\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nPLP-Pipeline Series Part 3: Lessons Learned, Key Challenges, and What Comes Next\n\n\n\n\n\nReflections on building an end-to-end PLP pipeline in Julia tools - lessons learned, current challenges, and future directions.\n\n\n\n\n\nApr 21, 2025\n\n\nKosuri Lakshmi Indu\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nPLP-Pipeline Series Part 2: From Raw Clinical Data to Predictive Models\n\n\n\n\n\nPart 2 of the PLP-Pipeline blog series – how we preprocess OMOP CDM data, extract features, and train ML models using Julia tools\n\n\n\n\n\nApr 20, 2025\n\n\nKosuri Lakshmi Indu\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nPLP-Pipeline Series Part 1: From Research Question to Cohort Construction\n\n\n\n\n\nKicking off the PLP-Pipeline blog series - how we define research questions and construct cohorts using OMOP CDM and Julia tools.\n\n\n\n\n\nApr 12, 2025\n\n\nKosuri Lakshmi Indu\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ’24: Adding dataset-wide functions and integrations of augmentations\n\n\n\n\n\nMedPipe3D - Medical segmentation pipeline with dataset-wide functions and augmentations.\n\n\n\n\n\nNov 3, 2024\n\n\nJan Zubik\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ’24: Adding functionalities to medical imaging visualizations\n\n\n\n\n\nA summary of my project for Google Summer of Code - 2024\n\n\n\n\n\nNov 1, 2024\n\n\nDivyansh Goyal\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC Co-Mentoring Experience\n\n\n\n\n\nMy experience as a GSoC co-mentor within JuliaHealth\n\n\n\n\n\nSep 12, 2024\n\n\nMounika Thakkallapally\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ’24: Developing Tooling for Observational Health Research in Julia\n\n\n\n\n\nA summary of my project for Google Summer of Code - 2024\n\n\n\n\n\nSep 7, 2024\n\n\nJay Sanjay Landge\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ’24: Enhancements to KomaMRI.jl GPU Support\n\n\n\n\n\nA summary of my project for Google Summer of Code\n\n\n\n\n\nAug 30, 2024\n\n\nRyan Kierulf\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ’24: IPUMS.jl Small Project\n\n\n\n\n\nA summary of my project for Google Summer of Code\n\n\n\n\n\nAug 26, 2024\n\n\nMichela Rocchetti\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Post\n\n\n\n\n\nPost description\n\n\n\n\n\nJun 22, 2024\n\n\nFoobar\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nn.d."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "",
    "text": "These emoticons may resemble hieroglyphics, but very soon you will realize that they mean more than 1000s of lines of code.\n\n\nDescription of the emojis used in the title\n\n\n\n📝 Action Plan: A clear, structured plan that guides each step of the MedPipe3D pipeline.\n\n\n🩻 3D Medical Images: Medical imaging data, such as MRI scans in Nifti format.\n\n\n📎 AI Model: The initial AI model that will be trained and refined within the pipeline.\n\n\n📉 Loss Function: A function that measures the model’s performance during training, guiding the optimization process.\n\n\n🗃️ Data Loading: Preparation and loading of data and metadata into HDF5 format.\n\n\n📚 Data Splitting: Dividing data into training, validation, and test sets.\n\n\n♻️ Data Augmentation: Increasing data variability through augmentation.\n\n\n🧑‍🏫 AI Training: Using Lux.jl framework to train the AI model.\n\n\n🤖 Model: The trained AI model that can perform tasks like segmentation on medical images.\n\n\n👁️ Data for Visualization: Output data, such as masks and segmentations.\n\n\n📈 Performance Logs: Logs and metrics documenting the AI’s performance.\n\n\n❤️‍🩹 Purpose of MedPipe3D\n\n\n\n\nIn this post, I’d like to summarize what I did this summer and everything I learned along the way, rebuilding the MedPipe3D medical imaging pipeline. I will not start typically, but so that anyone even a novice can visualize what this project has achieved, while the latter part is intended for more experienced readers. It will be easiest to divide it into 4 steps separated by ➡️ in the title above. Each emoji stands for a different piece of pipeliner and will be described below.\n📝🩻📎📉 What we need from the user\nMedPipe3D requires four essential inputs from the user to get started: a clear action plan 📝, 3D medical images like MRI scans 🩻, an AI model 📎, and a loss function 📉.\n🗃️📚♻️🧑‍🏫 The Pipeline essential AI manufacturing line\nFollowing the plan 📝, MedPipe3D loads data, pre-processes, and organizes it 🗃️. Allowing data to be easily split 📚, and efficiently augmented ♻️ in many ways for learning AI 🧑‍🏫 model effectively. In the end, performing testing and post-processing for better determination of AI skills.\nIt’s designed to transform raw medical data into a format that your AI can learn from, segmenting meaningful patterns and structures.\n🤖👁️📈 Results and Insights\nMedPipe3D is a tool for researchers and for that, it cannot do without analysis, testing, and evaluation. The result of the pipeline is a model 🤖 as well as data 👁️ and logs 📈 needed in MedEval3D that are ready for visualization and further analysis with MedEye3D. In a nutshell, it makes visualizing results easy, tumor locations or other medical features directly as masks on the scans.\n❤️‍🩹 Purpose-Driven Technology\nMedPipe3D’s mission goes beyond technology. It’s about providing the tools to create AIs that support healthcare professionals in making faster, more accurate decisions, with the ultimate goal of saving lives.\nThis four-part journey captures the heart of the MedPipe3D toolkit for advancing medical AI, from raw data to life-saving insight.\n\n\nMedPipe3D is a framework created from hundreds of hours over summer vacation, thousands of lines of code, hundreds of mistakes, and most importantly the guidance of my mentor and author of all of these libraries Dr. Jakub Mitura. At its core, MedPipe3D combines sophisticated data handling from MedImage thanks to the hard work of Divyansh Goyal. Newly developed pipeline for model training, validation, and testing with existing MedEval3D, and result visualization with MedEye3D. Unfortunately, not all of the project’s goals have been fully achieved, and thereby there is one section ➡️ too many. Hopefully not for long. My name is Jan Zubik, and I wrote this entire library from scratch, which is currently my most complex project.\nIf you are a data scientist, programmer, or code enthusiast, I invite you to read the next section where I go into detail and present version 1 of this tool in detail.\nI’m a 3rd-year student of BSc in Data Science and Machine Learning, I know that many things can be done better, expanded, debugged, and optimized. Now it just works, but don’t hesitate to write to me personally on LinkedIn, Julia’s Slack or GitHub! With your comments, and direct critique you will help me to be a better programmer and one day MedPipe3D will contribute in a tiny way to save someone’s life!\nExact work from the Google Summer of Code project you will find in GitHub the repository."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#introduction",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#introduction",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "",
    "text": "MedPipe3D is a framework created from hundreds of hours over summer vacation, thousands of lines of code, hundreds of mistakes, and most importantly the guidance of my mentor and author of all of these libraries Dr. Jakub Mitura. At its core, MedPipe3D combines sophisticated data handling from MedImage thanks to the hard work of Divyansh Goyal. Newly developed pipeline for model training, validation, and testing with existing MedEval3D, and result visualization with MedEye3D. Unfortunately, not all of the project’s goals have been fully achieved, and thereby there is one section ➡️ too many. Hopefully not for long. My name is Jan Zubik, and I wrote this entire library from scratch, which is currently my most complex project.\nIf you are a data scientist, programmer, or code enthusiast, I invite you to read the next section where I go into detail and present version 1 of this tool in detail.\nI’m a 3rd-year student of BSc in Data Science and Machine Learning, I know that many things can be done better, expanded, debugged, and optimized. Now it just works, but don’t hesitate to write to me personally on LinkedIn, Julia’s Slack or GitHub! With your comments, and direct critique you will help me to be a better programmer and one day MedPipe3D will contribute in a tiny way to save someone’s life!\nExact work from the Google Summer of Code project you will find in GitHub the repository."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#integrate-augmentations-for-medical-data",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#integrate-augmentations-for-medical-data",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Integrate augmentations for medical data 🆙",
    "text": "Integrate augmentations for medical data 🆙\nAugmenting medical data is a crucial step for enhancing model robustness, especially given the variations in imaging conditions and patient anatomy.\n\nThis pipeline currently supports multiple augmentation techniques:\n\nBrightness transform ✅\nContrast augmentation transform ✅\nGamma Transform ✅\nGaussian noise transform ✅\nRician noise transform ✅\nMirror transform ✅\nScale transform 🆙\nGaussian blur transform ✅\nSimulate low-resolution transform 🆙\nElastic deformation transform 🆙\n\n\nWhich have been fully integrated. Each of these methods helps the model generalize better by simulating diverse imaging scenarios.\n\nComments:\nAugmentations such as scaling, and low-resolution simulation use interpolation that is not yet GPU-accelerated.\nElastic deformation with simulation of different tissue elasticities is a potential development opportunity that would further improve the model’s adaptability by mimicking more complex variations found in medical imaging."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#invertible-augmentations-and-support-test-time-augmentations",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#invertible-augmentations-and-support-test-time-augmentations",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Invertible augmentations and support test time augmentations 🆙",
    "text": "Invertible augmentations and support test time augmentations 🆙\nThis section focuses on the ability to apply reversible augmentations to test data, allowing the model to be evaluated with different transformations. Only rotation is available at this time. The function evaluate_patches performs this evaluation by applying specified augmentations, dividing the test data into patches, and reconstructing the full image from the patches. During testing, one can choose to use of largest connected component post-processing. Metrics are calculated and results are saved for analysis.\n\n\nevaluate_test:\n\n# ...\nfor test_group in test_groups\n    test_data, test_label, attributes = fetch_and_preprocess_data([test_group], h5, config)\n    results, test_metrics = evaluate_patches(test_data, test_label,  tstate, model, config)\n    y_pred, metr = process_results(results, test_metrics, config)\n    save_results(y_pred, attributes, config)\n    push!(all_test_metrics, metr)\nend\n# ...\nfunction evaluate_patches(test_data, test_label, tstate, model, config, axis, angle)\n    println(\"Evaluating patches...\")\n    results = []\n    test_metrics = []\n    tstates = [tstate]\n    test_time_augs = []\n\n    for i in config[\"learning\"][\"n_invertible\"]\n        data = rotate_mi(test_data, axis, angle)\n        for tstate_curr in tstates\n            patch_results = []\n            patch_size = Tuple(config[\"learning\"][\"patch_size\"])\n            idx_and_patches, paded_data_size = divide_into_patches(test_data, patch_size)\n            coordinates = [patch[1] for patch in idx_and_patches]\n            patch_data = [patch[2] for patch in idx_and_patches]\n            for patch in patch_data\n                y_pred_patch, _ = infer_model(tstate_curr, model, patch)\n                push!(patch_results, y_pred_patch)\n            end\n            idx_and_y_pred_patch = zip(coordinates, patch_results)\n            y_pred = recreate_image_from_patches(idx_and_y_pred_patch, paded_data_size, patch_size, size(test_data))\n            if config[\"learning\"][\"largest_connected_component\"]\n                y_pred = largest_connected_component(y_pred, config[\"learning\"][\"n_lcc\"])\n            end\n            metr = evaluate_metric(y_pred, test_label, config[\"learning\"][\"metric\"])\n            push!(test_metrics, metr)\n        end\n    end\n    return results, test_metrics\nend\nfunction divide_into_patches(image::AbstractArray{T, 5}, patch_size::Tuple{Int, Int, Int}) where T\n    println(\"Dividing image into patches...\")\n    println(\"Size of the image: \", size(image)) \n\n    # Calculate the required padding for each dimension (W, H, D)\n    pad_size = (\n        (size(image, 1) % patch_size[1]) != 0 ? patch_size[1] - size(image, 1) % patch_size[1] : 0,\n        (size(image, 2) % patch_size[2]) != 0 ? patch_size[2] - size(image, 2) % patch_size[2] : 0,\n        (size(image, 3) % patch_size[3]) != 0 ? patch_size[3] - size(image, 3) % patch_size[3] : 0\n    )\n\n    # Pad the image if necessary\n    padded_image = image\n    if any(pad_size .&gt; 0)\n        padded_image = crop_or_pad(image, (size(image, 1) + pad_size[1], size(image, 2) + pad_size[2], size(image, 3) + pad_size[3]))\n    end\n\n    # Extract patches\n    patches = []\n    for x in 1:patch_size[1]:size(padded_image, 1)\n        for y in 1:patch_size[2]:size(padded_image, 2)\n            for z in 1:patch_size[3]:size(padded_image, 3)\n                patch = view(\n                    padded_image,\n                    x:min(x+patch_size[1]-1, size(padded_image, 1)),\n                    y:min(y+patch_size[2]-1, size(padded_image, 2)),\n                    z:min(z+patch_size[3]-1, size(padded_image, 3)),\n                    :,\n                    :\n                )\n                push!(patches, [(x, y, z), patch])\n            end\n        end\n    end\n    println(\"Size of padded image: \", size(padded_image))\n    return patches, size(padded_image)\nend\n\nfunction recreate_image_from_patches(\n    coords_with_patches,\n    padded_size,\n    patch_size,\n    original_size\n)\n    println(\"Recreating image from patches...\")\n    reconstructed_image = zeros(Float32, padded_size...)\n    \n    # Place patches back into their original positions\n    for (coords, patch) in coords_with_patches\n        x, y, z = coords\n        reconstructed_image[\n            x:x+patch_size[1]-1,\n            y:y+patch_size[2]-1,\n            z:z+patch_size[3]-1,\n            :,\n            :\n        ] = patch\n    end\n\n    # Crop the reconstructed image to remove any padding\n    final_image = reconstructed_image[\n        1:original_size[1],\n        1:original_size[2],\n        1:original_size[3],\n        :,\n        :\n    ]\n    println(\"Size of the final image: \", size(final_image))\n    return final_image\nend\n\nComment: In this section, there is significant potential to incorporate additional types of invertible augmentations."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#patch-based-data-loading-with-probabilistic-oversampling",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#patch-based-data-loading-with-probabilistic-oversampling",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Patch-based data loading with probabilistic oversampling ✅",
    "text": "Patch-based data loading with probabilistic oversampling ✅\nIn this section, patches are extracted using extract_patch from the medical images for model training, with a probability-based method to decide between a random patch or a patch with non-zero labels. Helper functions like get_random_patch and get_centered_patch determine the starting indices and dimensions for the patches based on given configurations, while padding methods ensure consistency even if the patch exceeds the original image dimensions. Probabilistic oversampling, as configured, allows for more balanced and informative data sampling, which improves the model’s ability to detect specific medical features.\n\n\nextract_patch:\n\nfunction extract_patch(image, label, patch_size, config)\n    # Fetch the oversampling probability from the config\n    println(\"Extracting patch.\")\n    oversampling_probability = config[\"learning\"][\"oversampling_probability\"]\n    # Generate a random number to decide which patch extraction method to use\n    random_choice = rand()\n\n    if random_choice &lt;= oversampling_probability\n        return extract_nonzero_patch(image, label, patch_size)\n    else\n\n        return get_random_patch(image, label, patch_size)\n    end\nend\n#Helper function, in case the mask is emptyClick to apply\nfunction extract_nonzero_patch(image, label, patch_size)\n    println(\"Extracting a patch centered around a non-zero label value.\")\n    indices = findall(x -&gt; x != 0, label)\n    if isempty(indices)\n        # Fallback to random patch if no non-zero points are found\n        return get_random_patch(image, label, patch_size)\n    else\n        # Choose a random non-zero index to center the patch around\n        center = indices[rand(1:length(indices))]\n        return get_centered_patch(image, label, center, patch_size)\n    end\nend\n# Function to get a patch centered around a specific index\nfunction get_centered_patch(image, label, center, patch_size)\n    center_coords = Tuple(center)\n    half_patch = patch_size .÷ 2\n    start_indices = center_coords .- half_patch\n    end_indices = start_indices .+ patch_size .- 1\n\n    # Calculate padding needed\n    pad_beg = (\n        max(1 - start_indices[1], 0),\n        max(1 - start_indices[2], 0),\n        max(1 - start_indices[3], 0)\n    )\n    pad_end = (\n        max(end_indices[1] - size(image, 1), 0),\n        max(end_indices[2] - size(image, 2), 0),\n        max(end_indices[3] - size(image, 3), 0)\n    )\n\n    # Adjust start_indices and end_indices after padding\n    start_indices_adj = start_indices .+ pad_beg\n    end_indices_adj = end_indices .+ pad_beg\n\n    # Convert padding values to integers\n    pad_beg = Tuple(round.(Int, pad_beg))\n    pad_end = Tuple(round.(Int, pad_end))\n\n    # Pad the image and label using pad_mi\n    image_padded = pad_mi(image, pad_beg, pad_end, 0)\n    label_padded = pad_mi(label, pad_beg, pad_end, 0)\n\n    # Extract the patch\n    image_patch = image_padded[\n        start_indices_adj[1]:end_indices_adj[1],\n        start_indices_adj[2]:end_indices_adj[2],\n        start_indices_adj[3]:end_indices_adj[3]\n    ]\n    label_patch = label_padded[\n        start_indices_adj[1]:end_indices_adj[1],\n        start_indices_adj[2]:end_indices_adj[2],\n        start_indices_adj[3]:end_indices_adj[3]\n    ]\n\n    return image_patch, label_patch\nend\n\nfunction get_random_patch(image, label, patch_size)\n    println(\"Extracting a random patch.\")\n    # Check if the patch size is greater than the image dimensions\n    if any(patch_size .&gt; size(image))\n        # Calculate the needed size to fit the patch\n        needed_size = map(max, size(image), patch_size)\n        # Use crop_or_pad to ensure the image and label are at least as large as needed_size\n        image = crop_or_pad(image, needed_size)\n        label = crop_or_pad(label, needed_size)\n    end\n\n    # Calculate random start indices within the new allowable range\n    start_x = rand(1:size(image, 1) - patch_size[1] + 1)\n    start_y = rand(1:size(image, 2) - patch_size[2] + 1)\n    start_z = rand(1:size(image, 3) - patch_size[3] + 1)\n    start_indices = [start_x, start_y, start_z]\n    end_indices = start_indices .+ patch_size .- 1\n\n    # Extract the patch directly when within bounds\n    image_patch = image[start_indices[1]:end_indices[1], start_indices[2]:end_indices[2], start_indices[3]:end_indices[3]]\n    label_patch = label[start_indices[1]:end_indices[1], start_indices[2]:end_indices[2], start_indices[3]:end_indices[3]]\n\n    return image_patch, label_patch\nend"
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#calculate-median-and-mean-spacing-with-resampling",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#calculate-median-and-mean-spacing-with-resampling",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Calculate Median and Mean Spacing with resampling 🆙",
    "text": "Calculate Median and Mean Spacing with resampling 🆙\nThis part ensures that all images in the dataset have consistent real coordinates, spacing, and shape. It’s a critical factor in medical imaging for accurate analysis. Calculating and applying set values, median or mean across images ensures uniformity.\n\nResample images to target image 🆙\nThis step aligns each image to the reference coordinates of the main image, ensuring that all images share a common spatial alignment. The resample_to_image function from MedImage.jl is used here, applying interpolation to adjust each image.\n\n\nresample_images_to_target:\n\nif resample_images_to_target && !isempty(Med_images)\n    println(\"Resampling $channel_type files in channel '$channel_folder' to the first $channel_type in the channel.\")\n    reference_image = Med_images[1]\n    Med_images = [resample_to_image(reference_image, img, interpolator) for img in Med_images]\nend\n\nComment: Resample_to_image uses interpolation that is not yet GPU-accelerated in this implementation, this step slows down the data preparation phase significantly.\n\n\nEnsure uniform spacing across the entire dataset 🆙\nThis step brings all images to a consistent voxel spacing across the dataset using resample_to_spacing from MedImage.jl. This uniform spacing is crucial for creating a standardized dataset where each image voxel represents the same physical volume.\n\n\nesample_to_spacing:\n\nif resample_images_spacing == \"set\"\n    println(\"Resampling all $channel_type files to target spacing: $target_spacing\")\n    target_spacing = Tuple(Float32(s) for s in target_spacing)\n    channels_data = [[resample_to_spacing(img, target_spacing, interpolator) for img in channel] for channel in channels_data]\nelseif resample_images_spacing == \"avg\"\n    println(\"Calculating average spacing across all $channel_type files and resampling.\")\n    all_spacings = [img.spacing for channel in channels_data for img in channel]\n    avg_spacing = Tuple(Float32(mean(s)) for s in zip(all_spacings...))\n    println(\"Average spacing calculated: $avg_spacing\")\n    channels_data = [[resample_to_spacing(img, avg_spacing, interpolator) for img in channel] for channel in channels_data]\nelseif resample_images_spacing == \"median\"\n    println(\"Calculating median spacing across all $channel_type files and resampling.\")\n    all_spacings = [img.spacing for channel in channels_data for img in channel]\n    median_spacing = Tuple(Float32(median(s)) for s in all_spacings)\n    println(\"Median spacing calculated: $median_spacing\")\n    channels_data = [[resample_to_spacing(img, median_spacing, interpolator) for img in channel] for channel in channels_data]\nelseif resample_images_spacing == false\n    println(\"Skipping resampling of $channel_type files.\")\n    # No resampling will be applied, channels_data remains unchanged.\nend\n\nComment: Resample_to_spacing uses interpolation that is not yet GPU-accelerated in this implementation, this step slows down the data preparation phase significantly.\n\n\nResizing all channel files to average or target size ✅\nTo create a cohesive 5D tensor, all images in each channel are resized to a uniform shape, either the average size of all images or a specific target size. This resizing process uses crop_or_pad, ensuring that all images match the specified dimensions, making them suitable for model input.\n\n\ncrop_or_pad:\n\nif resample_size == \"avg\"\n    sizes = [size(img.voxel_data) for img in channels_data for img in img]  # Get sizes from all images\n    avg_dim = map(mean, zip(sizes...))\n    avg_dim = Tuple(Int(round(d)) for d in avg_dim)\n    println(\"Resizing all $channel_type files to average dimension: $avg_dim\")\n    channels_data = [[crop_or_pad(img, avg_dim) for img in channel] for channel in channels_data]\nelseif resample_size != \"avg\"\n    target_dim = Tuple(resample_size)\n    println(\"Resizing all $channel_type files to target dimension: $target_dim\")\n    channels_data = [[crop_or_pad(img, target_dim) for img in channel] for channel in channels_data]\nend"
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#basic-post-processing-operations",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#basic-post-processing-operations",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Basic Post-processing operations",
    "text": "Basic Post-processing operations\nPost-processing operations involve the algorithm largest_connected_components. It is achieved by label initialization and propagation in the segmented mask. The initialize_labels_kernel function assigns unique labels to different regions.\n\n\ninitialize_labels_kernel:\n\n@kernel function initialize_labels_kernel(mask, labels, width, height, depth)\n    idx = @index(Global, Cartesian)\n    i = idx[1]\n    j = idx[2]\n    k = idx[3]\n    \n    if i &gt;= 1 && i &lt;= width && j &gt;= 1 && j &lt;= height && k &gt;= 1 && k &lt;= depth\n        if mask[i, j, k] == 1\n            labels[i, j, k] = i + (j - 1) * width + (k - 1) * width * height\n        else\n            labels[i, j, k] = 0\n        end\n    end\nend\n\nPropagate_labels_kernel iteratively updates the labels to maintain connected regions. propagate_labels_kernel:\n\n@kernel function propagate_labels_kernel(mask, labels, width, height, depth)\n    idx= @index(Global, Cartesian)\n    i = idx[1]\n    j = idx[2]\n    k = idx[3]\n\n    if i &gt;= 1 && i &lt;= width && j &gt;= 1 && j &lt;= height && k &gt;= 1 && k &lt;= depth\n        if mask[i, j, k] == 1\n            current_label = labels[i, j, k]\n            for di in -1:1\n                for dj in -1:1\n                    for dk in -1:1\n                        if di == 0 && dj == 0 && dk == 0\n                            continue\n                        end\n                        ni = i + di\n                        nj = j + dj\n                        nk = k + dk\n                        if ni &gt;= 1 && ni &lt;= width && nj &gt;= 1 && nj &lt;= height && nk &gt;= 1 && nk &lt;= depth\n                            if mask[ni, nj, nk] == 1 && labels[ni, nj, nk] &lt; current_label\n                                labels[i, j, k] = labels[ni, nj, nk]\n                            end\n                        end\n                    end\n                end\n            end\n        end\n    end\nend\n\nThis process facilitates the identification of the largest connected components in 3D space, helping to isolate relevant medical structures, such as tumors, in the segmented mask. Allowing determining how many such areas are to be returned.\n\n\nlargest_connected_components:\n\nfunction largest_connected_components(mask::Array{Int32, 3}, n_lcc::Int)\n    width, height, depth = size(mask)\n    mask_gpu = CuArray(mask)\n    labels_gpu = CUDA.fill(0, size(mask))\n    dev = get_backend(labels_gpu)\n    ndrange = (width, height, depth)\n    workgroupsize = (3, 3, 3)\n\n    # Initialize labels\n    initialize_labels_kernel(dev)(mask_gpu, labels_gpu, width, height, depth, ndrange = ndrange)\n    CUDA.synchronize()\n\n    # Propagate labels iteratively\n    for _ in 1:10 \n        propagate_labels_kernel(dev, workgroupsize)(mask_gpu, labels_gpu, width, height, depth, ndrange = ndrange)\n        CUDA.synchronize()\n    end\n\n    # Download labels back to CPU\n    labels_cpu = Array(labels_gpu)\n    \n    # Find all unique labels and their sizes\n    unique_labels = unique(labels_cpu)\n    label_sizes = [(label, count(labels_cpu .== label)) for label in unique_labels if label != 0]\n\n    # Sort labels by size and get the top n_lcc\n    sort!(label_sizes, by = x -&gt; x[2], rev = true)\n    top_labels = label_sizes[1:min(n_lcc, length(label_sizes))]\n\n    # Create a mask for each of the top n_lcc components\n    components = [labels_cpu .== label[1] for label in top_labels]\n    return components\nend"
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#structured-configuration-of-all-hyperparameters",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#structured-configuration-of-all-hyperparameters",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Structured configuration of all hyperparameters 🆙",
    "text": "Structured configuration of all hyperparameters 🆙\nHyperparameters for the entire pipeline are stored in a JSON configuration file, enabling straightforward adjustments for experimentation (just swap values, save and resume the study). This structured setup allows easy modification of key parameters, such as data set preparation, training settings, data augmentation, and resampling options.\n\n\nExample configuration:\n\n{\n    \"model\": {\n        \"patience\": 10,\n        \"early_stopping_metric\": \"val_loss\",\n        \"optimizer_name\": \"Adam\",\n        \"loss_function_name\": \"l1\",\n        \"early_stopping\": true,\n        \"early_stopping_min_delta\": 0.01,\n        \"optimizer_args\": \"lr=0.001\",\n        \"num_epochs\": 10\n    },\n    \"data\": {\n        \"batch_complete\": false,\n        \"resample_size\": [200,101,49],\n        \"resample_to_target\": false,\n        \"resample_to_spacing\": false,\n        \"batch_size\": 3,\n        \"standardization\": false,\n        \"target_spacing\": null,\n        \"channel_size\": 1,\n        \"normalization\": false,\n        \"has_mask\": true\n    },\n    \"augmentation\": {\n        \"augmentations\": {\n            \"Brightness transform\": {\n                \"mode\": \"additive\",\n                \"value\": 0.2\n            }\n        },\n        \"p_rand\": 0.5,\n        \"processing_unit\": \"GPU\",\n        \"order\": [\n            \"Brightness transform\"\n        ]\n    },\n    \"learning\": {\n        \"Train_Val_Test_JSON\": false,\n        \"largest_connected_component\": false,\n        \"n_lcc\": 1,\n        \"n_folds\": 3,\n        \"invertible_augmentations\": false,\n        \"n_invertible\": true,\n        \n        \"class_JSON_path\": false,\n        \"additional_JSON_path\": false,\n        \"patch_size\": [50,50,50],\n        \"metric\": \"dice\",\n        \"n_cross_val\": false,\n        \"patch_probabilistic_oversampling\": false,\n        \"oversampling_probability\": 1.0,\n        \"test_train_validation\": [\n            0.6,\n            0.2,\n            0.2\n        ],\n        \"shuffle\": false\n    }\n}\n\nComments: The current configuration is loaded as a dictionary, which simplifies access and modification. This setup presents a strong foundation for integrating automated search algorithms for hyperparameter tuning, enabling more efficient model optimization. The configuration structure could be reorganized and re-named to improve readability, making it easier for users to locate and adjust specific parameters."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#visualization-of-algorithm-outputs",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#visualization-of-algorithm-outputs",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Visualization of algorithm outputs ⚠️",
    "text": "Visualization of algorithm outputs ⚠️\nThis module provides basic visualization functionality by saving output masks and images first to MedImage format and then to Nifti format. The create_nii_from_medimage function from MedImage.jl generates Nifti files, which can be loaded into MedEye3D for 3D visualization.\nComments: Integrating this visualization module more fully with the pipeline could eliminate unnecessary steps. By automatically loading output masks and images as raw data into MedEye3D for 3D visualization and supporting a more efficient end-to-end workflow."
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#k-fold-cross-validation-functionality",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#k-fold-cross-validation-functionality",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "K-fold cross-validation functionality ✅",
    "text": "K-fold cross-validation functionality ✅\nK-fold cross-validation is implemented to evaluate model performance more robustly. The data is split into multiple folds, with each fold serving as a validation set once, while the others form the training set. This functionality provides a better assessment of model performance across different subsets of the data.\n\n\nK-fold cross-validation functionality:\n\n...\n  tstate = initialize_train_state(rng, model, optimizer)\n  if config[\"learning\"][\"n_cross_val\"]\n      n_folds = config[\"learning\"][\"n_folds\"]\n      all_tstate = []\n      combined_indices = [indices_dict[\"train\"]; indices_dict[\"validation\"]]\n      shuffled_indices = shuffle(rng, combined_indices)\n      for fold in 1:n_folds\n          println(\"Starting fold $fold/$n_folds\")\n          train_groups, validation_groups = k_fold_split(combined_indices, n_folds, fold, rng)\n          \n          tstate = initialize_train_state(rng, model, optimizer)\n          final_tstate = epoch_loop(num_epochs, train_groups, validation_groups, h5, model, tstate, config, loss_function, num_classes)\n          \n          push!(all_tstate, final_tstate)\n      end\n  else\n      final_tstate = epoch_loop(num_epochs, train_groups, validation_groups, h5, model, tstate, config, loss_function, num_classes)\n  end\n  return final_tstate\n...  \n\nThe k_fold_split function organizes the indices for each fold, ensuring comprehensive coverage of the dataset during training.\n\n\nk_fold_split\n\nfunction k_fold_split(data, n_folds, current_fold)\n    fold_size = length(data) ÷ n_folds\n    validation_start = (current_fold - 1) * fold_size + 1\n    validation_end = validation_start + fold_size - 1\n    validation_indices = data[validation_start:validation_end]\n    train_indices = [data[1:validation_start-1]; data[validation_end+1:end]]\n    return train_indices, validation_indices\nend"
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#necessary-enhancements",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#necessary-enhancements",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Necessary Enhancements",
    "text": "Necessary Enhancements\nComprehensive Logging: Develop detailed logging mechanisms that capture a wide range of events, including system statuses, model performance metrics, and user activities, to facilitate debugging and system optimization. This is currently executed as a simple println function.\nTensorBoard Integration: Implement an interface for TensorBoard to allow users to visualize training dynamics in real time, providing insights into model behavior and performance trends.\nError and Warning Logs: Introduce advanced error and warning logging capabilities to alert users of potential issues before they affect the pipeline’s performance, ensuring smoother operations and maintenance.\nAutomated Visualization: Integrate MedEye3D directly into MedPipe3D to enable automated visualization of outputs, such as segmentation masks or other relevant medical imaging features. This feature would provide users with real-time visual feedback on model performance and data quality. Code-Level Documentation: Due to needed changes in the fundamental structure of the pipeline in the final phase of the project, it is necessary to reevaluate all documentation.\nOfficial JuliaHealth Documentation: Extend the documentation efforts to include official entries on juliahealth.org, providing a centralized and authoritative resource for users seeking to learn more about MedPipe3D and its capabilities with examples shown"
  },
  {
    "objectID": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#potential-enhancements",
    "href": "JuliaHealthBlog/posts/JZubik-gsoc/GSoC_Jan_Zubik_MedPipe3D.html#potential-enhancements",
    "title": "GSoC ’24: Adding dataset-wide functions and integrations of augmentations",
    "section": "Potential Enhancements",
    "text": "Potential Enhancements\nGPU support for interpolation will allow for significant acceleration of such functions as Scale transform, Simulate, Low-resolution transform, Elastic deformation transform, and Resampling spacing.\nAdd more reversible augmentations to test time.\nCalculating the average of the edges of the picture: checking the type of photo and calculating more correctly on this basis\nElastic deformation transforms with the simulation of different tissue elasticities."
  },
  {
    "objectID": "JuliaHealthBlog/posts/dummy/index.html",
    "href": "JuliaHealthBlog/posts/dummy/index.html",
    "title": "Dummy Post",
    "section": "",
    "text": "Seciton 1\nSmall dummy blog post\n\n2 + 2\n\n4\n\n\n\nprintln(2 + 2)\n\n4\n\n\n\n\nSection 2\n\n\nSection 3\n\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {, Foobar},\n  title = {Dummy {Post}},\n  date = {2024-06-22},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFoobar. 2024. “Dummy Post.” June 22, 2024."
  },
  {
    "objectID": "JuliaHealthBlog/posts/michela-gsoc/Michela_JSoC.html",
    "href": "JuliaHealthBlog/posts/michela-gsoc/Michela_JSoC.html",
    "title": "GSoC ’24: IPUMS.jl Small Project",
    "section": "",
    "text": "Hello! 👋\nHi! I am Michela, I have a Master’s degree in Physics of Complex Systems and I am currently working as a software engineer in Rome, where I am from. During my studies, I became interested in the use of modeling and AI methods to improve healthcare and how these tools can be used to better understand how cultural and social backgrounds influence the health of individuals. I am also interested in the computational modeling of the brain and the human body and its implications for a better understanding of certain pathological conditions.\nWith these motivations in mind, I heard about Google Summer of Code. Since I had studied Julia in some courses and given that the language is expanding rapidly, I decided to find a project within Julia. As a result, I found the project of Jacob Zelko (@TheCedarPrince) to start this experience.\n\nIf you want to learn more about me, you can connect with me here: LinkedIn, GitHub\n\n\n\nProject Description\nIPUMS is the “world’s largest available single database of census microdata”, providing survey and census data from around the world. It includes several projects that provide a wide variety of datasets. The information and data collected by IPUMS are useful for comparative research, as well as for the analysis of individuals in their life contexts. These data can be used to create a more comprehensive dataset that will facilitate research on the social determinants of health for different types of diseases, social communities, and geographical areas.\n\n\nTo learn more about IPUMS, visit the website\n\n\n\nTasks and Goals\nThe primary objectives of this proposal are to:\n\nDevelop a native Julia package to interact with the APIs available around the datasets IPUMS provides.\nProvide useful utilities within this package for manipulating IPUMS datasets.\nCompose this package with the wider Julia ecosystem to enable novel research in health, economics, and more.\n\nTo achieve this, the work was distributed as follows:\n\nExpand some of the functionality developed in ipumsr IPUMS NHGIS\n\nCreate a link between OpenAPI documentation and the functions internally used in IPUMS.jl: updating already present functions, determining if updating is needed, and testing them\nDevelop functionality similar to the get_metadata_nghis function present in ipumsr\n\nUpdate IPUMS documentation\n\nSet up and deploy DocumenterVitepress.jl\n\nWrite a blog post on how IPUMS.jl can be composed within the ecosystem.\n\n\n\n\nHow the work was done\nThe first task was to migrate documents from Documenter to DocumenterVitepress.This issue aims to support the significant refactoring underway across JuliaHealth, aimed at improving the discoverability and cohesion of the JuliaHealth ecosystem, particularly about documentation. This issue is intended to create a more attractive entry point for new Julia users interested in health research within the Julia community. To accomplish this task, a dependency of DocumenterVitepress was added to the docs directory of the IPUMS.jl repository. Once this was done, the Documenter.jl make.jl file was migrated into a DocumenterVitepress.jl make.jl file. Working on the make.jl file, the pages structure were added to the web page explaining the IPUMS.jl package. With this in mind, those were added: 1. Home: to explain the main purpose of the package 2. Workflows: to explain the working process 3. How to: to give general information 4. Tutorials: to show how to use IPUMS.jl\n5. Examples: some examples of activities 6. Mission: to explain why the package is useful for the community 7. References: references used to write the pages.\nThis first task takes some time, especially setting up GitHub and cloning the repository locally. At this point, my experience with GitHub was really limited and I had to learn how to use the Git environment from scratch, for example how to do continuous integration (to commit code to a shared repository), documentation release and merge, and local testing. I found the support of my mentors and searching for material online was really helpful.\nThe second task was to update the documentation of IPUMS.jl by modifying the functionality within the model folder in the IPUMS.jl folder. The main aim of this task was to a description of the function and its attributes, an example of possible implementation and result, and finally to show how to use it. The documentation to be updated as of several types of functions: 1. Data extract 2. Data set 3. Data Table 4. Time series table 5. Error 6. Shapefile. Each of these macro-categories (from 1 to 4) contains a set of functions, each signaling the different expected output and specific purpose. Information about what each function does, and the meaning of each specific input variable, has been found on the IPUMS website and references have been made in the written documentation.\n\n\nHow to work with IPUMS\nAfter writing down the description of the function and the inputs, examples were formulated, starting from the IPUMS website: when you register at IPUMS, an API key is given. which is used, among other things, to run pre-written code on the website. This code contains examples of these functions, and these examples have been adapted by changing some input values and adapting them to work in the Julia framework. The latter task was done by simply rewriting some structures, such as dictionaries, maps, or lists, in the Julia language. Here is a small guide on how to set up working with the API: 1. Create an IPUMS account 2. Log in to your account 3. Copy the API key, which can be obtained from the website 4. Use the key to run the code that is already available on the IPUMS Developer Portal, where you will also find information about the variables and packages.\n\n\nFunctions testing\nA final task was to test the functions in the ‘api_IPUMSAPI.jl’ file. In this file, the function to be tested and other functions are defined and the most important ones are extracted to be available in the available throughout the framework. Some of the functions to be tested were the following:\n\nmetadata_nhgis_data_tables_get\nmetadata_nhgis_datasets_dataset_data_tables_data_table_get\nmetadata_nhgis_datasets_dataset_get\nmetadata_nhgis_datasets_get\n\nBefore working on the Julia files, testing and understanding the original R function was done using R studio.\n\nEach function was then tested using the API key from the IPUMS registration as well as other input examples taken from the documentation or the IPUMS website. or from the IPUMS website. All functions were displayed successfully, giving the expected result, so it can be concluded that the translation from R to Julia is successful.\n\nusing IPUMS\nusing OpenAPI\n\napi_key = \"insert your key here\"\n\nversion = \"2\"\npage_number = 1\npage_size = 2500\n#media_type = \n\napi = IPUMSAPI(\"https://api.ipums.org\", Dict(\"Authorization\" =&gt; api_key));\n\nres1 = metadata_nhgis_data_tables_get(api, version)\n\nres2 = metadata_nhgis_datasets_dataset_get(api, \"2022_ACS1\", \"2\");\n\nres3 = metadata_nhgis_datasets_dataset_data_tables_data_table_get(api, \"2022_ACS1\",\"B01001\", \"2\");\n\nres4 = metadata_nhgis_datasets_get(api, \"2\");\n\nAn example of the output is:\n. . .\n\n{\n  \"name\": \"NT1\",\n  \"nhgisCode\": \"AAA\",\n  \"description\": \"Total Population\",\n  \"universe\": \"Persons\",\n  \"sequence\": 1,\n  \"datasetName\": \"1790_cPop\",\n  \"nVariables\": [\n    1\n  ]\n}\n\n. . .\n\n\nAccomplished Goals and Future Development\nThe project was a 90-hour small project and during this time the documentation was completed and the testing of the metadata function was done, as well as the migration from Documenter.jl to DocumenterVitepress.jl. During these months some things took longer than I expected because of some problems that occurred, so some things were missing in relation to the original plan. However, this time was useful for learning new things: - I saw how to work with a package under development, how to work with large datasets, and how to write documentation - I had the opportunity to better understand how to work with Git and GitHub - I learned some new things about R, which was a completely unknown language to me. - I deepened my knowledge of Julia, a language I had worked with during my time at university. - I had the chance to work on a large open-source project, to be part of a large community, and to learn how to communicate with it efficiently.\nA special thanks goes to my mentors, Jacob Zelko and Krishna Bhogaonker, for helping me through this process.\nFuture developments of this work could include deepening the work that my mentors and I have started, with the possibility of integrating this package with other machine learning packages in Julia and, from there, doing new analyses of the data in terms of social and geographical implications for health.\n\n\n\n\nCitationBibTeX citation:@online{rocchetti2024,\n  author = {Rocchetti, Michela},\n  title = {GSoC ’24: {IPUMS.jl} {Small} {Project}},\n  date = {2024-08-26},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRocchetti, Michela. 2024. “GSoC ’24: IPUMS.jl Small\nProject.” August 26, 2024."
  },
  {
    "objectID": "JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html",
    "href": "JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html",
    "title": "GSoC ’24: Enhancements to KomaMRI.jl GPU Support",
    "section": "",
    "text": "Hi! 👋\nI am Ryan, an MS student currently studying computer science at the University of Wisconsin-Madison. Looking for a project to work on this summer, my interest in high-performance computing and affinity for the Julia programming language drew me to Google Summer of Code, where I learned about this project opportunity to work on enhancing GPU support for KomaMRI.jl.\nIn this post, I’d like to summarize what I did this summer and everything I learned along the way!\n\nIf you want to learn more about me, you can connect with me here: LinkedIn, GitHub\n\n\n\nWhat is KomaMRI?\nKomaMRI is a Julia package for efficiently simulating Magnetic Resonance Imaging (MRI) acquisitions. MRI simulation is a useful tool for researchers, as it allows testing new pulse sequences to analyze the signal output and image reconstruction quality without needing to actually take an MRI, which may be time or cost-prohibitive.\nIn contrast to many other MRI simulators, KomaMRI.jl is open-source, cross-platform, and comes with an intuitive user interface (To learn more about KomaMRI, you can read the paper introducing it here). However, being developed fairly recently, there are still new features that can be added and optimization to be done.\n\n\nProject Goals\nThe goals outlined by Carlos (my project mentor) and I the beginning of this summer were:\n\nExtend GPU support beyond CUDA to include AMD, Intel, and Apple Silicon GPUs, through the packages AMDGPU.jl, oneAPI.jl, and Metal.jl\nCreate a CI pipeline to be able to test each of the GPU backends\nCreate a new kernel-based simulation method optimized for the GPU, which we expected would outperform array broadcasting\n(Stretch Goal) Look into ways to support running distributed simulations across multiple nodes or GPUs\n\n\n\nStep 1: Support for Different GPU backends\nPreviously, KomaMRI’s support for GPU acceleration worked by converting each array used within the simulation to a CuArray, the device array type defined in CUDA.jl. This was done through a general gpu function. The inner simulation code is GPU-agnostic, as the same operations can be performed on a CuArray or a plain CPU Array. This approach is good for extensibility, as it does not require writing different simulation code for the CPU / GPU, or different GPU backends, and would only work in a language like Julia based on runtime dispatch!\nTo extend this to multiple GPU backends, all that is needed is to generalize the gpu function to convert to either the device types of CUDA.jl, AMDGPU.jl, Metal.jl, or oneAPI.jl, depending on which backend is being used. To give an idea of what the gpu conversion code looked like before, here is a snippet:\nstruct KomaCUDAAdaptor end\nadapt_storage(to::KomaCUDAAdaptor, x) = CUDA.cu(x)\n\nfunction gpu(x)\n    check_use_cuda()\n    return use_cuda[] ? fmap(x -&gt; adapt(KomaCUDAAdaptor(), x), x; exclude=_isleaf) : x\nend\n\n#CPU adaptor\nstruct KomaCPUAdaptor end\nadapt_storage(to::KomaCPUAdaptor, x::AbstractArray) = adapt(Array, x)\nadapt_storage(to::KomaCPUAdaptor, x::AbstractRange) = x\n\ncpu(x) = fmap(x -&gt; adapt(KomaCPUAdaptor(), x), x)\nThe fmap function is from the package Functors.jl and can recursively apply a function to a struct tagged with @functor. The function being applied is adapt from Adapt.jl, which will call the lower-level adapt_storage function to actually convert to / from the device type. The second parameter to adapt is what is being adapted, and the first is what it is being adapted to, which in this case is a custom adapter struct KomaCUDAAdapter.\nOne possible approach to generalize to different backends would be to define additional adapter structs for each backend and corresponding adapt_storage functions. This is what the popular machine learning library Flux.jl does. However, there is a simpler way!\nEach backend package (CUDA.jl, Metal.jl, etc.) already defines adapt_storage functions for converting different types to / from corresponding device type. Reusing these functions is preferable to defining our own since, not only does it save work, but it allows us to rely on the expertise of the developers who wrote those packages! If there is an issue with types being converted incorrectly that is fixed in one of those packages, then we would not need to update our code to get this fix since we are using the definitions they created.\nOur final gpu and cpu functions are very simple. The backend parameter is a type derived from the abstract Backend type of KernelAbstractions.jl, which is extended by each of the backend packages:\nimport KernelAbstractions as KA\n\nfunction gpu(x, backend::KA.GPU)\n    return fmap(x -&gt; adapt(backend, x), x; exclude=_isleaf)\nend\n\ncpu(x) = fmap(x -&gt; adapt(KA.CPU(), x), x, exclude=_isleaf)\nThe other work needed to generalize our GPU support involved switching to use package extensions to avoid having each of the backend packages as an explicit dependency, and defining some basic GPU functions for backend selection and printing information about available GPU devices. The pull request for adding support for multiple backends is linked below:\n\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/405\n\n\n\nStep 2: Buildkite CI\nAt the time the above pull request was merged, we weren’t sure whether the added support for AMD and Intel GPUs actually worked, since we only had access to CUDA and Apple Silicon GPUs. So the next step was to set up a CI to test each GPU backend. To do this, we used Buildkite, which is a CI platform that many other Julia packages also use. Since there were many examples to follow, setting up our testing pipeline was not too difficult. Each step of the pipeline does the required environment setup and then calls Pkg.test() for KomaMRICore. As an example, here is what the AMDGPU step of our pipeline looks like:\n      - label: \"AMDGPU: Run tests on v{{matrix.version}}\"\n        matrix:\n          setup:\n            version:\n              - \"1\"\n        plugins:\n          - JuliaCI/julia#v1:\n              version: \"{{matrix.version}}\"\n          - JuliaCI/julia-coverage#v1:\n              codecov: true\n              dirs:\n                - KomaMRICore/src\n                - KomaMRICore/ext\n        command: |\n          julia -e 'println(\"--- :julia: Instantiating project\")\n              using Pkg\n              Pkg.develop([\n                  PackageSpec(path=pwd(), subdir=\"KomaMRIBase\"),\n                  PackageSpec(path=pwd(), subdir=\"KomaMRICore\"),\n              ])'\n          \n          julia --project=KomaMRICore/test -e 'println(\"--- :julia: Add AMDGPU to test environment\")\n              using Pkg\n              Pkg.add(\"AMDGPU\")'\n          \n          julia -e 'println(\"--- :julia: Running tests\")\n              using Pkg\n              Pkg.test(\"KomaMRICore\"; coverage=true, test_args=[\"AMDGPU\"])'\n        agents:\n          queue: \"juliagpu\"\n          rocm: \"*\"\n        timeout_in_minutes: 60\nWe also decided that in addition to a testing CI, it would also be helpful to have a benchmarking CI to track performance changes resulting from each commit to the main branch of the repository. Lux.jl had a very nice-looking benchmarking page, so I decided to look into their approach. They were using github-action-benchmark, a popular benchmarking action that integrates with the Julia package BenchmarkTools.jl. github-action-benchmark does two very useful things:\n\nCollects benchmarking data into a json file and provides a default index.html to display this data. If put inside a relative path in the gh-pages branch of a repository, this results in a public benchmarking page which is automatically updated after each commit!\nComments on a pull request with the benchmarking results compared with before the pull request. Example: https://github.com/JuliaHealth/KomaMRI.jl/pull/442#pullrequestreview-2213921334\n\nThe only issue was that since github-action-benchmark is a github action, it is meant to be run within github by one of the available github runners. While this works for CPU benchmarking, only Buildkite has the CI setup for each of the GPU backends we are using, and Lux.jl’s benchmarks page only included CPU benchmarks, not GPU benchmarks (Note: we talked with Avik, the repository owner of Lux.jl, and Lux.jl has since adopted the approach outlined below to display GPU and CPU benchmarks together). I was not able to find any examples of other julia packages using github-action-benchmark for GPU benchmarking.\nFortunately, there is a tool someone developed to download results from Buildkite into a github action (https://github.com/EnricoMi/download-buildkite-artifact-action). This repository only had 1 star when I found it, but it does exactly what we needed: it identifies the corresponding Buildkite build for a commit, waits for it to finish, and then downloads the artifacts for the build into the github action it is being run from. With this, we were able to download the Buildkite benchmark results from a final aggregation step into our benchmarking action and upload to github-action-benchmark to publish to either the main data.js file for our benchmarking website, or pull request.\nOur final benchmarking page looks like this and is publicly accessible:\n\nOne neat thing about github-action-benchmark is that the default index.html is extensible, so even though by deault it only shows time, the information for memory usage and number of allocations is also collected into the json file, and can be displayed as well.\nA successful CI run on Buildkite Looks like this:\n\nThe pull requests for creating the CI testing and benchmarking pipeline, and changing the index.html for our benchmark page are listed below:\n\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/411\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/418\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/421\n\n\n\nStep 3: Optimization\nWith support for multiple backends enabled, and a robust CI, the next step was to optimize our simulation code as much as possible. Our original idea was to create a new GPU-optimized simulation method, but before doing this we wanted to look more at the existing code and optimize for the CPU.\nThe simulation code is solving a differential equation (the [Bloch equations(https://en.wikipedia.org/wiki/Bloch_equations)]) over time. Most differential equation solvers step through time, updating the current state at each time step, but our previous simulation code, more optimized for the GPU, did a lot of computations across all time points in a simulation block, allocating a matrix of size Nspins by NΔt each time this was done. Although this is beneficial for the GPU, where there are millions of threads available on which to parallelize these computations, for the CPU it is more important to conserve memory, and the aforementioned approach of stepping through time is preferable.\nAfter seeing that this approach did help speed up simulation time on the CPU, but was not faster on the GPU (7x slower for Metal!) we decided to separate our simulation code for the GPU and CPU, dispatching based on the KernelAbstractions.Backend type depending on if it is &lt;:KernelAbstractions.CPU or &lt;:KernelAbstractions.GPU.\nOther things we were able to do to speed up CPU computation time:\n\nPreallocating each array used inside the core simulation code so it can be re-used from one simulation block to the next.\nSkipping an expensive computation if the magnetization at that time point is not added to the final signal\nEnsuring that each statement is fully broadcasted. We were surprised to see the difference between the following examples:\n\n#Fast\nBz = x .* seq.Gx' .+ y .* seq.Gy' .+ z .* seq.Gz' .+ p.Δw ./ T(2π .* γ)\n\n#Slow\nBz = x .* seq.Gx' .+ y .* seq.Gy' .+ z .* seq.Gz' .+ p.Δw / T(2π * γ)\n\nUsing the cis function for complex exponentiation, which is faster than exp\n\nWith these changes, the mean improvement in simulation time aggregating across each of our benchmarks for 1, 2, 4, and 8 CPU threads was ~4.28. For 1 thread, the average improvement in memory usage was 90x!\nThe next task was optimizing the simulation code for the GPU. Although our original idea was to put everything into one GPU kernel, we found that the existing broadcasting operations were already very fast, and that custom kernels we wrote were not able to outperform the previous implementation. The Julia GPU compiler team deserves a lot of credit for developing such fast broadcasting implementations!\nHowever, this does not mean that we were unable to improve the GPU simulation time. Similar to with the CPU, preallocation made a substantial difference. Parallelizing as much work as possible across the time points for a simulation block was also found to beneficial. For the parts that needed to be done sequentially, a custom GPU kernel was written which used the KernelAbstractions.@localmem macro for arrays being updated at each time step to yield faster memory access.\nThe mean speedup we saw across the 4 supported GPU backends was 4.16, although this varied accross each backend (for example, CUDA was only 2.66x faster while oneAPI was 28x faster). There is a remaining bottleneck in the run_spin_preceession! function having to do with logical indexing that I was not able to resolve, but could be solved in the future to speed up the GPU simulation time even further!\nThe pull requests optimizing code for the CPU and GPU are below:\n\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/443\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/459\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/462\n\n\n\n4. Step 4: Distributed Support\nThis last step was a stretch goal for exploring how to add distributed support to KomaMRI. MRI simulations can become quite large, so it is useful to be able to distribute work across either multiple GPUs or multiple compute nodes.\nA nice thing about MRI simulation is the independent spin property: if a phantom object (representing, for example a brain tissue slice) is divided into two parts, and each part is simulated separately, the signal result from simulating the whole phantom will be equal to the sum of the signal results from simulating each subdivision of the original phantom. This makes it quite easy to distribute work, either across more than one GPU or accross multiple compute nodes.\nThe following scripts worked, with the only necessary code change to the repository being a new + function to add two RawAcquisitionData structs:\n#Use multiple GPUs:\nusing Distributed\nusing CUDA\n\n#Add workers based on the number of available devices\naddprocs(length(devices()))\n\n#Define inputs on each worker process\n@everywhere begin\n    using KomaMRI, CUDA\n    sys = Scanner()\n    seq = PulseDesigner.EPI_example()\n    obj = brain_phantom2D()\n    #Divide phantom\n    parts = kfoldperm(length(obj), nworkers())\nend\n\n#Distribute simulation across workers\nraw = Distributed.@distributed (+) for i=1:nworkers()\n    KomaMRICore.set_device!(i-1) #Sets device for this worker, note that CUDA devices are indexed from 0\n    simulate(obj[parts[i]], seq, sys)\nend\n#Use multiple compute nodes\nusing Distributed\nusing ClusterManagers\n\n#Add workers based on the specified number of SLURM tasks\naddprocs(SlurmManager(parse(Int, ENV[\"SLURM_NTASKS\"])))\n\n#Define inputs on each worker process\n@everywhere begin\n    using KomaMRI\n    sys = Scanner()\n    seq = PulseDesigner.EPI_example()\n    obj = brain_phantom2D()\n    parts = kfoldperm(length(obj), nworkers())\nend\n\n#Distribute simulation across workers\nraw = Distributed.@distributed (+) for i=1:nworkers()\n    simulate(obj[parts[i]], seq, sys)\nend\nPull reqeust for adding these examples to the KomaMRI documentation: https://github.com/JuliaHealth/KomaMRI.jl/pull/468\n\n\nConclusions / Future Work\nThis project was a 350-hour large project, since there were many goals to accomplish. To summarize what changed since the beginning of the project:\n\nAdded support for AMDGPU.jl, Metal.jl, and oneAPI.jl GPU backends\nCI for automated testing and benchmarking accross each backend + public benchmarks page\nSignificantly faster CPU and GPU performance\nDemonstrated distributed support and examples added in documentation\n\nFuture work could look at ways to further optimize the simulation code, since despite the progress made, I believe there is more work to be done! The aforementioned logical indexing issue is still not resolved, and the kernel used inside the run_spin_excitation! function has not been profiled in depth. KomaMRI is also looking into adding support for higher-order ODE methods, which could require more GPU kernels being written.\n\n\nAcknowledgements\nI would like to thank my mentor, Carlos Castillo, for his help and support on this project. I would also like to thank Jakub Mitura, who attended some of our meetings to help with GPU optimization, Dilum Aluthge who helped set up our BuildKite pipeline, and Tim Besard, who answered many GPU-related questions that Carlos and I had.\n\n\n\n\nCitationBibTeX citation:@online{kierulf2024,\n  author = {Kierulf, Ryan},\n  title = {GSoC ’24: {Enhancements} to {KomaMRI.jl} {GPU} {Support}},\n  date = {2024-08-30},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKierulf, Ryan. 2024. “GSoC ’24: Enhancements to KomaMRI.jl GPU\nSupport.” August 30, 2024."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "",
    "text": "Hi everyone! I’m Kosuri Lakshmi Indu, a third-year undergraduate student in Computer Science and an aspiring GSoC 2025 contributor. My interest in using data science for public health led me to the JuliaHealth community and, under the mentorship of Jacob S. Zelko, I began working on a project titled PLP-Pipeline. This project focuses on building modular, efficient tooling for Patient-Level Prediction (PLP) entirely in Julia, using the OMOP Common Data Model (OMOP CDM).\nIn this post, I’ll walk through the first part of a three-part blog series documenting my work on building a Patient-Level Prediction (PLP) pipeline in Julia. Each post focuses on a different stage of the pipeline:\n\nFrom Research Question to Cohort Construction (this post)\nFrom Raw Clinical Data to Predictive Models\nLessons Learned, Key Challenges, and What Comes Next\n\nIn Part 1, we’ll start at the very beginning-formulating the research question, exploring the OMOP CDM, setting up the local database, and defining target and outcome cohorts using Julia tools. Whether you’re a health researcher, a GSoC aspirant, or a Julia enthusiast, I hope this gives you a clear and accessible introduction to how observational health research can be made more composable, reproducible, and efficient using Julia.\nYou can find my PLP-Pipeline Project Link Here\nLinkedIn | GitHub"
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#what-is-observational-health",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#what-is-observational-health",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "What is Observational Health?",
    "text": "What is Observational Health?\nObservational health research examines real-world patient data such as electronic health records (EHRs), claims, and registries to understand health and disease outside of controlled trial environments. This type of research plays a vital role in informing decisions by clinicians, policymakers, and researchers, especially when addressing population-level health questions and disparities.\nA core aspect of observational health is the use of phenotype definitions, which describe a specific set of observable patient characteristics (e.g., diagnosis codes, symptoms, demographics, biomarkers) that define a population of interest. Creating accurate and reproducible phenotype definitions is essential for ensuring research validity. However, challenges such as missing data, demographic biases, and inconsistently recorded information can significantly impact the reliability of these definitions.\nTo support reproducible research at scale, communities like OHDSI (Observational Health Data Sciences and Informatics) have developed standards such as the OMOP Common Data Model (CDM) and workflows for developing computable phenotype definitions.\nIn our work, we utilize observational health data already structured through the OMOP Common Data Model (CDM). We construct patient cohorts based on existing phenotype definitions. These cohorts then serve as the basis for building patient-level prediction models, enabling us to explore and generate insights that can support data-driven clinical decision-making."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#what-is-the-omop-cdm",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#what-is-the-omop-cdm",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "What Is the OMOP CDM?",
    "text": "What Is the OMOP CDM?\nThe Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) is a standardized framework for organizing and analyzing observational healthcare data. The OMOP CDM converts diverse sources of health data into a common format that supports large-scale, systematic analysis.\nThe OMOP CDM organizes data into a consistent set of relational tables like condition_occurrence, drug_exposure, person, visit_occurrence etc, using standardized vocabularies. These tables are interconnected, allowing for relational analysis across a patient’s medical history.\nBy transforming diverse healthcare datasets into a common format, the OMOP CDM enables reproducibility, interoperability, and large-scale studies across institutions and populations.\n\n\n\nOMOP Common Data Model"
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#what-is-patient-level-prediction-plp",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#what-is-patient-level-prediction-plp",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "What is Patient-Level Prediction (PLP)?",
    "text": "What is Patient-Level Prediction (PLP)?\nPatient-Level Prediction (PLP) is a data-driven approach that uses machine learning or statistical models to estimate the risk of specific clinical outcomes for individual patients, based on their historical healthcare data.\nThe key goal of PLP is to answer personalized clinical questions like:\n\n“For patients who present with chest pain leading to a hospital visit, can we predict which of these patients will go on to experience a heart attack after their hospital visit?”\n\nPLP focuses on using observational patient data such as diagnoses, medications, procedures, and demographics - to predict individual-level risks of future health events. While it may sound similar to precision medicine, there’s a key distinction: precision medicine aims to tailor treatment plans based on a patient’s genetics, environment, and lifestyle, whereas PLP is specifically about forecasting outcomes for individual patients using data-driven models. These predictions can support timely and personalized clinical decisions."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#why-plp-in-julia",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#why-plp-in-julia",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "Why PLP in Julia?",
    "text": "Why PLP in Julia?\nWhile established PLP workflows are well-supported in R through OHDSI’s suite of packages, our work explores an alternative approach using Julia - a high-performance language that enables building efficient and reproducible pipelines from end to end.\nJulia offers several advantages that make it well-suited for observational health research:\n\nComposability: Julia’s modular design supports reusable components, making PLP pipelines easier to maintain and extend.\nSpeed: With performance comparable to C, Julia efficiently handles large, complex healthcare datasets.\nUnified Ecosystem: Tools like OHDSICohortExpressions.jl, DataFrames.jl, MLJ.jl etc. integrate seamlessly, enabling cohort definition, data transformation, and modeling within one consistent environment.\n\nAdditionally, Julia features a rich and growing ecosystem with many tools for scientific computing and data science, making it a strong alternative for modern health informatics workflows.\n\n\n\nJulia Equivalents"
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#methodologies-from-the-paper",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#methodologies-from-the-paper",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "Methodologies from the Paper",
    "text": "Methodologies from the Paper\n\nStandardized Framework for PLP - Outlines a consistent process for building patient-level prediction models across datasets and settings.\nDefining the Prediction Problem - Emphasizes clear definition of target, outcome, and time-at-risk for valid predictions.\nCohort Definition and Data Extraction - Uses standardized OMOP CDM cohorts to ensure reproducibility and consistent data extraction.\nFeature Construction - Derives meaningful predictors from observational data like conditions and demographics.\nModel Training and Evaluation - Trains ML models and evaluates them using metrics like AUC and cross-validation.\n\nWe are adapting this framework for our PLP pipeline to ensure a consistent approach."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#cohort-construction",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#cohort-construction",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "Cohort Construction",
    "text": "Cohort Construction\nCohorts are groups of patients defined by specific criteria that are relevant to the research question. For this task, two main cohorts need to be defined:\n\nTarget Cohort: This refers to the group of patients we want to make predictions for. In our case, it includes patients who have been diagnosed with hypertension. These patients serve as the starting point for our prediction timeline.\nOutcome Cohort: This refers to the clinical event we aim to predict. In our case, it includes patients from the target cohort who are subsequently diagnosed with diabetes within a specified time window. This event marks the outcome that our model will learn to forecast.\n\nThese cohort definitions are central to structuring the data pipeline, as they form the foundation for downstream tasks like feature extraction, model training, and evaluation."
  },
  {
    "objectID": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#acknowledgements",
    "href": "JuliaHealthBlog/posts/indu-plp-part1/plp-part1.html#acknowledgements",
    "title": "PLP-Pipeline Series Part 1: From Research Question to Cohort Construction",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Jacob Zelko for his mentorship, clarity, and constant feedback throughout the project. I also thank the JuliaHealth community for building an ecosystem where composable science can thrive.\nJacob S. Zelko: aka, TheCedarPrince\nNote: This blog post was drafted with the assistance of LLM technologies to support grammar, clarity and structure."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JuliaHealth",
    "section": "",
    "text": "Transforming Health Research!\n    \n    \n        Improving medicine, health and bio-medical research using the power of Julia.\n    \n\n\n    Explore Packages\n    Below are some of the powerful packages developed by the community.\n    \n        \n            \n                \n                KomaMRI.jl\n                \n                    KomaMRI.jl is a Julia package for highly efficient ⚡ MRI simulations. KomaMRI was built from the ground up to be: easy to use, extensible, cross-platform, and powered by open-source community standards.\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                NeuroAnalyzer.jl\n                \n                    NeuroAnalyzer is a Julia toolbox for analyzing neurophysiological data. Currently it covers importing, editing, processing, visualizing, and analyzing EEG, MEP and EDA data. Preliminary functionality is also available for MEG, NIRS, ECoG, SEEG and iEEG recordings.\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                DICOMTree.jl\n                \n                    A little Julia package for visualizing DICOM file metadata in the form of a tree. The package have been tested with CT Scanner, RTDose and RTStruct files.\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                ICD_GEMs.jl\n                \n                    ICD_GEMs.jl is a Julia package that allows to translate ICD-9 codes in ICD-10 and viceversa via the General Equivalence Mappings (GEMs) of the International Classification of Diseases (ICD).\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                MedEye3d.jl\n                \n                    Julia library for visualization and annotation medical images, specialized particularly for rapid development segmentation of 3 dimensional images like CT or PET/CT scans. Has full support of nuclear medicine Data.\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                    \n                        And many more...\n                    \n                \n            \n        \n    \n\n\n    Similar Organizations\n    We are not the only one, there are other communities researching on various similar aspects.\n    \n        \n            \n                \n                BioJulia\n                \n                    Fast, open, extensible software for bioinformatics and computational biology.\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                EcoJulia\n                \n                    Ecological research in the Julia language.\n                \n                \n                    \n                        Github\n                    \n                    \n                        Website\n                    \n                \n            \n        \n        \n            \n                \n                    \n                        And many more..."
  },
  {
    "objectID": "pages/meeting_notes.html",
    "href": "pages/meeting_notes.html",
    "title": "Meeting Notes",
    "section": "",
    "text": "These are the public notes for the JuliaHealth Community. Notes are published publicly here and are available for comments and review on the public HackMD. Additionally, the notes are hosted publicly on the GitHub and are open for PRs or edits as needed"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-summary-americaseuropeafrica-specific",
    "href": "pages/meeting_notes.html#meeting-summary-americaseuropeafrica-specific",
    "title": "Meeting Notes",
    "section": "Meeting Summary (Americas/Europe/Africa Specific)",
    "text": "Meeting Summary (Americas/Europe/Africa Specific)\nIn Attendance: Jacob Zelko, Anshul Singhvi, Adam Wysokiński, Aurora Rossi, Dan Getz, Luna Fazio, Jay Landge, Edwin Mkwanazi, Alice Piller, Thembi Ndimande, Siyabonga Nxumalo, Hlengiwe, Muhammad Mahmoud, Jan Zubik, Sfundo Khumalo, Carlos Castillo Passi, Ram Samarth, Dina Khalid\nLocation: Virtual (Northeastern University Zoom)\nSummary: Introducing new JuliaHealth projects, JuliaHealth blog, Google Summer of Code, and planning a JuliaHealth Day\nKeywords: #juliahealth #meeting #americas #africa #europe #neuro #imaging #gsoc #planning"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes",
    "href": "pages/meeting_notes.html#meeting-outcomes",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\nJacob follows up with Carlos and Boris about synthetic MRI generation\n\n\n\nLong-Term Outcomes"
  },
  {
    "objectID": "pages/meeting_notes.html#notes",
    "href": "pages/meeting_notes.html#notes",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nAnnouncements:\n\n\nMeeting recording logistics\n\n\nNew member introductions\n\nLuna Fazio\n\nStatistics PhD\nComing back to epidemiology\nComing back to health roots\n\nAdam Wysokiński\n\nCreator of NeuroAnalyzer.jl\nPsychiatrist\nMany different modalities of research\n\nAurora Rossi\n\nFunctional MRI\nPhD student\n\nAlice Piller\n\nApplying Julia in bioinformatics\n\nEdwin Mkwanazi\n\nJulia in clinical trials\nLearn more about how to implement more in Julia\n\nCarlos Castillo\n\nCreator of KomaMRI\nPhD student\n\n\nNew contributor round-up!\n\nKomaMRI\nNeuroAnalyzer Adam Wysokinski\n\nJuliaHealth News\n\nNortheastern University RISE Conference\nA JuliaHealth Blog?!?!?\n\nTask Follow-ups\n\nJacob follows up with Carlos and Boris about synthetic MRI generation\n\nGSoC + JuliaHealth\n\nProjects\nImportant dates\nOpen discussion\n\nBrainstorming a JuliaHealth Day\n\nJuliaHealth is growing rapidly!!!\nMight be confusing about where to go/get started\n\nThree core ares\n\nLuna\n\nHad a mixture of working with different data\nPublic health approach\nI as a doctor want to predict for patients\nPerhaps it would be interesting to see what problems they have\n\nPossible approaches\n\n\nJan\n\nMore about pipelines\nWhat’s their strength in practice\nSeeing pipelines in action\n\nRam\n\nHow is Julia being used in health already?\n\n\nGlass Notebooks\n\nCreated by Dale Black\nLink: https://glassnotebook.io\n\nUpcoming and ongoing research opportunities\n\nObservational Health Research at Northeastern Uni\n\nUpcoming Events\n\n\nJuliaCon 2024\n\n\nOpen Discussion"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-summary-americaseuropeafrica-specific-1",
    "href": "pages/meeting_notes.html#meeting-summary-americaseuropeafrica-specific-1",
    "title": "Meeting Notes",
    "section": "Meeting Summary (Americas/Europe/Africa Specific)",
    "text": "Meeting Summary (Americas/Europe/Africa Specific)\nIn Attendance: Jay Sanjay, Abhirath Anand, Carlos Castillo, Boris Enrique, Jacob Zelko\nLocation: Virtual (JuliaHealth Google Meet)\nSummary: Medical imagining, fairness and health equity in observational health, and dashboards!\nKeywords: #juliahealth #meeting #americas #africa #europe #fairness #koma #fairness #dashboards"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-1",
    "href": "pages/meeting_notes.html#meeting-outcomes-1",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\nJacob follows up with Carlos and Boris about synthetic MRI generation\n\nPulls in Jakub and Zachary to discussion\n\n\n\n\nLong-Term Outcomes"
  },
  {
    "objectID": "pages/meeting_notes.html#notes-1",
    "href": "pages/meeting_notes.html#notes-1",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nNew member introductions\n\nCarlos Castillo\n\nKing’s College London\nPhD student\n\nAbhirath Anand\n\nFinal year undergraduate\nCurious about getting more into life sciences\nBiology and healthcare\n\n\nAnnouncements:\n\nNew meeting times\n\nLast Thursday of every month at 12PM EST\n\nWhy two separate meetings?\n\nOne for Asia/Oceania\n\nThanks Jay Sanjay for running this!!!\n\nOne for Americas/Africa/Europe\nTrying to improve accessibility and inclusion\n\nMeeting recordings\n\nGoing forward, meetings will be recorded\nAdded to a playlist on Julia YouTube page\n\n\nNew contributor round-up!\n\nNothing this meeting\n\nRunning tasks follow-ups:\n\nNothing this meeting\n\nPresentation by Carlos Castillo Passi on GSoC projects on medical imaging.\n\n\nWritten using CuDA\nDoing MRI simulation very quickly\n\nCan be used for machine learning overview\n\nBuilt around several packages with MRI\n\nIncredible work with coverage\n\nSuper friendly GUI\nBloch equations are hard to understand\nGSoC Project\n\nTrying to do actual kernel programming\nKernelAbstractions.jl\nSolving DifferentialEquations.jl\nBoost speed\nImplement new algorithms\nSuggested skills\n\nExperience with Julia\nMRI concepts\nGPU programming\n\nGoals:\n\nNew Bloch kernel methods\nFurther tests on build kite/GPU testing\nDocumentation\n\n\n\n\nFairness and health equity within Observational Health Research\n\nAssessing phenotype fairness\nForthcoming package\nWork done so far\nPaper reference: https://arxiv.org/pdf/2203.05174.pdf\n\nCreating dashboards for JuliaHealth\n\nAnnouncement from Genie.jl\nCustom dashboard components\nQuestion: What would this look like for JuliaHealth?\n\nCreate a standard interface across JuliaHealth packages\nCan interface with a JuliaHealthDashboards package\n\nHealthDashboard.jl?\nCustom components for the general JuliaHealth ecosystem could be housed in package\n\nResearchers can easily build together commonly used health dashboards\n\n\nEvent Reminders\n\nGoogle Summer of Code\nJuliaCon 2024\n\nUpcoming and ongoing research opportunities\n\nObservational Health Research at Northeastern Uni\nGlass Notebooks from Dale Black (Not Discussed; saved for next month)"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-summary-oceaniaasia-specific",
    "href": "pages/meeting_notes.html#meeting-summary-oceaniaasia-specific",
    "title": "Meeting Notes",
    "section": "Meeting Summary (Oceania/Asia specific)",
    "text": "Meeting Summary (Oceania/Asia specific)\nIn Attendance: Jay Sanjay, Abhirath Anand, Jacob Zelko\nLocation: Virtual (JuliaHealth Google Meet)\nSummary: Overview of the Oceania/Asia specific JuliaHealth monthly meeting\nKeywords: #juliahealth #meeting #asia #oceania #llms #beginner"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-2",
    "href": "pages/meeting_notes.html#meeting-outcomes-2",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\n\nLong-Term Outcomes"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-notes",
    "href": "pages/meeting_notes.html#meeting-notes",
    "title": "Meeting Notes",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\nAbhirath Anand\n\nFinal year CS student in India.\nFormer GSoCer.\n\nWorked on MetalHead.jl\nNo longer quite interested in Computer CV\n\nInterested in JuliaHealth.\n\nPeople excited about separate JuliaHealth meeting\n\nGrown to a separate JuliaHealth meeting for Oceania/Asia specific times.\nWanted more people to join .\n\nDifferent packages and ideas\n\nJuliaHealthLLMs\n\nHow can we use LLMs for JuliaHealth?\n\n\nHow to get started with JuliaHealth - Abhirath\n\nMedical imaging looks well-aligned but want to explore some different.\nWhat is the observational health subecosystem?\n\nGo through documentation of JuliaHealth.\nJay can send some."
  },
  {
    "objectID": "pages/meeting_notes.html#agenda",
    "href": "pages/meeting_notes.html#agenda",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nNew member introductions\nNew contributor round-up!\nRunning tasks follow-ups:\nState of the JuliaHealth community discussion\n\nTalking about the different aspects of the JuliaHealth community\n\nMapping the JuliaHealth community\n\nAccomplishments throughout the year\n\nJuliaCon 2023\nGSoC\nPublications/etc.\n\nOpen Problems and ongoing work\n\nTechnical problems\nMaking JuliaHealth more accessible for all\n\nFuture goals for the JuliaHealth ecosystem\nOpen discussion\n\nJuliaCon 2024!\nGoogle Summer of Code Discussion\n\nWhat it is\nProposed projects and ideas\nOpen discussion\n\nCalls for collaboration\nOpen discussion"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-3",
    "href": "pages/meeting_notes.html#meeting-outcomes-3",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\nJacob follows-up with Zach.\n\n\n\nLong-Term Outcomes\n\nIncreasing code ownership."
  },
  {
    "objectID": "pages/meeting_notes.html#notes-2",
    "href": "pages/meeting_notes.html#notes-2",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nIntroductions\n\nDivital coder\n\nAspiring contributor for the 2024 Julia Organization.\n\n\nContributor Round-Up\n\nShout outs to Farreeda for working on JuliaHealth Observational Health Sub-ecosystem Juliacon proceddings paper.\nShout outs to Jay-Sanjay for tagging new release of OMOPCDMCohortCreator.\n\nState of the JuliaHealth community discussion\n\nTalking about the different aspects of the JuliaHealth community\n\nMapping the JuliaHealth community\n\nAccomplishments throughout the year\n\nJuliaCon 2023\n\nBirds of Feather: Julia for Health and Medicine – Dilum Aluthge, Jacob Zelko\n100 Million Patients: Julia for international Health studies\n\nFirst ever JuliaHealth GSoC fellow - Fareeda Abdelazeez\nODHSI Global Symposium 2023\n\nOpen Problems and ongoing work\n\nTechnical problems\nMaking JuliaHealth more accessible for all\nFuture goals for the JuliaHealth ecosystem\nExpanding the OMOPCDM for hospital price transparency and transparency coverage.\n\nOpen discussion\n\nOpen discussion on standards across JuliaHealth\nZach happy to support and think around this\nSchedule one-off discussion\nMaking juliahealth calls more Europe+asia/pacific friendly. Suggestions to have a one meet each for american time zone separate and one for asia/pacific time zone\n\n\nJuliaCon 2024!\n\nProposal-a-thon\n\nGoogle Summer of Code Discussion\n\nWhat is GSoC/JSoC ?\nProposed projects and ideas\nMedPipe3D\n\nLoading medical imaging data\nModeling perspective most generally developed\nSuper-voxels image mapping\nEdge matching; can make this code within Julia vs. Cpp\nDisplay borders of images\nIntegrate segmentation like rotations recalling gamma.\nAdd basic post-processing like largest corrected components.\nAdd patch based data loading with probabilistic oversampling.\n\nOpen discussion\n\nCalls for collaboration\nOpen discussion\n\nJuliaCon 2024 and Proposal-a-thon\nAddressing the “Paradox of Composition”"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-1",
    "href": "pages/meeting_notes.html#agenda-1",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nNew member introductions\nNew contributor round-up!\nRunning tasks follow-ups:\n\n\nShort-term task follow-ups:\n\nJacob shares info on waste water management + viral load information\n\nLong-term task follow-ups:\n\nCreating a template repository\n\n\n\nPresentation by Jakub Mitura on sub-ecosystem he created for working with CT, PET, and other medical imaging types of data.\nDebrief from OHDSI Symposium (Observational Health research venue)\nGoogle Summer of Code Project Discussion\n\n\nJuliaHealth documentation improvement\nObservational Health Tooling improvements and discussion\nVisualization tools\n\n\nUpcoming and ongoing research opportunities\n\n\nCall for collaboration on using JuliaHealth observational health tools for multi-site study\n\n\nMedical Imaging Extension for Real World Evidence exploration\nOpen discussion"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-4",
    "href": "pages/meeting_notes.html#meeting-outcomes-4",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\nJacob intro’s Phil and Jakub\nJacob follows-up with Phil\n\n\n\nLong-Term Outcomes\n\nCreate a template repository for JuliaHealth"
  },
  {
    "objectID": "pages/meeting_notes.html#notes-3",
    "href": "pages/meeting_notes.html#notes-3",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nNew member introductions\n\nPhil Vernes\n\nWorks at JuliaHub\nDeveloping platform for running Julia jobs\nMany people at JuliaHub using tools within epi\nCan solve many problems in DSL\n\nJay Sanjay\n\nStarted contributing to the JuliaHealth ecosystem\nLooking forward to collaborating\n\n\nRunning tasks follow-ups:\n\nShort-term task follow-ups:\n\nJacob shares info on waste water management + viral load information\n\nLong-term task follow-ups:\n\nCreating a template repository\n\nWe need to have a data structure to hold metadata (DICOM, NIFTI, etc.)\nJuliaNeuro\nHDF5 for long-term storage\n\nWould be great to see everyone using this\nTo work on this to bring this together\nMultiple packages could have same\n\n\n\n\nPresentation by Jakub Mitura on sub-ecosystem he created for working with CT, PET, and other medical imaging types of data.\n\nCreated three packages\nMainly talking about MedEye3D\nSegment data and iterate to see what is going on\nWanted to create tools for everything around model creation\nWanted to make a viewer that is well-suited for the Julia ecosystem\n\nMost medical viewers are quite “old”\nNot really dynamic\nHard to show changes within run-time\n\nEasy to get big increase in Julia\n\nUsually something like 10x’s faster\n\nWe do not yet standardize way to load data\nMetadata is saved to HDF5 format\nCan introduce dynamic annotations\nCan have layers and switch on and switch layers\nCan annotate for saying where is the problem in the viewer\nViewer can dynamically update\nQuestions\n\nTested some semi-automatic algorithms\nDo evaluate repeat\nMakes it faster for evaluation and reviewing of medical images\nDepends on OpenGL and NVIDIA drivers\nWorking on Docker container that keeps\nWhat segmentation algorithm? Approach?\n\nBased on Gaussian probability distributions\nSome relaxation applied\nBased mainly on the units and different kinds\nBecoming more interested in transformers\nImplemented in JAX but want to bring it into Julia\nSegmentation for bladder cancer in image analysis\nRestarted work recently in Julia\n\nWould be useful for others?\n\nNew segmentation for other ecosystem within Julia\n\n\n\nUpcoming and ongoing research opportunities\nCall for collaboration on using JuliaHealth observational health tools for multi-site study\nMedical Imaging Extension for Real World Evidence exploration\n\nIdea was to implement package for medical imaging\nPillars\n\nComputing statistics across medical imaging\nComplete datasets for experimenting\nFeature segmentation and scanning\n\nAlign probabilistic model between different scans\nBecome easier for physicians\n\nML model for complex models for image segmentation\n\nThing to consider – need more robustness for image alignment?\n\nSome transformations are relatively easier to repair\nElastic deformations"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-2",
    "href": "pages/meeting_notes.html#agenda-2",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nNew member introductions\nRunning tasks follow-ups:\n\n\nShort-term task follow-ups:\nLong-term task follow-ups:\n\ni. Creating a template repository\n\nInfectious Disease load for various sewage water data\nUpcoming research opportunities and events\n\n\nNot too early to start thinking about GSoC\nJulia and OHDSI Symposium\n\n\nOpen discussion"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-5",
    "href": "pages/meeting_notes.html#meeting-outcomes-5",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\nJacob shares info on waste water management + viral load information\n\n\n\nLong-Term Outcomes"
  },
  {
    "objectID": "pages/meeting_notes.html#notes-4",
    "href": "pages/meeting_notes.html#notes-4",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nNew member introductions\n\nTiem van der Deure\n\nUniversity of Copenhagen PhD\nVector-borne Disease Modeling\nEpidemiological modeling and climate effects on health\nRafael Schoueten\n\nScott Jones\n\nHeavily involved in healthcare IT\n\ndx/dt\n\nGoogle Summer of Code\n\nDidn’t know it existed\nGoogle Season of Docs is great too\n\nBest for long-term maintenance\nIn the Julia docs ecosystem is kinda a mess\n\n\nOHDSI + Julia\n\nHow difficult it has been to work with EHR from EPIC\n\nStill a bit manual but getting better\n\nTuring modeling “making them work”\n\nGetting them to run\n\nMaking it run fast enough\nMuch easier to use but not as fast as otherwise\n\nExtremely mathy very fast\n\n\nSewage water information for disease population estimations\n\nWeekly excerpt\nInfectious disease doctor\n\nWould be really neat to make some kind of app\nTo check wastewater\n\nPropensity of viruses in ER\n\n\nPhysician testing for rough understanding of what is happening in community\n\nYou don’t just need to look for one disease, but rather multiple co-factors\n\nMany healthcare systems put together monitoring systems\n\nNHS (in UK) dismantled their monitoring systems"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-3",
    "href": "pages/meeting_notes.html#agenda-3",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nNew member introductions\nRunning tasks follow-ups:\n\nShort-term task follow-ups:\nLong-term task follow-ups:\n\nCreating a template repository\n\n\nUpcoming research opportunities and events\n\nNot too early to start thinking about GSoC\nJulia and OHDSI Symposium\n\nInfectious Disease load for various sewage water data\nOpen discussion"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-6",
    "href": "pages/meeting_notes.html#meeting-outcomes-6",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\nJacob shares info on waste water management + viral load information"
  },
  {
    "objectID": "pages/meeting_notes.html#notes-5",
    "href": "pages/meeting_notes.html#notes-5",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nIntroductions\n\nTiem van der Deure\n\nUniversity of Copenhagen PhD\nVector-borne Disease Modeling\nEpidemiological modeling and climate effects on health\nRafael Schoueten\n\nScott Jones\n\nHeavily involved in healthcare IT\n\ndx/dt\n\nGoogle Summer of Code\n\nRecently discovered by the team\nGoogle Season of Docs\n\nBest for long-term maintenance\nSignificant challenge organizing in Julia docs ecosystem\n\n\nOHDSI + Julia\n\nWorking with EHR from EPIC is demanding\n\nLabour intensive albeit improving\n\nTuring modeling “making them work”\n\nGetting them to run\n\nMaking it run fast enough\nTrade off ease-of-use for computation speed\n\nRequires significant mathematical ability for speed gains\n\n\nSewage water information for disease population estimations\n\nWeekly excerpt\nInfectious disease doctor\n\nWould be really neat to make some kind of app to check wastewater\n\nPropensity of viruses in ER\n\n\nPhysician testing for rough understanding of what is happening in community\n\nAbility to look for multiple co-factors instead of just one disease\n\nMany healthcare systems put together monitoring systems\n\nNHS (in UK) dismantled their monitoring systems\n\n\nDatabases and JuliaHealth\n\nShow how to do the basics\nCommon database errors\n\nHow to address them\n\nConsider having more people working in this space?\nNot really a problem within ecosystem\nLook at drivers across all packages to see how things work in Julia ecosystem\n\nSee how we can address issues across ecosystem"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-4",
    "href": "pages/meeting_notes.html#agenda-4",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nNew member introductions\nMisc Announcements\n\nCalciumScoring.jl – Dale Black\nSurvival Analyses – Arin Basu\nGoogle Summer of Code Fellowship wrapping up\nWe are on the Julia Community Calendar!\nSmall updates to the JuliaHealth website\n\nRunning tasks follow-ups:\n\nShort-term task follow-ups:\n\n@Jacob Set-up HackMD to take notes going forward\n\nCopy and paste meeting minutes over to JuliaHealth PR to update at end of meetings\n\n\n@Dilum finds out how to live stream JuliaHealth BoF\n\nLong-term task follow-ups:\n\nCreating a template repository \n\nDebrief from JuliaCon\n\nInteroperability of Julia with health research ecosystems (R )\nDevelop and document tutorials showcasing compositional solutions to JuliaHealth ecosystem problems\nCoordinate with bigger Julia Blog to bridge between communities even better\nDatabases and JuliaHealth\n\nJon Starr and NumFOCUS’s OSSci Program\nOpen discussion on next steps for the JuliaHealth community"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-7",
    "href": "pages/meeting_notes.html#meeting-outcomes-7",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\n@Jacob follow-up with Jonathan about JuliaHealth + OSSci\n@Edmund let Jacob know about blog posts solving problems\n\n\n\nLong-Term Outcomes\n\nSupport OSSci about JuliaHealth"
  },
  {
    "objectID": "pages/meeting_notes.html#notes-6",
    "href": "pages/meeting_notes.html#notes-6",
    "title": "Meeting Notes",
    "section": "Notes",
    "text": "Notes\n\nIntroductions\n\nClark C. Evans\n\nMaster cobbler of YAML\nUsed to work at Prometheus Research\n\nSold to IQVIA\n\nWorked under MechanicalRabbit Umbrella\n\nDeveloped FunSQL.jl with Kirill\nDatabase characterization\n\nJoined Tufts University CTSA\n\nHelping with data warehousing\n\nObjects to query OHDSI databases and EPIC Clarity\n\nGetting Pluto working\n\n\nJonathan\n\nManager for OSSci for NumFOCUS\nGoal: Mapping open source science ecosystem\nWork with Distributed Computing\n\nBerkeley technology\nBlocks and chains!\n\nUsing Open Source and Science to drive research\n\nEdmund\n\nPhD Candidate at Texas Dallas\n\nMolecular and Cell Biology\nFunctional Genomics\n\nComing from JuliaCon\nExcited about Health stuff\n\n\nInteroperability of Julia with health research ecosystems (R)\n\nEasiest way to interoperate is to call them directly from the command line\nBuild your own executables\nMost reliable/easiest\nDatabase approach:\n\nBuild table in one language\nIngest in another\n\nCombining executables in one location – use Docker?\n\nCan run on several different machines\n\nBuilding R packages with Julia backends is possible\n\nDevelop and document tutorials showcasing compositional solutions to JuliaHealth ecosystem problems\n\nCompeting Julia with other tutorials?\nSwitching over to Julia from what?\nWhy are people still not switching?\n\nDemonstrating the use is one way\n\nObviously, one could write more posts\nBut there seems to be a lot of content already – what is missing?\nDoes seem like there is two different levels of documentation\n\nBeginner\nAdvanced\n\nWhere are the practical means of solving problems in Julia?\n\nDatabases and JuliaHealth\n\nShow how to do the basics\nCommon database errors\n\nHow to address them\n\nUnclear on how to solve it; more people working in this space?\nNot really a problem within ecosystem\nLook at drivers across all packages to see how things work in Julia ecosystem\n\nSee how we can address issues across ecosystem\n\n\nJonathan Starr and NumFOCUS’s OSSci Program\n\nGetting to deep diving within Julia ecosystem\nResearchers who want to find a package that they can use and develop\nMapping projects and people to a given tool\n\nCan look at map to see where packages are needed for a particular ecosystem\nCan click on and connect with researchers\nHighlighting of credit for researchers\n\nStarting with NumFOCUS projects\nBuilding out knowledge of all ongoing projects/software\n\nJulia is little represented right now\n\nHow to show to funders/orgs what projects to support\nHow to build support across or collaboration between groups\nTrying to stop abandonware from happening\nAttempting to build social infrastructure\nQ&A\n\nTufts doing something very similar – happy to collaborate\nHow can JuliaHealth get started and involved?\n\nJonathan: Send me reference page and we can get this started!\n\n\nLinks:\n\nAbout: https://numfocus.org/open-source-science-initiative-ossci\nHow To Join: https://opensource.science\nMap of Open Source Science (MOSS)"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-5",
    "href": "pages/meeting_notes.html#agenda-5",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nIntroductions and what people in the community are using Julia for in health research\nWhat is missing of painful in Julia that is needed to drive health research forward\nThoughts on how to address some of these problems\nOpen discussion and next steps for JuliaHealth\n\n\nShort-Term Outcomes\nNot Available\n\n\nLong-Term Outcomes\n\nACTION: Develop and document tutorials showcasing compositional solutions to JuliaHealth ecosystem problems.\nACTION: Establish cohesive and organized Julia Blog to guide users and highlight official blogs."
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-notes-1",
    "href": "pages/meeting_notes.html#meeting-notes-1",
    "title": "Meeting Notes",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\nAttendee interests and background\n\nHere to learn\nFrom EHR development and background\nGenie folks here to support JuliaHealth endeavors\nGenomics research and prevention\nQuebec Heart and Lung Institute\nRepresenting PumasAI\nConsulting group\n\nDeveloping health research in Michigan area\nAggregating claims data\nTo learn what is going on in the community\n\nCreator of MetaAnalysis.jl\nInvolved with backend of healthcare IT\nWorking on JuliaHub\n\nLearning about packages that are out there\nHere to support JuliaHealth members\nNew Zealand longitudinal child health\n\nHave own secure system\nPost-COVID syndrome\n\nComputational biology\n\nSickle Cell\nApplying some ML\n\n\n\nProblems within the Julia ecosystem\n\nJulia needs more database connectivity to more easily do operations research\nDatabases are a pain point and composing with other aspects of the ecosystem\nInteroperability within Julia and other sorts of resources\nI end up doing the bare minimum in SQL\n\nDo we have RAM?\nCan we pull this into the Julia ecosystem?\nCrank up the RAM! But only so much scaling\nMinimal SQL writing\n\nSearchlight.jl: Julia ORM layer within\n\nIs Genie like a shiny?\n\nNo, more of a full-stack\nGoes beyond just visualization dashboards\n\n\nSequencing data\n\nEqually data\nEveryone uploads data in slightly different ways\nMake simple ways to pull that data\nR Conductor –&gt; JuliaConductor?\n\nWould make genomic pipelines within Julia pipelines a lot easier\n\nWe need to understand the underlying structures\nOne of the big pain points\n\nOften to have roll your own\n\n\nEpiR –&gt; EpiJ?\n\nPower calculators\n\nCo-founder of start-up\n\nFound unmet need for remote monitoring for neuotropenia\nNon-invasive screen for neutropenia\nDevice runs Julia\nPain points:\n\nTestability of hardware\nLOTS of CI – bit of a pain\nHow much repetition happens in CI\n\nPart of the problem for these problems:\n\nThere are still going to be folks who use the same organizations\nOvercoming inertia to do the same or similar things in Julia\nWrapping around Julia?\n\nBringing it into the R ecosystem\nLeading to big impacts for callable things from R by having smaller static binaries\nWrapping Julia packages in R\n\nN3C – National COVID Cohort Collaborative\n\nWent to many healthcare systems across the US to get COVID data\nShelled out to Palantir\nOpen source tools within the ecosystem\nJuliaHub has Boeing board member\n\nTrusted within security community\nCould help in this situation\n\n\n\n\n\nThoughts on how to address some of these problems\n\nUsing other packages outside of Julia\n\nIf you have some way to wrap around it\nGetting support\nPythonCall.jl or RCall.jl\n\nNot clear how to make this compositional\n\n\nThe paradox of compositionality\n\nBlog posts go a huge ways to solving problems\nTutorials showing how things can be combined together\nPromotional type material\nNice docs are nice\n\nThe Julia Blog itself\n\nMentions JuliaBloggers but doesn’t help with guiding users to read\nBlogs need to go on as official blogs\nJulia Forem – is it maintained?\n\nHook into the tags from blogs\nCross-posting where appropriate\n\n\nHow to learn Julia within the context of health\n\nCarpentries for learning resources"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-summary",
    "href": "pages/meeting_notes.html#meeting-summary",
    "title": "Meeting Notes",
    "section": "Meeting Summary",
    "text": "Meeting Summary\nIn Attendance: Jacob Zelko, Fareeda Abdelazeez, Zachary Christensen\nLocation: Virtual\nSummary: Discussed new members, upcoming JuliaCon, JuliaHealth Birds of a Feather discussion on topics like neural decoding and OMOP tooling, managing logistics for Julia organizations, and JuliaHealth PR reviews.\nKeywords: #brain #imaging #neural #decoding #collaboration #community #engagement"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-6",
    "href": "pages/meeting_notes.html#agenda-6",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nNew member welcomes!\nPlanning JuliaHealth Birds of a Feather\n\nTopics?\nFacilitators?\nCreating actionable outcomes?\n\nOpen discussion on Julia Orgs, How Do You Manage Logistics?\nMisc topics\nJulia for Health Informatics Research & Bridging community organizations\n\n1. Open Discussion on [The Graphs Ecosystem](https://discourse.julialang.org/t/the-graphs-ecosystem/99463?u=thecedarprince)"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-8",
    "href": "pages/meeting_notes.html#meeting-outcomes-8",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\n@Jacob Set-up HackMD to take notes going forward\n\nCopy and paste meeting minutes over to JuliaHealth PR to update at end of meetings\n\n\n\n\nLong-Term Outcomes\n\nACTION: Creating a template repository"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-notes-2",
    "href": "pages/meeting_notes.html#meeting-notes-2",
    "title": "Meeting Notes",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\nNew members:\n\nZachary Christensen\n\nNeuroimaging research\nMD/PhD\n\nTrying to finish this year!!!\n\nLots of background work like in JuliaData\nWorks on making Julia interface\n\n\nAnnouncement: JuliaCon about 1 month away!\n\nWe have our own track: biology and medicine\nMany people working on different things\n\nJuliaHealth Birds of a Feather Discussion\n\nPossible Topics:\n\nNeural decoding \n\nInspired by MATLAB: http://www.readout.info \nSister organization: https://julianeuro.github.io/packages\n\nOMOP Tooling for Real World Data\nHow to start collaborations?\n\nMaybe grant collaborations?\nGetting access to datasets\n\nComing up with different research questions\n\n\nHow can we integrate across the community?\n\nWhat problem can we solve?\n\nBecome a community resource to point to packages\nDon’t need to keep recreating or developing new packages\n\nPackages could be applications built on top of a specific use case\nCombining old packages in new ways\n\n\n\n\n\nOpen discussion on Julia Orgs, How Do You Manage Logistics?\n\nHave multiple persons part of the organizations\nSharing meeting documentation\n\nShare Google Doc at the beginning or before a meeting in announcement\nPublish notes on website publicly\n\nPR to update the JuliaHealth website with new tab for meeting minutes\n\nACTION: Using HackMD to take notes going forward\nCopy and paste meeting minutes over to JuliaHealth PR to update at end of meetings\n\n\n\nConsistent APIs for JuliaHealth\n\nInitial first pass with HealthBase.jl: https://github.com/JuliaHealth/HealthBase.jl \nAs free as possible from niche\nCould become quickly overwhelming or run risk of bikeshedding\nArrayInterface is a learning example in this context\nLight dependency package is great with a well-described API \nHow to move forward and get momentum\n\nWithout it turning into a mess\n\nCommon ontologies: http://obofoundry.org \n\nJuliaHealth PR Reviews\n\nPR Checklist:\n\nPurpose\nReduce cognitive load\n\nJuliaHealth package forks: https://github.com/JuliaCI/PkgTemplates.jl \nACTION: Creating a template repository"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-summary-1",
    "href": "pages/meeting_notes.html#meeting-summary-1",
    "title": "Meeting Notes",
    "section": "Meeting Summary",
    "text": "Meeting Summary\nIn Attendance: Jacob Zelko, Dilum Aluthge, Asher Wasserman, Fareeda Abdelazeez, Kyle Beggs\nLocation: Virtual\nSummary: First JuliaHealth community call to meet other Julians, learn how we can galvanize the Juliahealth Community, and open discussion on paths forward\nKeywords: #data #analysis #hemodynamics #omop #machine #learning"
  },
  {
    "objectID": "pages/meeting_notes.html#agenda-7",
    "href": "pages/meeting_notes.html#agenda-7",
    "title": "Meeting Notes",
    "section": "Agenda",
    "text": "Agenda\n\nIntroductions\nWhat people are using Julia for in health research\nSelected topics and state within the Julia ecosystem:\n\nObservational Health\nMedical Imaging\nMachine Learning and Health\nInteroperability Standards\nDrug Discovery\n\nStandard Interfaces"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-outcomes-9",
    "href": "pages/meeting_notes.html#meeting-outcomes-9",
    "title": "Meeting Notes",
    "section": "Meeting Outcomes",
    "text": "Meeting Outcomes\n\nShort-Term Outcomes\n\n@Dilum finds out how to live stream JuliaHealth BoF\n\n\n\nLong-Term Outcomes"
  },
  {
    "objectID": "pages/meeting_notes.html#meeting-notes-3",
    "href": "pages/meeting_notes.html#meeting-notes-3",
    "title": "Meeting Notes",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\nIntroductions\n\nDilum Aluthge – MD/PhD Student Brown University (BCBI), PumasAI\n\nJulia Community Involvement\n\nPkg\nGeneral Registry\nContinuous Integration\n\nJuliaHealth and beyond\n\nOriginally created JuliaHealth to bring people together in health\nBioJulia folks are a great source of inspiration for packages!\n\nBirds of a Feather!!! COME VISIT! – Friday July 28th, 4PM EST in Boston, MA!\n\nAsher Wasserman – Astronomy PhD, Data Scientist in BioTech\n\nJulia Community Involvement\n\nDifferential Equations\nOne off deployments\n\n\nFareeda Abdelazeez – GSoC JuliaHealth (First GSoC Student!!!!!)\n\nJulia Community Involvement\n\nObservational Health tooling JuliaHealth!\n\n\nKyle Beggs – Software Engineer in small Optics company, Finishing PhD in MechE\n\nJulia Community Involvement\n\nPDEs\nHemodynamics research focus\nTake advantage of these tools for imaging, segmentation\n\n\n\nWhat people are using Julia for in health research\n\nAsher: Cancer patient data\n\nPDFs and other data formats \n\nCDA documents\n\nHow to structure this ad hoc type of data into common data model\nDeveloping processes to automatically make these documents useful\nHow do we clean the data to match actual reality\nHow do we make this data actionable/useful\nCould match towards goals of OHDSI/observational health\n\nAnalyses at population level?\nOutcome propensity scores?\nPatient phenotype development?\n\nRole of Julia:\n\nMainly as a scripting language\nSupplement to a lot of SQL scripting (FunSQL discovered)\nPython is generally being deployed because of software devs\n\nHow to not crash AWS, etc.\n\nJulia deployment for risk (?)\nSurvival Analysis in Julia; lifelines in Python otherwise\n\n\nKyle: Vascular Surgical Planning\n\nUnobvious on where to place graft, etc – educated guesses\nCreating a tool to simulate operations\nWhy Julia?\n\nExisting tools are open source but really GUI-driven\nIntegration across ecosystem would be even better for hemodynamics in Julia\nGive a base to simulate the mechanics involved with this\n\nJuliaFEM, etc. \n\n\nMesh list methods\n\nPoint clouds\nMain application is within hemodynamics\n\n\nFareeda: JuliaHealth GSoC Student\n\nWorking on OMOP Common Data Model\nStandard model for observational health patient data\nDevelop infrastructure of JuliaHealth to work with OMOP CDM data\n\nImprove DBConnector\nOMOPCDMCohortCreator.jl – add tooling\nOHDSIAPI.jl – creating interfaces for ATHENA/ATLAS\n\nPatient Level Prediction tooling\n\nUsing MLJ algorithms\nAttempting to solve a research question\n\nEvaluate success of package\n\n\nStretch goals:\n\nCohort Quality and underlying data is “good”\nBuild support for OBDC connections\n\n\nOverlap with other organizations\n\nDoesn’t happen in a vacuum\nServing as a bridge between a bridge and a community between other groups\nWhat should be JuliaHealth?\n\nBringing together people \n\n\n\nSelected topics and state within the Julia ecosystem:\n\nObservational Health\nMedical Imaging\nMachine Learning and Health\nInteroperability Standards\nDrug Discovery\n\nStandard Interfaces\n\nJune 30th, 2023\nAttending:\nAgenda:\n\nNew member welcomes!\nPlanning JuliaHealth Birds of a Feather\n\nTopics?\nFacilitators?\nCreating actionable outcomes?\n\nOpen discussion on Julia Orgs, How Do You Manage Logistics?\nMisc topics\n\nJulia for Health Informatics Research & Bridging community organizations\n\nOpen Discussion on The Graphs Ecosystem\n\n\n\nNotes: \n\nNew members:\n\nZachary Christensen\n\nNeuroimaging research\nMD/PhD\n\nTrying to finish this year!!!\n\nLots of background work like in JuliaData\nWorks on making Julia interface\n\n\nAnnouncement: JuliaCon about 1 month away!\n\nWe have our own track: biology and medicine\nMany people working on different things\n\nJuliaHealth Birds of a Feather Discussion\n\nPossible Topics:\n\nNeural decoding \n\nInspired by MATLAB: http://www.readout.info \nSister organization: https://julianeuro.github.io/packages\n\nOMOP Tooling for Real World Data\nHow to start collaborations?\n\nMaybe grant collaborations?\nGetting access to datasets\n\nComing up with different research questions\n\n\nHow can we integrate across the community?\n\nWhat problem can we solve?\n\nBecome a community resource to point to packages\nDon’t need to keep recreating or developing new packages\n\nPackages could be applications built on top of a specific use case\nCombining old packages in new ways\n\n\n\n\n\nOpen discussion on Julia Orgs, How Do You Manage Logistics?\n\nHave multiple persons part of the organizations\nSharing meeting documentation\n\nShare Google Doc at the beginning or before a meeting in announcement\nPublish notes on website publicly\n\nPR to update the JuliaHealth website with new tab for meeting minutes\n\nACTION: Using HackMD to take notes going forward\nCopy and paste meeting minutes over to JuliaHealth PR to update at end of meetings\n\n\n\nConsistent APIs for JuliaHealth\n\nInitial first pass with HealthBase.jl: https://github.com/JuliaHealth/HealthBase.jl \nAs free as possible from niche\nCould become quickly overwhelming or run risk of bikeshedding\nArrayInterface is a learning example in this context\nLight dependency package is great with a well-described API \nHow to move forward and get momentum\n\nWithout it turning into a mess\n\nCommon ontologies: http://obofoundry.org \n\nJuliaHealth PR Reviews\n\nPR Checklist:\n\nPurpose\nReduce cognitive load\n\nJuliaHealth package forks: https://github.com/JuliaCI/PkgTemplates.jl \nACTION: Creating a template repository"
  },
  {
    "objectID": "JuliaHealthBlog/posts/juliahealth-ecosystem/ecosystem.html",
    "href": "JuliaHealthBlog/posts/juliahealth-ecosystem/ecosystem.html",
    "title": "Growing Together: Exploring Sub-Ecosystems within JuliaHealth 👋",
    "section": "",
    "text": "Welcome to the vibrant and expanding world of JuliaHealth! As our community grows and the number of powerful Julia packages dedicated to health, medicine, and biomedical research increases, we’re seeing natural clusters of activity form around specific domains. To foster collaboration, improve discoverability, and accelerate progress in these specialized areas, we’re excited to introduce the concept of Sub-Ecosystems within JuliaHealth.\nThis post aims to:\n\nExplain what we mean by a “sub-ecosystem.”\nHighlight some of the prominent and emerging focus areas we see.\nInvite you to participate in shaping and growing these vital communities within the broader JuliaHealth organization.\n\n\nWhat is a JuliaHealth Sub-Ecosystem?\nThink of JuliaHealth as a large, bustling city. A sub-ecosystem is like a specialized district within that city – a place where people working on similar problems congregate, share tools, develop specific expertise, and collaborate closely.\nMore formally, a JuliaHealth sub-ecosystem is:\n\nA collection of related Julia packages addressing challenges within a specific health domain (e.g., medical imaging, observational health data).\nA community of developers and users passionate about that domain.\nA focal point for collaboration, potentially leading to shared standards, interoperable tools, and targeted documentation or tutorials within that niche.\n\nThese aren’t rigid silos, but rather areas of concentrated effort that make it easier for newcomers and veterans alike to find relevant tools and expertise.\n\n\nWhy Sub-Ecosystems? The Benefits\nStructuring our community efforts around these focus areas offers several advantages:\n\nImproved Discoverability: Newcomers looking for tools for, say, MRI simulation or analyzing OMOP CDM data can more easily find the relevant cluster of packages.\nFocused Collaboration: Developers working on related packages can coordinate efforts, avoid duplication, and build tools that work well together.\nDeeper Expertise: Sub-ecosystems become hubs of specialized knowledge, accelerating innovation within that domain.\nTargeted Resources: It becomes easier to create specific tutorials, documentation examples, and showcase demonstrations (like introductory videos!) for a particular area.\nClearer Roadmap: Identifying strengths and gaps within a sub-ecosystem can help guide future development efforts.\n\n\n\nExploring Potential & Existing Sub-Ecosystems\nBased on the packages currently under the JuliaHealth organization and exciting areas of growth, here are some potential sub-ecosystems we envision. This is a starting point for discussion, and we expect these areas to evolve!\n\n\n\n\n\n\nMedical Imaging\n\n\n\n Point of Contact: Jakub Mitura | cncastillo | Divyansh Goyal\nThis is already a strong area within JuliaHealth, focusing on the handling, processing, visualization, and analysis of medical images from various modalities.\n\nFocus: Reading, writing, viewing, segmenting, registering, and analyzing medical image data (DICOM, NIfTI, etc.). Simulation of imaging processes.\nRelevant Packages: DICOM.jl, DICOMClient.jl, DICOMTree.jl, ITKIOWrapper.jl (and archived predecessors), MedEval3D.jl, MedEye3d.jl, MedImages.jl, MedPipe3D.jl, KomaMRI.jl.\nEmerging Areas: Expanding support for diverse modalities like Cellular Imaging, Molecular Imaging, Transmission Electron Microscopy (TEM), X-ray, advanced CT/PET analysis. Developing robust pipelines from image acquisition to analysis.\nShowcase: [Placeholder for a short video showcasing Medical Imaging packages like MedEye3d.jl and DICOM.jl interaction]\n\n\n\n\n\n\n\n\n\nObservational Health Data Science & OMOP CDM\n\n\n\n Point of Contact: Jacob Zelko | Jay Sanjay\nThis area centers on leveraging real-world health data, often standardized using models like the OMOP Common Data Model, for research and analysis.\n\nFocus: Working with standardized health data formats (especially OMOP CDM), creating cohorts, analyzing patient pathways, generating metrics, connecting to relevant APIs (OHDSI). Handling code mappings (ICD, RxNorm, etc.).\nRelevant Packages: OMOPCommonDataModel.jl, OMOPCDMDatabaseConnector.jl, OMOPCDMCohortCreator.jl, OMOPCDMPathways.jl, OMOPVocabMapper.jl, OHDSIAPI.jl, ICD_GEMs.jl, DiagnosisClassification.jl, PharmaceuticalClassification.jl. EpiJ often uses data derived from these sources.\nEmerging Areas: Patient-level prediction (OMOPCDMPredictor), advanced analytics on OMOP data, integration with federated learning frameworks.\nShowcase: [Placeholder for a short video demonstrating cohort creation with OMOPCDMCohortCreator.jl]\n\n\n\n\n\n\n\n\n\nNeuroscience & Neurophysiology\n\n\n\n Point of Contact: Jacob Zelko | Adam Wysokiński\nDedicated to tools for analyzing data from the brain and nervous system.\n\nFocus: Processing and analyzing neurophysiological data like EEG, MEG, ECOG, NIRS. Applying computational neuroscience techniques.\nRelevant Packages: NeuroAnalyzer.jl.\nEmerging Areas: Seizure detection algorithms, analysis of brain connectivity, integration with neuroimaging data (linking to Medical Imaging), modeling neural dynamics.\nShowcase: [Placeholder for a short video showing EEG data analysis with NeuroAnalyzer.jl]\n\n\n\n\n\n\n\n\n\nSimulation & Computational Modeling\n\n\n\n Point of Contact: Jacob Zelko | Yolhan Mannes | Divyansh Goyal\nUsing computational power to simulate biological and physiological processes relevant to health.\n\nFocus: Simulating complex systems like MRI physics, cardiac electrophysiology and mechanics, blood flow dynamics.\nRelevant Packages: KomaMRI.jl, Thunderbolt.jl, BloodFlowTrixi.jl.\nEmerging Areas: Virtual cell modeling, pharmacokinetic/pharmacodynamic (PK/PD) modeling, multi-scale modeling integrating different physiological levels, agent-based modeling for epidemiology.\nShowcase: [Placeholder for a short video illustrating an MRI simulation using KomaMRI.jl]\n\n\n\n\n\n\n\n\n\nPublic Health & Epidemiology\n\n\n\n Point of Contact: Krishna Bhogaonker\nApplying computational tools to population health, disease surveillance, and epidemiological research.\n\nFocus: Analyzing population datasets (like IPUMS), environmental health data (NCEI), implementing epidemiological methods, disease surveillance techniques.\nRelevant Packages: EpiJ, IPUMS.jl, NCEI.jl, ICD_GEMs.jl (for population statistics). Connects strongly with Observational Health Data.\nEmerging Areas: Geospatial analysis for health, infectious disease modeling (could overlap with Simulation), analysis of social determinants of health.\nShowcase: [Placeholder for a short video on using EpiJ for a simple epidemiological calculation]\n\n\n\n\n\n\n\n\n\nBioinformatics & Data Querying\n\n\n\n Point of Contact: Jacob Zelko\nTools focused on interacting with biomedical databases and literature.\n\nFocus: Programmatic access to databases like PubMed/MEDLINE, PubChem. Analyzing co-occurrences in literature.\nRelevant Packages: BioMedQuery.jl, PubMedMiner.jl, PubChemCrawler.jl. (Note: This overlaps with the larger BioJulia ecosystem, focusing here on health-specific applications).\nEmerging Areas: Integration with knowledge graphs, advanced text mining for clinical notes (see NLP/ML).\n\n\n\n\n\n\n\n\n\nInteroperability, Standards & Foundational Tools\n\n\n\n Point of Contact: Jacob Zelko\nPackages providing core functionalities, implementing health standards (FHIR, DICOM, OMOP), or offering essential utilities.\n\nFocus: Implementing standards like FHIR and SMART on FHIR, DICOM communication, providing common data structures or functions, managing authentication, handling sensitive data.\nRelevant Packages: FHIRClient.jl, SMARTAppLaunch.jl, SMARTBackendServices.jl, DICOM.jl, DICOMClient.jl, OMOPCommonDataModel.jl, HealthBase.jl, HealthSampleData.jl, DateShifting.jl, EHRAuthentication.jl.\n\n\n\n\n\n\n\n\n\nHealth AI/ML & Text Analysis\n\n\n\n Point of Contact: Jacob Zelko | Param Thakkar\nApplying machine learning techniques, including NLP, to health data. * Focus: Association rule learning, parsing clinical text (cTAKES), using specific ML models for health predictions (CloToP), foundational ML tools, interacting with health LLMs. * Relevant Packages: ARules.jl, CAOS.jl, CTakesParser.jl, MTIWrapper.jl, CloToP.jl, HealthMLBase.jl, JuliaHealthLLM, OMOPCDMPredictor.\n\n\n\n\n\n\n\n\nExciting Frontiers\n\n\n\nBeyond these, we see potential for growth in areas like:\n\nEpigenetics: Tools for analyzing epigenetic data in health contexts.\nGenomics in Health: While overlapping with BioJulia, specific clinical genomics applications could form a focus.\nClinical Trials: Tools for design, simulation, and analysis (e.g., BlindingIndex.jl).\n\n\n\n\n\nHow You Can Get Involved!\nThis is a community effort! We need your input and participation to make these sub-ecosystems thrive:\n\nJoin the Discussion: Share your thoughts on these proposed areas. Do they make sense? Are key areas missing? Discuss on:\n\nDiscourse: Biology, Health, and Medicine category\nZulip: #biology-health-and-medicine stream\nSlack: #health-and-medicine channel (Invite link)\n\nContribute to Packages: Find a sub-ecosystem that interests you and contribute to the existing packages within it – documentation, bug fixes, new features are all welcome!\nPropose New Packages: See a gap within a sub-ecosystem? Consider starting a new package to fill that need.\nDevelop Showcase Materials: Help create tutorials, examples, or even short introductory videos (like the placeholders above!) demonstrating how packages within a sub-ecosystem work together. Let us know if you’re interested in contributing a video!\nLead or Champion: Passionate about a specific area? Consider helping to coordinate efforts or act as a point person for that sub-ecosystem.\n\n\n\nConclusion: Building Focused Communities\nBy recognizing and nurturing these specialized sub-ecosystems, we aim to make JuliaHealth even more effective, collaborative, and welcoming. This structure can help channel the amazing energy within our community, leading to more robust, interoperable, and impactful tools for improving health worldwide.\nThis is just the beginning of the conversation. Let’s work together to map out these territories and build thriving communities within them. We’re excited to see where these focused efforts take us!\n\n\n\n\nCitationBibTeX citation:@online{community_(leader_jacobzelko,_contributor_divyansh_goyal)2025,\n  author = {Community (Leader JacobZelko, Contributor Divyansh Goyal),\n    JuliaHealth},\n  title = {Growing {Together:} {Exploring} {Sub-Ecosystems} Within\n    {JuliaHealth} 👋},\n  date = {2025-04-29},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCommunity (Leader JacobZelko, Contributor Divyansh Goyal), JuliaHealth.\n2025. “Growing Together: Exploring Sub-Ecosystems Within\nJuliaHealth 👋.” April 29, 2025."
  }
]